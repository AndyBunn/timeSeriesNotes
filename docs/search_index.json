[["index.html", "A collection of note for ESCI 504 1 Preamble 1.1 Technical Setup", " A collection of note for ESCI 504 Andy Bunn 08-August-2025 1 Preamble This document is a compilation of notes, examples, and exercises developed for our course on time series analysis, with a particular focus on environmental applications. Rather than follow a traditional textbook, we’ve taken a modular, hands-on approach. Each section builds on what came before, emphasizing interpretation, practical tools, and critical thinking about how time series models apply to real-world environmental questions. You’ll find annotated code, worked examples, and commentary throughout — some informal, some technical — all aimed at helping you do time series work, not just read about it. The examples often draw from environmental data: climate records, ecological time series, and other systems that change over time and space. But the core ideas and tools are broadly applicable. From time to time, I’ll refer you to more formal textbooks or peer-reviewed articles to go deeper into theory, context, or application. These serve as complements, not replacements, for what we’re doing here. The tone of these notes is meant to be accessible, pragmatic, and occasionally opinionated. You’ll see suggestions, cautions, and nudges — all based on what I’ve found helpful (and confusing) when learning and teaching these methods. This isn’t a comprehensive or final text. It’s a live document, shaped by your questions, feedback, and curiosity. Think of it as a field guide: not everything you’ll ever need, but hopefully exactly what you need to get your bearings, spot the patterns, and know what to do next. Please note! This is a very drafty document and there are typos and all kinds of silliness in it. 1.1 Technical Setup This document was written in Markdown using the bookdown package and built with R version 4.5.0`. You should be up to date — or at least close — on your versions of R, RStudio, and relevant packages. You can keep your packages updated by running: update.packages() 1.1.1 Project Structure To follow along with the examples, you’ll need a working RStudio project. Here’s what to do: Create a new RStudio project Go to File → New Project → New Directory → New Project. Give it a name (e.g., timeseries-notes) and choose where to save it. Download the data/ folder You’ll find a folder named data that contains most of the datasets we’ll use (anything we don’t download directly in the code). Save this folder inside your project directory. Your folder structure should look like this: timeseries-course/ ├── data/ │ ├── HansenSockeye.rds │ ├── jul65N.rds │ └── ... └── timeseries-notes.Rproj Refer to data files using relative paths In your code, use paths like \"data/jul65N.rds\" rather than full file paths. This makes your work portable and easier to share or rerun later. Store your own scripts in the root directory Any code you write for homework, labs, or your own exploration should be saved in the main project folder (alongside the .Rproj file). This keeps your work organized and ensures it runs smoothly within the project environment. If you’re new to R projects, the key idea is this: treat your project folder as your workspace. All code, data, and outputs should live inside it. It makes everything cleaner and reproducible. 1.1.2 Why Use Projects in R? Think of an RStudio project like a toolbox or field kit. Everything you need for an analysis — your scripts, data, figures, notes — lives in one container. When you open the project, RStudio knows: this is your workspace. You don’t have to tell it where to find files or worry about breaking things if you move the folder. Projects help you: Keep everything for a project in one place Use relative paths (like \"data/file.csv\") that work on any computer Avoid hard-coded setwd() calls that break when folders change Switch between different projects without mixing things up Work seamlessly with Git and other tools for version control In short, RStudio projects make your work more organized, more robust, and easier to share. 1.1.3 Why Reproducibility Matters Science is only as strong as its ability to be checked, tested, and built upon. Reproducibility means that someone else — or your future self — can rerun your analysis and get the same results. In the world of data analysis, that means: Your code runs start to finish without manual tweaking Your data is accessible, documented, and versioned Your results (figures, summaries, statistics) are generated from your code, not copy-pasted You (and others) can trace how a result came to be — and rerun it with new data if needed Working reproducibly isn’t just good practice — it’s essential for credible, transparent science. RStudio projects, scripted workflows, and literate programming tools like R Markdown and Bookdown are all part of that ecosystem. This course will help you build those habits as you learn the methods. "],["the-measure-of-time.html", "2 The Measure of Time 2.1 Big Idea 2.2 Reading 2.3 Packages 2.4 What Kinds of Data? 2.5 The ts Class 2.6 Trend in time 2.7 The zoo Class 2.8 Lubricate the Dates 2.9 Tidy it Up 2.10 Your Work", " 2 The Measure of Time 2.1 Big Idea One of the weirder things in our understanding of the universe is that time flows in only one direction. This “arrow of time” creates some wrinkles in how we handle time-series data. 2.2 Reading Have a look at Chapter one (through section 1.4) from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great text for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 2.3 Packages 2.3.1 Load We will use the tidyverse (Wickham 2023), zoo (Zeileis, Grothendieck, and Ryan 2025), and broom (Robinson, Hayes, and Couch 2025). Install them if you haven’t done so already (e.g.,: install.packages(c(\"tidyverse\", \"zoo\"))). library(tidyverse) library(zoo) 2.3.2 Keep Up to Date It’s important that you are (reasonably) current in your R and RStudio installations. That goes for packages too. Here are the versions of the packages above that are loaded when this page was knit. packageVersion(&quot;tidyverse&quot;) ## [1] &#39;2.0.0&#39; packageVersion(&quot;zoo&quot;) ## [1] &#39;1.8.14&#39; Get in the habit of updating packages regularly. A good way of doing that is via update.packages(). Because I’m usually content to let R update everything semi-automatically I usually run that as update.packages(ask = FALSE, type = \"binary\") from a fresh R session. Fresh here means that don’t have packages already loaded – e.g., go to Session | Restart R in Rstudio. Running that regularly (I do it every week, but I’m a pretty heavy user) is a very good idea. You can also click the little update button under the pacakges in RStudio if you prefer that kind of interaction. 2.4 What Kinds of Data? So weirdly enough we have to ask, what is a time series? My default place for answers for non-controversial things is Wikipedia, “A time series is a series of data points indexed in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time.” OK. That’s actually perfect for our purposes. It has some key aspects that we should consider. First, the data are in a series meaning they come one after another. Second, they are indexed in time. Third, the data are typically at regularly spaced intervals of time.1 We will refer to a time series \\(y\\) measured/observed at time \\(t\\) as \\(y_t\\). For regular time series \\(t\\) is the index of the time (e.g., year, day, second, etc.) so that \\(t_i - t_{i-1} = 1\\). More formally, we treat \\(y_t\\) as random variable that is a realization of a process at time \\(t\\) so that \\(\\{y_t,t \\in T \\}\\). This is all straightforward. The confusing part in getting started with working with time series is often figuring out how to tell the computer about how the indexing is done. Are the data measured at arbitrary time steps? Does the index refer to actual dates and times? If so, how do we deal with leap days? Or time zones? And how do computers even store these kinds of data? 2.5 The ts Class 2.5.1 A Very Useful Class As you know R has a lot of of onboard data sets. You can take a look at the data sets package to get an idea of what’s easy to grab. But you can also look at the time-series packages (like my package dplR) many of which have data available as well. We’ll also work with data we can read into R from an online source or via a text file of some sort. The options are, for almost any reasonable definition, infinite. Let’s start with two onboard data sets. data(LakeHuron) plot(LakeHuron) OK, what’s going on here? We loaded a data set of the level of Lake Huron (in feet) from 1875 to 1972, made a plot, and looked at some summary statistics. You can look at the help page for the data via ?LakeHuron. R knows this is a time series – the class of the object LakeHuron is class ts which is one of the ways we will work with time-series data. class(LakeHuron) ## [1] &quot;ts&quot; Look at the help page for ts to understand what that structure looks like. The most important thing to understand is that the time values are attributes and not, say, a stand-alone column of times next to the data the way you might do it in Excel. Because this is a regular time series, the object just has data values (the lake height in feet). The index of time values isn’t stored row-by-row the way you might keep them in a spreadsheet. Rather, the index is calculated dynamically using start, end and frequency values stored as attributes. We can see these using the very handy tsp function. tsp(LakeHuron) ## [1] 1875 1972 1 This tells us the start time, end time, and frequency of the data. The LakeHuron time series starts in the year 1875 and runs until 1972 with one observation per year. Before we go on, note that there are several methods for working with this class (like plotting). Recall that classes with methods can have shortcuts to commonly used functions. For instance, calling plot(LakeHuron) is the same as calling plot.ts(LakeHuron) because the ts class has a plot method. We will talk more about this later but you can see all of the methods that are in the ts class via methods(class=\"ts\"). Let’s look at another data set: data(co2) str(co2) ## Time-Series [1:468] from 1959 to 1998: 315 316 316 318 318 ... plot(co2) These data are monthly and run from 1959 to 1998. From a purely practical standpoint you should make sure that you and R agree on the time-series properties of a data set. For instance, you should know the start and end times and the frequency (the number of observations per unit of time). Before you go further, can you predict what the output will be when you run tsp(co2)? To look under the hood further and get an idea of how these objects exist in R run attributes(co2) and class(co2). 2.5.2 Digression: Describing a Time Series We’ve been focusing on the time-series qualities of the data (e.g., the index). But we can also treat these data they way we would other data. That is, when working with a time series you should be able to describe it the same way you’d describe any data. The summary function is most useful but I like to know things like the standard deviation too. And I like diagnostic plots like boxplots and histograms. Recall that time-series data are most importantly data and you should always explore your data thoroughly with descriptive statistics and plots. We will learn more techniques that are specific to time series (e.g., autocorrelation) later. But first, let’s plot a histogram of the Lake Huron data, overlay a normal curve, and look at the shape. This is a moderately involved bit of plotting but you can step through it bit by bit. I’ll go old school here with some base graphics. my.xlim &lt;- range(LakeHuron) h&lt;-hist(LakeHuron, breaks=10, col=&quot;lightblue&quot;, xlab=&quot;Lake Level (ft)&quot;, main=&quot;&quot;,xlim=my.xlim) xfit &lt;- seq(min(LakeHuron),max(LakeHuron),length=100) yfit &lt;- dnorm(xfit,mean=mean(LakeHuron),sd=sd(LakeHuron)) yfit &lt;- yfit*diff(h$mids[1:2])*length(LakeHuron) lines(xfit, yfit, col=&quot;darkblue&quot;, lwd=2) boxplot(LakeHuron, horizontal=TRUE, outline=TRUE, axes=FALSE, ylim=my.xlim, col = &quot;lightgreen&quot;, add = TRUE, boxwex=3) Given a sample size of less than 100, these data don’t show much kurtosis or skew and in fact look pretty normal. That is, I wouldn’t say they were platykurtic or negatively skewed as compared to a random expectation. How could I test such a thing? Well, there are tests in the moments package (e.g., agostino.test(LakeHuron), anscombe.test(LakeHuron), jarque.test(c(LakeHuron)) or we could use some kind of resampling method (e.g., bootstrapping). But here is where we get to the single most important mantra I have when working with data: plot and explore it. Plot your data. Plot your data. Plot your data. Statistical significance is like pornography (to steal from United States Supreme Court Justice Potter Stewart), you know it when you see it. There are other plots I like to make when exploring data. I’m partial to plotting the cumulative distribution function (take a look at ecdf) and maybe a normal QQ plot (qqnorm) but we can leave it here for the time being. Just remember: Plot your data. 2.5.3 More on the ts Index Let’s go back to the LakeHuron data. We think of these data as annual with the time index in years. But the ts class itself just thinks in terms of start, end, and frequency. It’s a simple system but not always what you want – in fact ts with a frequency of one isn’t really in years. From the help page for ts: The value of argument frequency is used when the series is sampled an integral number of times in each unit time interval. For example, one could use a value of 7 for frequency when the data are sampled daily, and the natural time period is a week, or 12 when the data are sampled monthly and the natural time period is a year. Values of 4 and 12 are assumed in (e.g.) print methods to imply a quarterly and monthly series respectively. time(LakeHuron) ## Time Series: ## Start = 1875 ## End = 1972 ## Frequency = 1 ## [1] 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 ## [16] 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 ## [31] 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 ## [46] 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 ## [61] 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 ## [76] 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 ## [91] 1965 1966 1967 1968 1969 1970 1971 1972 The times in the ts class aren’t really related to reality. Run the code below and then use time(foo) for each line to look at what ‘times’ you created. n &lt;- 100 # Annual data starting in the year 2000 foo &lt;- ts(rnorm(n),start=2000,frequency = 1) #time(foo) # 100 months starting in the year 2000 foo &lt;- ts(rnorm(n),start=2000,frequency = 12) #time(foo) # quarterly data foo &lt;- ts(rnorm(n),start=2000,frequency = 4) #time(foo) # less than one year of daily data foo &lt;- ts(rnorm(n),start=2000,frequency = 365) #time(foo) # decades! foo &lt;- ts(rnorm(n),start=2000,frequency = 0.1) #time(foo) # hours! foo &lt;- ts(rnorm(n),start=2000,frequency = 365*24) #time(foo) We are definitely going to want to use time series with real dates and times. What if you have data that are hourly and start on January 3, 2021 at 1:01AM PST? Can you coerce that into the ts class? Well, maybe but it’s not intuitive and there are better ways in R of doing this. The ts class is only one of the ways that R deals with time series (say hello zoo library!). The ts class very simple but not great for dates and times of day. As you work with other data you’ll encounter some of the other ways that R stores times and dates. Trying to understand these can get tricky very fast. Consider all the things you have to account for with calendar data. Leap years (leap seconds!), time zones, daylight savings, and so on. You can look at the help page for Date and DateTime to get started but in reality you’ll be better off searching the internet for explanations of how time is accounted for in R. And ultimately you just have to dive in. This is not an issue that is specific to R. It’s a devilish problem for every computer platform that is not unlike dealing with projections in GIS. I’m going to be as gentle as possible about it but I know that we’ll get stumped soon. The lubridate package is exciting as a way of making working with time-series data more pleasant. The package promises to make “working with dates fun instead of frustrating.” That would be nice. 2.6 Trend in time One of the most common things to do with a time series is to fit a linear model and see if there is a trend in the data as a function of time. Note that later in class we will worry about how to do this in a way that allows us to test the significance of the trend in a responsible way but for now let’s get a slope on the Lake Huron data. Were lake levels increasing or decreasing over those years? LakeHuronLinearModel &lt;- lm(LakeHuron~time(LakeHuron)) LakeHuronLinearModel ## ## Call: ## lm(formula = LakeHuron ~ time(LakeHuron)) ## ## Coefficients: ## (Intercept) time(LakeHuron) ## 625.5549 -0.0242 plot(LakeHuron) abline(LakeHuronLinearModel,col=&quot;red&quot;) We can see that how lake levels are changing at a rate of -0.02 feet per year. 2.7 The zoo Class When you have run up against the limitations of the ts class it’s time to visit the zoo. Let’s look at using the zoo library to work with time series. When I have dates and times that is the package I tend to use. Let’s look at the list of the vignettes in zoo. vignette(package=&quot;zoo&quot;) I suggest perusing these. E.g., vignette(topic=\"zoo\", package=\"zoo\"). Now to work. I have compiled weather data from the Bellingham Airport using the GSODR package. kbli &lt;- readRDS(&quot;data/kbli.rds&quot;) Run str, summary, and head on them to what this object is all about see. E.g., str(kbli) ## &#39;zoo&#39; series from 2000-01-01 to 2024-12-31 ## Data: num [1:9131, 1:2] 5.3 3.6 1.1 7.6 5.1 3.7 5.2 7.2 4.3 2.6 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:2] &quot;TEMP&quot; &quot;PRCP&quot; ## Index: Date[1:9131], format: &quot;2000-01-01&quot; &quot;2000-01-02&quot; &quot;2000-01-03&quot; &quot;2000-01-04&quot; &quot;2000-01-05&quot; ... summary(kbli) ## Index TEMP PRCP ## Min. :2000-01-01 Min. :-12.80 Min. : 0.000 ## 1st Qu.:2006-04-02 1st Qu.: 6.40 1st Qu.: 0.000 ## Median :2012-07-02 Median : 10.40 Median : 0.000 ## Mean :2012-07-01 Mean : 10.36 Mean : 2.389 ## 3rd Qu.:2018-10-01 3rd Qu.: 15.00 3rd Qu.: 2.290 ## Max. :2024-12-31 Max. : 28.00 Max. :87.880 head(kbli) ## TEMP PRCP ## 2000-01-01 5.3 1.219000e+01 ## 2000-01-02 3.6 1.270000e+00 ## 2000-01-03 1.1 1.270000e+00 ## 2000-01-04 7.6 8.890000e+00 ## 2000-01-05 5.1 5.100000e-01 ## 2000-01-06 3.7 1.110223e-15 OK. What have we learned? The object kbli is class zoo the time index is a Date, there are two columns of data (precipitation and temperature) and therefore we refer to this as a multivariate time series. Let’s do some plotting with the built in plot function for zoo plot(kbli) And look at some precip data with window. This is the 2024 water year. plot(window(kbli$PRCP, start = as.Date(&quot;2023-11-01&quot;), end = as.Date(&quot;2024-10-31&quot;)), ylab=&quot;Precip (mm)&quot;) Reminder, this is a zoo object and so R uses plot.zoo to do the actual plotting even though you just type plot. Same goes for window. The important distinction here is that unlike class ts, the times are actual dates and not an arbitrary index like we had with the LakeHuron data. The times are stored as class Date which is a good class for dates without times. If we had more finely resolved data we might want a different class like POSIXlt or POSIXct. Peruse the help file for Date-Time Classes via ?DateTimeClasses. 2.8 Lubricate the Dates The package lubridate has some nifty functions for working with dates and times. There is one very nice vignette to look at and I suggest perusing it via vignette(topic=\"lubridate\",package=\"lubridate\"). For instance, you can grab the month for each observation in kbli via month(kbli) which might come in handy for pulling out just the summer months. summerPrecip &lt;- kbli$PRCP[month(kbli) %in% 6:8] Or you might want to convert all the dates to decimal dates (which are just plain numbers) and use that to make a tibble. kbliDD &lt;- decimal_date(kbli) temp_tb &lt;- tibble(decimalDate = kbliDD, TEMP = as.vector(kbli$TEMP)) temp_tb %&gt;% ggplot(mapping = aes(x=decimalDate,y=TEMP)) + geom_line() + labs(x=&quot;Date&quot;,y=expression(degree~C), title=&quot;Average Daily Temperature, Bellingham WA&quot;) + theme_minimal() Most often I use lubridate for making date and time objects from characters or decimal dates. E.g., date_decimal(kbliDD) would go back to a date from the numbers. Or if you had characters of dates like “2009-02-10” you could make that a date via ymd(\"2009-02-10\"). You will encounter dates in an Excel spreadsheet at some point. And when that happens, even the most religious of people will declare that there is no god. Remember that lubridate can help. 2.9 Tidy it Up We can also work with these objects in the tidyverse but we need to convert the zoo object into a tibble. There are a few ways of doing this (e.g., like the example above) but the tidy function from broom does a nice job of making a long tibble from a zoo object. Note that I’m not loading the whole broom library here but just grabbing the function tidy. kbli_tb &lt;- broom::tidy(kbli) glimpse(kbli_tb) ## Rows: 18,262 ## Columns: 3 ## $ index &lt;date&gt; 2000-01-01, 2000-01-01, 2000-01-02, 2000-01-02, 2000-01-03, 20… ## $ series &lt;chr&gt; &quot;TEMP&quot;, &quot;PRCP&quot;, &quot;TEMP&quot;, &quot;PRCP&quot;, &quot;TEMP&quot;, &quot;PRCP&quot;, &quot;TEMP&quot;, &quot;PRCP&quot;,… ## $ value &lt;dbl&gt; 5.300000e+00, 1.219000e+01, 3.600000e+00, 1.270000e+00, 1.10000… This is similar to what we did above. And like the above, we can now treat this like any other tibble. kbli_tb %&gt;% mutate(series = recode(series, PRCP = &quot;Precipitation (mm)&quot;, TEMP = &quot;Temperature (C)&quot;)) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + facet_wrap(~series,scales = &quot;free_y&quot;,ncol=1) + labs(x=&quot;Date&quot;,y=element_blank(), title=&quot;Average Temperature and Precipitation, Bellingham WA&quot;) + theme_minimal() Also note that this works with ts objects as well. co2_tb &lt;- broom::tidy(co2) co2_tb %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Date&quot;,y=expression(CO[2]~(ppm))) I really hate use base plotting and will probably convert back and forth a good deal. You are welcome to use base plotting (e.g., plot) or ggplot. 2.10 Your Work That was a lot to deal with. I know. This week, let’s try to get comfortable reading in a time series from the web and working with it as a tibble, ts, and zoo objects. 2.10.1 Berkeley Earth Temperature Data The data I’ll give you are global temperature anomalies (in degrees C) for land surfaces from the Berkeley Earth group. This is one of the half dozen or so research groups that maintains global climate data sets in real time. “Global Land Only (1750 – Recent)” is the original source and you can read about how the data are created and formatted. This is the important bit from the top of the file: This file contains a detailed summary of the land-surface average results produced by the Berkeley Averaging method. Temperatures are in Celsius and reported as anomalies relative to the Jan 1951-Dec 1980 average. Uncertainties represent the 95% confidence interval for statistical and spatial undersampling effects. The time index is monthly and starts in 1750. So the first observation is January 1, 1750. Let’s go get the data. Below, I’ll download it from the web, save it into a file (that file goes into whatever your working directory is), and then read it. fname &lt;- &quot;https://berkeley-earth-temperature.s3.us-west-1.amazonaws.com/Global/Complete_TAVG_complete.txt&quot; download.file(fname, &quot;data/Complete_TAVG_complete.txt&quot;) tAnoms &lt;- read_table(&quot;data/Complete_TAVG_complete.txt&quot;, skip = 35,na = &quot;NaN&quot;,col_names = FALSE) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_double(), ## X2 = col_double(), ## X3 = col_double(), ## X4 = col_double(), ## X5 = col_double(), ## X6 = col_double(), ## X7 = col_double(), ## X8 = col_double(), ## X9 = col_double(), ## X10 = col_double(), ## X11 = col_double(), ## X12 = col_double() ## ) We will just keep the first three columns of the data which are the year, month, and temperature anomaly. tAnoms &lt;- tAnoms[,1:3] %&gt;% rename(Year = X1, Month = X2, degC = X3) glimpse(tAnoms) ## Rows: 3,300 ## Columns: 3 ## $ Year &lt;dbl&gt; 1750, 1750, 1750, 1750, 1750, 1750, 1750, 1750, 1750, 1750, 1750… ## $ Month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9… ## $ degC &lt;dbl&gt; -0.252, -1.261, 0.225, 0.288, -0.970, -0.573, 0.390, 0.947, -0.4… I’ll toss out the data prior to 1851 as well. The uncertainty is pretty large before that time because there weren’t a ton of thermometers contributing to this global record prior to the mid-19th century. tAnoms &lt;- tAnoms %&gt;% filter(Year &gt; 1850) glimpse(tAnoms) ## Rows: 2,088 ## Columns: 3 ## $ Year &lt;dbl&gt; 1851, 1851, 1851, 1851, 1851, 1851, 1851, 1851, 1851, 1851, 1851… ## $ Month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9… ## $ degC &lt;dbl&gt; -0.012, -0.618, -1.146, -1.051, -0.299, -0.238, 0.058, -0.277, -… Ok. We have tAnoms which is a standard tibble. This will suit many of our needs but we will need the ts and zoo classes from time to time. So, let’s make ts and zoo objects from the data. tAnoms_ts &lt;- ts(tAnoms$degC,start=c(1851,1),frequency = 12) tAnoms_zoo &lt;- zoo(tAnoms$degC, order.by = as.Date(paste(tAnoms$Year,&quot;-&quot;,tAnoms$Month,&quot;-&quot;,&quot;1&quot;,sep=&quot;&quot;))) Now we have three different objects all containing the same data. We have objects that are class tibble, ts, and zoo. Ideally you will be comfortable working with all three of these objects. E.g., figure out how to plot, summarize the all the objects. For instance, here is a plot of just August temperatures using each data set. Note how I used month from lubridate in one instance and cycle in another just to show you that there are many different ways of doing things. tAnoms %&gt;% filter(Month == 8) %&gt;% ggplot(mapping = aes(x=Year,y=degC)) + geom_line() + labs(y=expression(Anomaly~degree*C), title = &quot;August Temperatures&quot;, caption = &quot;Anomalies relative to the Jan 1951-Dec 1980 average&quot;) + theme_minimal() plot(tAnoms_zoo[month(tAnoms_zoo)==8], xlab=&quot;Year&quot;,ylab=expression(Anomaly~degree*C), main = &quot;August Temperatures&quot;, sub = &quot;Anomalies relative to the Jan 1951-Dec 1980 average&quot;) augtAnoms_ts &lt;- ts(tAnoms_ts[cycle(tAnoms_ts)==8],start=1851) plot(augtAnoms_ts, xlab=&quot;Year&quot;,ylab=expression(Anomaly~degree*C), main = &quot;August Temperatures&quot;, sub = &quot;Anomalies relative to the Jan 1951-Dec 1980 average&quot;) 2.10.2 Your Turn Get somewhat comfy with basic data manipulation. Try to get a feel for how the times are stored in each class. When you are ready, do two things with each class (tibble, ts, and zoo). First, create and plot an annual mean temperature anomaly from the monthly data (e.g., 1851 will have one value that is the average of 12 temperatures from January to December). Second, report the trend in the data using the slope coefficient from a linear model. And if you are feeling spunky, try an optional third step: add a five-year centered moving average to each plot. I want you to do these with each class (tibble, ts, and zoo) because we will encounter all them as we move forward. Your plots, slopes, etc. should look the same for each class. I’ll start you off by showing one way to take these monthly data and make them annual. tAnomsAnn &lt;- tAnoms %&gt;% group_by(Year) %&gt;% summarise(degC = mean(degC)) tAnomsAnn_ts &lt;- aggregate.ts(tAnoms_ts, nfrequency = 1, FUN = mean) tAnomsAnn_zoo &lt;- aggregate(tAnoms_zoo, by = year,FUN = mean) When I fit a linear model I find that the annual trend is 0.01 degrees C per year. That’s a degree per century. Yikes. I’ve hidden my code, but here is what I came up with in terms of plots. 2.10.3 Write Up and Reflect Pass in a R Markdown doc with those plots. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? We will consider irregular time series later in the the class but not deal with them too often.↩︎ "],["an-aside-understanding-methods-and-generics-in-r.html", "3 An Aside: Understanding Methods and Generics in R 3.1 You’ve Come a Long Way in R 3.2 Why Use Generics? 3.3 How This Works 3.4 What’s the Difference Between a Generic and a Method? 3.5 Common Generic Functions 3.6 How to Explore and Understand Methods 3.7 Why This Matters", " 3 An Aside: Understanding Methods and Generics in R 3.1 You’ve Come a Long Way in R In your journey through the curriculum, you’ve learned a lot of R. You can wrangle data, fit models, make plots, write functions, etc. But you might also be starting to wonder: What’s actually going on under the hood? For example, how does summary() know what to do when you give it the output from lm? Or why does plot() sometimes give you a histogram and other times a scatterplot? A good way to start answering those questions is to learn about methods and generic functions. 3.2 Why Use Generics? It’s nice when a function does the right thing depending on the kind of object you give it. You don’t want to memorize different function names for every kind of model—you just want to call summary(), plot(), or print() and get something useful. That’s what generic functions are for. In R, many common functions like summary(), plot(), and print() are generic—they automatically call the version that matches the class of your object. Let’s run a linear model and use that as an example. fit &lt;- lm(mpg ~ wt, data = mtcars) Ok. We have a new object called fit that is a list with a bunch of stuff in it that authors of the lm function think will be useful to you. Look at its structure to see what’s in there. str(fit) ## List of 12 ## $ coefficients : Named num [1:2] 37.29 -5.34 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;wt&quot; ## $ residuals : Named num [1:32] -2.28 -0.92 -2.09 1.3 -0.2 ... ## ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... ## $ effects : Named num [1:32] -113.65 -29.116 -1.661 1.631 0.111 ... ## ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;(Intercept)&quot; &quot;wt&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:32] 23.3 21.9 24.9 20.1 18.9 ... ## ..- attr(*, &quot;names&quot;)= chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... ## $ assign : int [1:2] 0 1 ## $ qr :List of 5 ## ..$ qr : num [1:32, 1:2] -5.657 0.177 0.177 0.177 0.177 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:32] &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... ## .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;wt&quot; ## .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ## ..$ qraux: num [1:2] 1.18 1.05 ## ..$ pivot: int [1:2] 1 2 ## ..$ tol : num 1e-07 ## ..$ rank : int 2 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ df.residual : int 30 ## $ xlevels : Named list() ## $ call : language lm(formula = mpg ~ wt, data = mtcars) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ wt ## .. ..- attr(*, &quot;variables&quot;)= language list(mpg, wt) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;mpg&quot; &quot;wt&quot; ## .. .. .. ..$ : chr &quot;wt&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;wt&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, wt) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mpg&quot; &quot;wt&quot; ## $ model :&#39;data.frame&#39;: 32 obs. of 2 variables: ## ..$ mpg: num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## ..$ wt : num [1:32] 2.62 2.88 2.32 3.21 3.44 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language mpg ~ wt ## .. .. ..- attr(*, &quot;variables&quot;)= language list(mpg, wt) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] &quot;mpg&quot; &quot;wt&quot; ## .. .. .. .. ..$ : chr &quot;wt&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;wt&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(mpg, wt) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mpg&quot; &quot;wt&quot; ## - attr(*, &quot;class&quot;)= chr &quot;lm&quot; That list has 12 things in it including the coefficients, residuals, fitted values and a lot more. But getting the information you want in terms of interpreting the linear model isn’t obvious. For instance, the object fit doesn’t directly contain the \\(R^2\\) or p-values. Those are computed when you call summary(fit). summary(fit) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 Ok. Check that out. We got a really nice formatted output that helps us interpret the linear model using the summary function. But note that the output of summary(fit) is really different than getting a summary of a numeric vector like the mpg in the mtcars data that went into the model. summary(mtcars$mpg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.40 15.43 19.20 20.09 22.80 33.90 3.2.1 Same Function, Different Behavior — What Gives? We used the same function (summary) and got two very different results. Answer? Generics and methods. When we used summary(fit), we’re not calling some one-size-fits-all summary function. We’re calling a specific method written for objects of class \"lm\". 3.3 How This Works Here’s what R does when you call summary(fit): It checks the class of the object (class(fit)). It looks for a function named summary.classname—in this case, summary.lm. If it finds that method, it runs it. If not, it falls back to summary.default(). You can look under the hood: class(fit) ## [1] &quot;lm&quot; methods(summary) ## [1] summary.aov summary.aovlist* ## [3] summary.aspell* summary.check_packages_in_dir* ## [5] summary.connection summary.data.frame ## [7] summary.Date summary.default ## [9] summary.difftime summary.ecdf* ## [11] summary.factor summary.glm ## [13] summary.infl* summary.lm ## [15] summary.loess* summary.manova ## [17] summary.matrix summary.mlm* ## [19] summary.nls* summary.packageStatus* ## [21] summary.POSIXct summary.POSIXlt ## [23] summary.ppr* summary.prcomp* ## [25] summary.princomp* summary.proc_time ## [27] summary.rlang_error* summary.rlang_message* ## [29] summary.rlang_trace* summary.rlang_warning* ## [31] summary.rlang:::list_of_conditions* summary.srcfile ## [33] summary.srcref summary.stepfun ## [35] summary.stl* summary.table ## [37] summary.tukeysmooth* summary.warnings ## see &#39;?methods&#39; for accessing help and source code summary.lm ## function (object, correlation = FALSE, symbolic.cor = FALSE, ## ...) ## { ## z &lt;- object ## p &lt;- z$rank ## rdf &lt;- z$df.residual ## if (p == 0) { ## r &lt;- z$residuals ## n &lt;- length(r) ## w &lt;- z$weights ## if (is.null(w)) { ## rss &lt;- sum(r^2) ## } ## else { ## rss &lt;- sum(w * r^2) ## r &lt;- sqrt(w) * r ## } ## resvar &lt;- rss/rdf ## ans &lt;- z[c(&quot;call&quot;, &quot;terms&quot;, if (!is.null(z$weights)) &quot;weights&quot;)] ## class(ans) &lt;- &quot;summary.lm&quot; ## ans$aliased &lt;- is.na(coef(object)) ## ans$residuals &lt;- r ## ans$df &lt;- c(0L, n, length(ans$aliased)) ## ans$coefficients &lt;- matrix(NA_real_, 0L, 4L, dimnames = list(NULL, ## c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;t value&quot;, &quot;Pr(&gt;|t|)&quot;))) ## ans$sigma &lt;- sqrt(resvar) ## ans$r.squared &lt;- ans$adj.r.squared &lt;- 0 ## ans$cov.unscaled &lt;- matrix(NA_real_, 0L, 0L) ## if (correlation) ## ans$correlation &lt;- ans$cov.unscaled ## return(ans) ## } ## if (is.null(z$terms)) ## stop(&quot;invalid &#39;lm&#39; object: no &#39;terms&#39; component&quot;) ## if (!inherits(object, &quot;lm&quot;)) ## warning(&quot;calling summary.lm(&lt;fake-lm-object&gt;) ...&quot;) ## Qr &lt;- qr.lm(object) ## n &lt;- NROW(Qr$qr) ## if (is.na(z$df.residual) || n - p != z$df.residual) ## warning(&quot;residual degrees of freedom in object suggest this is not an \\&quot;lm\\&quot; fit&quot;) ## r &lt;- z$residuals ## f &lt;- z$fitted.values ## if (!is.null(z$offset)) { ## f &lt;- f - z$offset ## } ## w &lt;- z$weights ## if (is.null(w)) { ## mss &lt;- if (attr(z$terms, &quot;intercept&quot;)) ## sum((f - mean(f))^2) ## else sum(f^2) ## rss &lt;- sum(r^2) ## } ## else { ## mss &lt;- if (attr(z$terms, &quot;intercept&quot;)) { ## m &lt;- sum(w * f/sum(w)) ## sum(w * (f - m)^2) ## } ## else sum(w * f^2) ## rss &lt;- sum(w * r^2) ## r &lt;- sqrt(w) * r ## } ## resvar &lt;- rss/rdf ## if (is.finite(resvar) &amp;&amp; resvar &lt; (mean(f)^2 + var(c(f))) * ## 1e-30) ## warning(&quot;essentially perfect fit: summary may be unreliable&quot;) ## p1 &lt;- 1L:p ## R &lt;- chol2inv(Qr$qr[p1, p1, drop = FALSE]) ## se &lt;- sqrt(diag(R) * resvar) ## est &lt;- z$coefficients[Qr$pivot[p1]] ## tval &lt;- est/se ## ans &lt;- z[c(&quot;call&quot;, &quot;terms&quot;, if (!is.null(z$weights)) &quot;weights&quot;)] ## ans$residuals &lt;- r ## ans$coefficients &lt;- cbind(Estimate = est, `Std. Error` = se, ## `t value` = tval, `Pr(&gt;|t|)` = 2 * pt(abs(tval), rdf, ## lower.tail = FALSE)) ## ans$aliased &lt;- is.na(z$coefficients) ## ans$sigma &lt;- sqrt(resvar) ## ans$df &lt;- c(p, rdf, NCOL(Qr$qr)) ## if (p != attr(z$terms, &quot;intercept&quot;)) { ## df.int &lt;- if (attr(z$terms, &quot;intercept&quot;)) ## 1L ## else 0L ## ans$r.squared &lt;- mss/(mss + rss) ## ans$adj.r.squared &lt;- 1 - (1 - ans$r.squared) * ((n - ## df.int)/rdf) ## ans$fstatistic &lt;- c(value = (mss/(p - df.int))/resvar, ## numdf = p - df.int, dendf = rdf) ## } ## else ans$r.squared &lt;- ans$adj.r.squared &lt;- 0 ## ans$cov.unscaled &lt;- R ## dimnames(ans$cov.unscaled) &lt;- dimnames(ans$coefficients)[c(1, ## 1)] ## if (correlation) { ## ans$correlation &lt;- (R * resvar)/outer(se, se) ## dimnames(ans$correlation) &lt;- dimnames(ans$cov.unscaled) ## ans$symbolic.cor &lt;- symbolic.cor ## } ## if (!is.null(z$na.action)) ## ans$na.action &lt;- z$na.action ## class(ans) &lt;- &quot;summary.lm&quot; ## ans ## } ## &lt;bytecode: 0x154ef4d80&gt; ## &lt;environment: namespace:stats&gt; getS3method(&quot;summary&quot;, &quot;lm&quot;) ## function (object, correlation = FALSE, symbolic.cor = FALSE, ## ...) ## { ## z &lt;- object ## p &lt;- z$rank ## rdf &lt;- z$df.residual ## if (p == 0) { ## r &lt;- z$residuals ## n &lt;- length(r) ## w &lt;- z$weights ## if (is.null(w)) { ## rss &lt;- sum(r^2) ## } ## else { ## rss &lt;- sum(w * r^2) ## r &lt;- sqrt(w) * r ## } ## resvar &lt;- rss/rdf ## ans &lt;- z[c(&quot;call&quot;, &quot;terms&quot;, if (!is.null(z$weights)) &quot;weights&quot;)] ## class(ans) &lt;- &quot;summary.lm&quot; ## ans$aliased &lt;- is.na(coef(object)) ## ans$residuals &lt;- r ## ans$df &lt;- c(0L, n, length(ans$aliased)) ## ans$coefficients &lt;- matrix(NA_real_, 0L, 4L, dimnames = list(NULL, ## c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;t value&quot;, &quot;Pr(&gt;|t|)&quot;))) ## ans$sigma &lt;- sqrt(resvar) ## ans$r.squared &lt;- ans$adj.r.squared &lt;- 0 ## ans$cov.unscaled &lt;- matrix(NA_real_, 0L, 0L) ## if (correlation) ## ans$correlation &lt;- ans$cov.unscaled ## return(ans) ## } ## if (is.null(z$terms)) ## stop(&quot;invalid &#39;lm&#39; object: no &#39;terms&#39; component&quot;) ## if (!inherits(object, &quot;lm&quot;)) ## warning(&quot;calling summary.lm(&lt;fake-lm-object&gt;) ...&quot;) ## Qr &lt;- qr.lm(object) ## n &lt;- NROW(Qr$qr) ## if (is.na(z$df.residual) || n - p != z$df.residual) ## warning(&quot;residual degrees of freedom in object suggest this is not an \\&quot;lm\\&quot; fit&quot;) ## r &lt;- z$residuals ## f &lt;- z$fitted.values ## if (!is.null(z$offset)) { ## f &lt;- f - z$offset ## } ## w &lt;- z$weights ## if (is.null(w)) { ## mss &lt;- if (attr(z$terms, &quot;intercept&quot;)) ## sum((f - mean(f))^2) ## else sum(f^2) ## rss &lt;- sum(r^2) ## } ## else { ## mss &lt;- if (attr(z$terms, &quot;intercept&quot;)) { ## m &lt;- sum(w * f/sum(w)) ## sum(w * (f - m)^2) ## } ## else sum(w * f^2) ## rss &lt;- sum(w * r^2) ## r &lt;- sqrt(w) * r ## } ## resvar &lt;- rss/rdf ## if (is.finite(resvar) &amp;&amp; resvar &lt; (mean(f)^2 + var(c(f))) * ## 1e-30) ## warning(&quot;essentially perfect fit: summary may be unreliable&quot;) ## p1 &lt;- 1L:p ## R &lt;- chol2inv(Qr$qr[p1, p1, drop = FALSE]) ## se &lt;- sqrt(diag(R) * resvar) ## est &lt;- z$coefficients[Qr$pivot[p1]] ## tval &lt;- est/se ## ans &lt;- z[c(&quot;call&quot;, &quot;terms&quot;, if (!is.null(z$weights)) &quot;weights&quot;)] ## ans$residuals &lt;- r ## ans$coefficients &lt;- cbind(Estimate = est, `Std. Error` = se, ## `t value` = tval, `Pr(&gt;|t|)` = 2 * pt(abs(tval), rdf, ## lower.tail = FALSE)) ## ans$aliased &lt;- is.na(z$coefficients) ## ans$sigma &lt;- sqrt(resvar) ## ans$df &lt;- c(p, rdf, NCOL(Qr$qr)) ## if (p != attr(z$terms, &quot;intercept&quot;)) { ## df.int &lt;- if (attr(z$terms, &quot;intercept&quot;)) ## 1L ## else 0L ## ans$r.squared &lt;- mss/(mss + rss) ## ans$adj.r.squared &lt;- 1 - (1 - ans$r.squared) * ((n - ## df.int)/rdf) ## ans$fstatistic &lt;- c(value = (mss/(p - df.int))/resvar, ## numdf = p - df.int, dendf = rdf) ## } ## else ans$r.squared &lt;- ans$adj.r.squared &lt;- 0 ## ans$cov.unscaled &lt;- R ## dimnames(ans$cov.unscaled) &lt;- dimnames(ans$coefficients)[c(1, ## 1)] ## if (correlation) { ## ans$correlation &lt;- (R * resvar)/outer(se, se) ## dimnames(ans$correlation) &lt;- dimnames(ans$cov.unscaled) ## ans$symbolic.cor &lt;- symbolic.cor ## } ## if (!is.null(z$na.action)) ## ans$na.action &lt;- z$na.action ## class(ans) &lt;- &quot;summary.lm&quot; ## ans ## } ## &lt;bytecode: 0x154ef4d80&gt; ## &lt;environment: namespace:stats&gt; 3.4 What’s the Difference Between a Generic and a Method? A generic function is the function you call—like summary() or plot(). It doesn’t actually do the work itself. Instead, it decides which method to run based on the class of the object. A method is the specific function written for a particular class. For example: summary.lm() handles linear models. summary.data.frame() handles data frames. summary.default() is the fallback if no class-specific method is found. You can look at the generic directly: summary ## function (object, ...) ## UseMethod(&quot;summary&quot;) ## &lt;bytecode: 0x154efcac8&gt; ## &lt;environment: namespace:base&gt; This call to UseMethod(\"summary\") tells R to dispatch to the right method depending on the class of the object. 3.5 Common Generic Functions Here are some other generic functions you’re likely to run into: print() – display a human-readable version summary() – provide a richer summary plot() – show plots predict() – make predictions residuals() – return residuals fitted() – return fitted values coef() – extract coefficients update() – refit a model with changes AIC() / BIC() – compare model fit anova() – compare nested models terms() / formula() – extract parts of model structure 3.6 How to Explore and Understand Methods Want to know what’s going on behind a function call? Here are a few tools: methods(summary) ## [1] summary.aov summary.aovlist* ## [3] summary.aspell* summary.check_packages_in_dir* ## [5] summary.connection summary.data.frame ## [7] summary.Date summary.default ## [9] summary.difftime summary.ecdf* ## [11] summary.factor summary.glm ## [13] summary.infl* summary.lm ## [15] summary.loess* summary.manova ## [17] summary.matrix summary.mlm* ## [19] summary.nls* summary.packageStatus* ## [21] summary.POSIXct summary.POSIXlt ## [23] summary.ppr* summary.prcomp* ## [25] summary.princomp* summary.proc_time ## [27] summary.rlang_error* summary.rlang_message* ## [29] summary.rlang_trace* summary.rlang_warning* ## [31] summary.rlang:::list_of_conditions* summary.srcfile ## [33] summary.srcref summary.stepfun ## [35] summary.stl* summary.table ## [37] summary.tukeysmooth* summary.warnings ## see &#39;?methods&#39; for accessing help and source code methods(plot) ## [1] plot.acf* plot.data.frame* plot.decomposed.ts* ## [4] plot.default plot.dendrogram* plot.density* ## [7] plot.ecdf plot.factor* plot.formula* ## [10] plot.function plot.hclust* plot.histogram* ## [13] plot.HoltWinters* plot.isoreg* plot.lm* ## [16] plot.medpolish* plot.mlm* plot.ppr* ## [19] plot.prcomp* plot.princomp* plot.profile* ## [22] plot.profile.nls* plot.R6* plot.raster* ## [25] plot.spec* plot.stepfun plot.stl* ## [28] plot.table* plot.ts plot.tskernel* ## [31] plot.TukeyHSD* ## see &#39;?methods&#39; for accessing help and source code methods(class = &quot;lm&quot;) ## [1] add1 alias anova case.names coerce ## [6] confint cooks.distance deviance dfbeta dfbetas ## [11] drop1 dummy.coef effects extractAIC family ## [16] formula hatvalues influence initialize kappa ## [21] labels logLik model.frame model.matrix nobs ## [26] plot predict print proj qr ## [31] residuals rstandard rstudent show simulate ## [36] slotsFromS3 summary variable.names vcov ## see &#39;?methods&#39; for accessing help and source code getS3method(&quot;summary&quot;, &quot;lm&quot;) ## function (object, correlation = FALSE, symbolic.cor = FALSE, ## ...) ## { ## z &lt;- object ## p &lt;- z$rank ## rdf &lt;- z$df.residual ## if (p == 0) { ## r &lt;- z$residuals ## n &lt;- length(r) ## w &lt;- z$weights ## if (is.null(w)) { ## rss &lt;- sum(r^2) ## } ## else { ## rss &lt;- sum(w * r^2) ## r &lt;- sqrt(w) * r ## } ## resvar &lt;- rss/rdf ## ans &lt;- z[c(&quot;call&quot;, &quot;terms&quot;, if (!is.null(z$weights)) &quot;weights&quot;)] ## class(ans) &lt;- &quot;summary.lm&quot; ## ans$aliased &lt;- is.na(coef(object)) ## ans$residuals &lt;- r ## ans$df &lt;- c(0L, n, length(ans$aliased)) ## ans$coefficients &lt;- matrix(NA_real_, 0L, 4L, dimnames = list(NULL, ## c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;t value&quot;, &quot;Pr(&gt;|t|)&quot;))) ## ans$sigma &lt;- sqrt(resvar) ## ans$r.squared &lt;- ans$adj.r.squared &lt;- 0 ## ans$cov.unscaled &lt;- matrix(NA_real_, 0L, 0L) ## if (correlation) ## ans$correlation &lt;- ans$cov.unscaled ## return(ans) ## } ## if (is.null(z$terms)) ## stop(&quot;invalid &#39;lm&#39; object: no &#39;terms&#39; component&quot;) ## if (!inherits(object, &quot;lm&quot;)) ## warning(&quot;calling summary.lm(&lt;fake-lm-object&gt;) ...&quot;) ## Qr &lt;- qr.lm(object) ## n &lt;- NROW(Qr$qr) ## if (is.na(z$df.residual) || n - p != z$df.residual) ## warning(&quot;residual degrees of freedom in object suggest this is not an \\&quot;lm\\&quot; fit&quot;) ## r &lt;- z$residuals ## f &lt;- z$fitted.values ## if (!is.null(z$offset)) { ## f &lt;- f - z$offset ## } ## w &lt;- z$weights ## if (is.null(w)) { ## mss &lt;- if (attr(z$terms, &quot;intercept&quot;)) ## sum((f - mean(f))^2) ## else sum(f^2) ## rss &lt;- sum(r^2) ## } ## else { ## mss &lt;- if (attr(z$terms, &quot;intercept&quot;)) { ## m &lt;- sum(w * f/sum(w)) ## sum(w * (f - m)^2) ## } ## else sum(w * f^2) ## rss &lt;- sum(w * r^2) ## r &lt;- sqrt(w) * r ## } ## resvar &lt;- rss/rdf ## if (is.finite(resvar) &amp;&amp; resvar &lt; (mean(f)^2 + var(c(f))) * ## 1e-30) ## warning(&quot;essentially perfect fit: summary may be unreliable&quot;) ## p1 &lt;- 1L:p ## R &lt;- chol2inv(Qr$qr[p1, p1, drop = FALSE]) ## se &lt;- sqrt(diag(R) * resvar) ## est &lt;- z$coefficients[Qr$pivot[p1]] ## tval &lt;- est/se ## ans &lt;- z[c(&quot;call&quot;, &quot;terms&quot;, if (!is.null(z$weights)) &quot;weights&quot;)] ## ans$residuals &lt;- r ## ans$coefficients &lt;- cbind(Estimate = est, `Std. Error` = se, ## `t value` = tval, `Pr(&gt;|t|)` = 2 * pt(abs(tval), rdf, ## lower.tail = FALSE)) ## ans$aliased &lt;- is.na(z$coefficients) ## ans$sigma &lt;- sqrt(resvar) ## ans$df &lt;- c(p, rdf, NCOL(Qr$qr)) ## if (p != attr(z$terms, &quot;intercept&quot;)) { ## df.int &lt;- if (attr(z$terms, &quot;intercept&quot;)) ## 1L ## else 0L ## ans$r.squared &lt;- mss/(mss + rss) ## ans$adj.r.squared &lt;- 1 - (1 - ans$r.squared) * ((n - ## df.int)/rdf) ## ans$fstatistic &lt;- c(value = (mss/(p - df.int))/resvar, ## numdf = p - df.int, dendf = rdf) ## } ## else ans$r.squared &lt;- ans$adj.r.squared &lt;- 0 ## ans$cov.unscaled &lt;- R ## dimnames(ans$cov.unscaled) &lt;- dimnames(ans$coefficients)[c(1, ## 1)] ## if (correlation) { ## ans$correlation &lt;- (R * resvar)/outer(se, se) ## dimnames(ans$correlation) &lt;- dimnames(ans$cov.unscaled) ## ans$symbolic.cor &lt;- symbolic.cor ## } ## if (!is.null(z$na.action)) ## ans$na.action &lt;- z$na.action ## class(ans) &lt;- &quot;summary.lm&quot; ## ans ## } ## &lt;bytecode: 0x154ef4d80&gt; ## &lt;environment: namespace:stats&gt; class(fit) ## [1] &quot;lm&quot; 3.7 Why This Matters Understanding how generics and methods work helps you read other people’s code, write better functions, and make sense of R’s behavior when things feel a little magic. "],["decomposition.html", "4 Decomposition 4.1 Big Idea 4.2 Reading 4.3 Packages 4.4 Seasonality and decomposition 4.5 Stationarity 4.6 Your Work", " 4 Decomposition 4.1 Big Idea We can model seasonal signals and find trends in time series. 4.2 Reading Read Chapter one from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great book for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 4.3 Packages You’ll want zoo (Zeileis, Grothendieck, and Ryan 2025), for the assignment and tidyverse (Wickham 2023). Note that I often use base R syntax rather than tidy with time series. This is mostly because the formats are most often used for time-series data (e.g., zoo and ts) don’t work as naturally with tidy syntax. Not using ggplot is a special hell for me. So you’ll see that I often break and go back to ggplot and use tidyverse syntax like mutate etc. library(tidyverse) library(zoo) 4.4 Seasonality and decomposition 4.4.1 Breaking down a series the tedious way In my work I routinely use time series that have a seasonal component. E.g., monthly weather data. Environmental time series often have a periodic component that relates to geophysics or biology in some way. This is called “seasonal” whether it relates to actual seasons (e.g., summer) or not. Let’s look at some data on atmospheric carbon dioxide (aka the Keeling curve) that have a long-term trend as well as a seasonal component and figure out a way to break it down. We already saw the onboard data set co2 which only goes to 1998 (it’s frozen in time for reproducibility). So let’s get the most up-to-date data from NOAA. fname &lt;- &quot;https://www.esrl.noaa.gov/gmd/webdata/ccgg/trends/co2/co2_mm_mlo.txt&quot; co2 &lt;- read.table(fname) co2 &lt;- co2[,4] co2 &lt;- ts(co2,start=c(1959,3),frequency = 12) co2 &lt;- window(co2,start=c(1960,1), end=c(2022,12)) data(co2) tsp(co2) #compare to print(tsp(co2),8) ## [1] 1959.000 1997.917 12.000 plot(co2,ylab=expression(CO[2]~(ppm))) We’ve seen these data before. Note that these data are in the ts class and are sampled monthly with freq=12 in the properties. Plotting is the best way to explore a time series. But we can get a little more formal about these data. It’s pretty clear that this time series has a trend and a seasonal component. There is some noise on top of that. We can write this out as an additive time series model: \\[y_t = m_t + s_t + z_t\\] where at time \\(t\\), \\(y_t\\) is the observed series, \\(m_t\\) is the trend in the data, \\(s_t\\) is the seasonal component, and \\(z_t\\) is the residual noise or error. Definitely read the part in the chapter about multiplicative time series models – we will see a few of those along the way. There are a few different ways to approach decomposing these data. We could get the trend (\\(m\\)) by fitting a model as a function of time or we could calculate a moving average of some kind that would remove the seasonal component (\\(s\\)). Let’s do the second approach as this will let us see the parts of the trend that aren’t purely linear (like we would get from fitting a model like lm(y~time(y))). A centered moving average (aka a moving average with symmetric window) will let us estimate the trend. \\[\\hat m_t = \\frac{\\frac{1}{2}y_{t-6} + y_{t-5} + y_{t-4} + y_{t-3} + y_{t-2} + y_{t-1} + y_{t} + y_{t+1} + y_{t+2}+ y_{t+3}+ y_{t+4}+ y_{t+5}+ \\frac{1}{2}y_{t+6}}{12}\\] where \\(t = 7, 8,...., n-6\\) where \\(n\\) is the length of \\(y\\). We can implement this using a loop where we start on the 7th observation of the co2 data (to keep the indexing happy) and calculate each value of \\(\\hat m\\). We’ll lose the fist six and last six observations. n &lt;- length(co2) # length of the record m.hat &lt;- rep(NA,n) # an empty vector to store results for(i in 7:(n-6)){ m.hat[i] &lt;- (0.5*co2[i-6] + co2[i-5] + co2[i-4] + co2[i-3] + co2[i-2] + co2[i-1] + co2[i] + co2[i+1] + co2[i+2] + co2[i+3] + co2[i+4] + co2[i+5] + 0.5*co2[i+6])/12 } m.hat &lt;- ts(m.hat) # make it a time series # give it the right start time and frequency by copying tsp() from co2 tsp(m.hat) &lt;- tsp(co2) plot(m.hat,ylab=expression(CO[2]~(ppm))) # here is the trend. The code above gave us an estimate of the trend (we put a hat on estimates by convention). But this is a bit ugly, right? Loops are often that way. But it’s important to see what is happening. We are stepping through the data and applying the moving average looking back six time steps and forward six time steps. At each iteration we write the value of the moving average to the ith space in the empty vector m.hat. While it’s good to know how this stuff works there are better ways of doing this and you’ve already probably thought of this: We don’t have to this with a loop. Let’s do this the easy way with the filter function. Note that if you have tidyverse located you might have a conflict between dpylr::filter from the tidyverse and stats::filter which is part of base R. We want stats::filter here. Watch and remember those messages when you load a package! f &lt;- frequency(co2) filt &lt;- c(0.5, rep(1, times=f-1), 0.5)/f m.hat &lt;- stats::filter(x=co2, filter=filt) # note stats::filter That’s also \\(\\hat m\\) but done with quite a bit less pain. But make sure you understand what the loop code did above. What about the seasonal component \\(s\\)? We can get an estimate (\\(\\hat s\\)) by subtraction: \\(\\hat s_t = y_t - \\hat m_t\\). s.hat &lt;- co2 - m.hat par(mfcol=c(2,1)) plot(s.hat,ylab=expression(CO[2]~(ppm))) boxplot(s.hat~cycle(s.hat),xlab=&quot;Month&quot;,ylab=expression(CO[2]~(ppm))) So, via subtraction we got an estimate of the seasonal component and by making boxplots by month we see the draw down of CO\\(\\mathrm{_2}\\) during the North American summer and the rise in winter. Thus at the moment we have a nice model of the this time series: \\(y_t=m_t+s_t\\) but we don’t have any residual noise \\(z_t\\). I’m going to show you how we can calculate the residual if we assume that the seasonal cycle is the same every year. I’ll do that by calculating the mean of \\(\\hat s\\) for each month and treat deviations from the mean as the noise term \\(\\hat z\\). s.hat &lt;- aggregate(s.hat~cycle(s.hat),FUN=mean) # length of 12 s.hat &lt;- rep(s.hat$s.hat,length(co2)/12) # repeat this component for each year s.hat &lt;- ts(s.hat) # make it a ts object tsp(s.hat) &lt;- tsp(co2) # thus noise is z.hat &lt;- co2 - m.hat - s.hat plot(z.hat) At some point when building a model you have to ask if it’s “good” or at least “useful.” We calculated the residuals (errors) on the decomposed time series as \\(\\hat z\\). Just like in a regression model or a machine learning model or any model really, we can examine the residuals for patterns, to see if they are normally distributed, and/or look at the mean squared error (MSE) or the root mean squared error (RMSE) # MSE mean(z.hat^2,na.rm = TRUE) ## [1] 0.0701273 # RMSE sqrt(mean(z.hat^2,na.rm = TRUE)) ## [1] 0.2648156 If we were comparing this decomposition to a different method we could compare the MSE or RMSE as a way of deciding which one was better. We can also combine these how we like and look at the CO\\(\\mathrm{_2}\\) trend and noise. This might be called the deseasonalized CO\\(\\mathrm{_2}\\). co2_deseas &lt;- m.hat + z.hat plot(co2_deseas,ylab=expression(CO[2]~(ppm)),main=&quot;Deseasonalized Carbon Dioxide&quot;) Let’s look at the whole thing by combining each of these parts into a single ts object. Note the scale of the y-axis on each plot (the units are still ppm). co2.hat &lt;- cbind(co2,m.hat,s.hat,z.hat) plot(co2.hat) We just did some fancy model building and broke an additive time series into its constituent parts! Walk through each line of code above and make sure you understand what we did. 4.4.2 Breaking down a series the easy way Now, as much as I’m sure you enjoyed the journey above, you’ll be pleased to know that R has function for doing this much more quickly: co2Decomp &lt;- decompose(co2,type = &quot;additive&quot;) plot(co2Decomp) Pretty cool, huh? The plot shows the original data plus all the components and these can be combined just like we did above. Look at str(co2Decomp). str(co2Decomp) ## List of 6 ## $ x : Time-Series [1:468] from 1959 to 1998: 315 316 316 318 318 ... ## $ seasonal: Time-Series [1:468] from 1959 to 1998: -0.0536 0.6106 1.3756 2.5168 3.0003 ... ## $ trend : Time-Series [1:468] from 1959 to 1998: NA NA NA NA NA ... ## $ random : Time-Series [1:468] from 1959 to 1998: NA NA NA NA NA ... ## $ figure : num [1:12] -0.0536 0.6106 1.3756 2.5168 3.0003 ... ## $ type : chr &quot;additive&quot; ## - attr(*, &quot;class&quot;)= chr &quot;decomposed.ts&quot; See that co2Decomp is a new class (decomposed.ts) and a list with six components. You can assess the components in the normal way (e.g., co2Decomp$trend). Also see that the function reminds us that this is an additive model. The decompose function also works with multiplicative time series: \\[y_t = m_t \\times s_t \\times z_t\\] The book has good stuff on the multiplicative model but also has a typo in Eq 1.3. The equation above is correct. See text for details and the help page for decompose. I don’t want to spoil the fun. 4.4.3 How does m differ from the slope in a lm? Let’s think for a minute about what a “trend” is. Note that we said above that \\(m_t\\) is the trend in the data. We’ve also said that we can use the slope in a linear model to describe the trend. Like this: co2LinearModel &lt;- lm(co2~time(co2)) plot(co2,ylab=expression(CO[2]~(ppm))) abline(co2LinearModel,col=&quot;red&quot;) We can look at those two together. plot(m.hat,ylab=expression(CO[2]~(ppm))) # trend from the decomposition. abline(co2LinearModel,col=&quot;red&quot;) # and the trend from a linear model Take a minute to appreciate how and why these are both trends and how and why they differ. 4.4.4 Warning Computers just do what they are told. The decompose function will find a trend and seasonal component even in data that don’t have one. I’m going to create a time series of “monthly” data that runs 10 years starting in the year 2010. Then I’ll decompose it. y &lt;- ts(rnorm(120),start=c(2010,1),frequency = 12) plot(decompose(y)) Note that there is NO trend in y – it’s just random numbers. And there is no seasonal structure in y – it’s just random numbers. But decompose was very content to pull out the trend and seasonal parts of the data. Note the magnitude of the y-axes and you can see that there isn’t a lot of there there. But, if you didn’t know better you might obsess over this plot and trick yourself into finding patterns where none exist. 4.5 Stationarity I linked to a nice video on stationarity in time series. It’s worth a watch and clearer than the book. I’ll spare you much more than that statement for now and not ask you to wade through any code. But be aware that data with a persistent trend are not stationary, nor are data with changing variance, nor data which have a seasonal component. E.g., look at the AirPassengers data below. It’s a classic time series that have been used to teach generations of time-series students and is featured in the reading. It fails on all three aspects of stationarity! data(AirPassengers) plot(AirPassengers) We will talk more in the future about when you should test for stationarity – maybe because a test you are using a model that assumes stationarity. And when you should just note it as a descriptive feature of a time series – remember, your eye is a really good diagnostic tool. 4.6 Your Work 4.6.1 Air Passengers First, look at the AirPassengers data above. Decompose it with an additive model and then a multiplicative model. Report on what you have found. Pay attention to the the units on the multiplicative model. Compare the two models using MSE (again, watch the units). 4.6.2 KBLI Second, revisit the climate data from the Bellingham airport from last week. Note the use of ggplot – which I prefer for my own work. kbli &lt;- readRDS(&quot;data/kbli.rds&quot;) kbli_tb &lt;- broom::tidy(kbli) kbli_tb &lt;- kbli_tb %&gt;% mutate(series = recode(series, PRCP = &quot;Precipitation (mm)&quot;, TEMP = &quot;Temperature (C)&quot;)) kbli_tb %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Date&quot;,y=element_blank()) + facet_wrap(~series,scales = &quot;free&quot;,ncol=1) + theme_minimal() Then (with good reporting, plotting, and other exploratory data analysis) try this: Aggregate the variables (temperature and precipitation) to monthly resolution using mean for temperature and sum for precipitation. You’ll probably want to do this using aggregate with as.yearmon – see the examples in aggregate.zoo. The hydroTSM package has some nice functions as well for this kind of thing (e.g., daily2monthly). Decompose these monthly data. The function decompose works on class ts and not on class zoo though, so you’ll have to change the class of the data. Report on what you find for the decomposition. Finally, aggregate the variables (temperature and precipitation) to annual resolution for each season (winter, spring, summer, fall) using mean for temperature and sum for precipitation. Use linear models and report any trends. E.g., are summer temperatures increasing? Meteorological seasons are groups of three months. I.e., winter is Dec, Jan, Feb; spring is Mar, Apr, May; summer is Jun, Jul, Aug, autumn is Sep, Oct, Nov.  As an example, here is a plot of summer temperatures. You can report the slopes in a table if you like. Easier and clearer than plotting everything probably. 4.6.3 Write Up and Reflect Pass in a R Markdown doc with your analysis. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? Are you clear on why \\(m_t\\) from decompose is not the same as the coefficient from a linear model (lm)? "],["arp-acf-pacf-etc..html", "5 AR(p), ACF, PACF, etc. 5.1 Big Idea 5.2 Reading 5.3 Packages 5.4 Correlation and Covariance 5.5 Autocorrelation 5.6 Autocorrelation Function 5.7 The Boring Presentation 5.8 Partial Autocorrelation Function 5.9 AR(1) to AR(p) 5.10 Random walks 5.11 Autocorrelation examples with real data 5.12 Choosing the Order of an AR(p) Model by AIC 5.13 Your work 5.14 Postscript: A Helpful Plotting Chunk", " 5 AR(p), ACF, PACF, etc. 5.1 Big Idea The values in a time series are often not independent – that can be a good thing to find out – understanding what the autocorrelation is like can often tell you a lot about what the underlying processes looks like. 5.2 Reading Have a look at Chapter two from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great book for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 5.3 Packages I’ll use tidyverse (Wickham 2023) some. And we will use the forecast (Hyndman et al. 2025) and gridExtra (Auguie 2017) package as well. library(tidyverse) library(forecast) library(gridExtra) 5.4 Correlation and Covariance The book spends a good deal of time on how to define correlation and covariance in the context of time series analysis. Read that stuff over even though you’ve covered it all before. This is important material to be aware of for a few reasons when doing time series but the biggest is the idea of stationarity. A stationary time series is one whose statistical properties such as mean and variance are constant over time and do not follow any trends. A little more formally, that means that a stationary time series is a random process whose joint probability distribution does not change over time. We will come back to this as we get into forecasting. 5.5 Autocorrelation Autocorrelation is a central feature of many time series. It means that values in a time series are correlated with each other as a function of time. For instance, tree-ring data are autocorrelated so that a measurement like ring width in year \\(t\\) is related to year in \\(t-1\\), \\(t-2\\), etc. If you think about the biology of how rings are formed you can imagine the mechanisms at play in terms of stored photosynthate, fine root generation, etc. Here is a simple way of writing a first-order autoregressive process where the prior observation influences the current observation: \\[ y_t = \\phi y_{t-1} + \\epsilon_t\\] where \\(y\\) is a time series, \\(t\\) is a measurement in time, \\(\\phi\\) (usually pronounced as “fie” but in Greece they’d say “fee”) is a coefficient ranging from -1 to 1, and \\(\\epsilon\\) is some other process. The term \\(\\epsilon\\) is typically called the error, the noise or the residuals but in time series lingo you’ll see it called the the random shock sometimes too. We call this a first-order autoregressive model and notate it as AR(1). Note that you’ll see this in some books written out slightly differently as \\(y_t - \\phi y_{t-1} = \\epsilon_t\\). I prefer thinking about it as a regression and so prefer the \\(y\\) term to be by itself on the left hand side of the equation. Operationally the math works out to be easier if the series has a mean of zero. So we typically remove the mean of the data to create a mean-adjusted time series if the series has a mean not equal to zero via \\(y=Y-\\bar{Y}\\). We’ll just stick with data that have a mean of zero for now. For now, let’s generate an AR(1) time series by hand. Later we’ll learn a better way to do this using the arima.sim() function, but until that happy day: n &lt;- 500 x &lt;- 1:n epsilon &lt;- rnorm(n = n,mean = 0,sd = 1) # white noise y &lt;- numeric(length = n) phi &lt;- 0.7 for(i in 2:n){ y[i] &lt;- phi * y[i-1] + epsilon[i] } ggplot() + geom_line(aes(x=x,y=y)) + theme_minimal() Take a look at how we did this. We initialized with a value of zero and then started building y on the second value. We couldn’t start the loop at i=1 because we needed the prior year for an AR(1) model. Keep this in mind when you get to the section for your work below. We could have started the time series with a random number instead of a zero if we were feeling adventurous. E.g., before the loop we could have set y[1]=rnorm(1). Run that loop above and make the plot a half dozen or so times. Get a feel for what that time series looks like. Compare it to uncorrelated times series: plot(rnorm(n),type=\"l\"). 5.6 Autocorrelation Function 5.6.1 The Very Fun Presentation Go read Allison Horst’s on autocorrelation on the autocorrelation function (ACF). She is amazing and I read everything she does with awe. 5.7 The Boring Presentation Fun time is over! How strongly correlated is \\(y_t\\) with \\(y_{t-1}\\)? The answer should be \\(\\rho_{y_t,y_{t-1}} \\approx \\phi\\) as above and we can calculate this via cor(y[2:n],y[1:(n-1)]) which is 0.668 for this particular random seed. The remainder (\\(\\epsilon\\)) is white noise. This is the standard way you are used to seeing correlation between two variables (e.g., \\(x\\) and \\(y\\)) but in this case we are looking at how \\(y\\) correlates with itself at different lags. Thus, more formally we can refer to autocorrelation of \\(\\rho\\) at lag \\(k\\) as: \\[\\rho(k) = \\frac{\\frac{1}{n-k}\\sum_{t=k+1}^n (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{ \\sqrt{\\frac{1}{n}\\sum_{t=1}^n (y_t - \\bar{y})}\\sqrt{\\frac{1}{n-k}\\sum_{t=k+1}^n (y_{t-k} - \\bar{y})}}\\] So how do you assess autocorrelation more generally? You calculate \\(\\rho\\) for successive lags \\(k\\) and plot it. We do this with the function Acf which produces a figure called a correlogram. Note that we are using the function Acf in forecast rather than the canonical acf function in stats. There are a few differences that you can read about (?Acf) but the two functions are essentially similar. Acf(y) This produces a plot of the correlation (\\(r\\)) at lags (\\(k\\)). The autocorrelation value at the first lag (\\(k=1\\)) is \\(\\phi=0.668\\) which is very close to the value of \\(\\phi\\) we set above. But also note that there is autocorrelation at higher lags (\\(k=2, k=3\\)) that follows a decay of \\(\\phi^k\\) (\\(\\phi_2=\\phi^2\\), \\(\\phi_3=\\phi^3\\), etc.). The correlogram provides a great visualization into the autocorrelation structure and can also tell us whether the correlation is significantly different from zero. The assumption is that the correlation at lag \\(k\\) should be zero if the series is an independent and identically distributed random variable. Specifically, the statistic \\(r\\) estimates the true value of the parameter \\(\\rho\\): \\(E(\\rho_k)=-1/N\\) and the variance is \\(Var(\\sigma^2_k)=1/N\\). Thus, the 95% confidence limits for the statistic \\(r\\) in the correlogram can be added to the plot (dashed lines) as \\(0 \\pm 1.96 / \\sqrt N\\). The acf function plots the confidence intervals at a default value of 95% using the argument ci via: ci &lt;- 0.95 qnorm((1 + ci)/2)/sqrt(n) ## [1] 0.08765225 Ok. You should be hip to the idea that by testing 25 lags like we did in the figure above means that having any one given lag \\(k\\) outside the 95% confidence limits is no reason to fire off a paper to Nature. I.e., you might well see a value outside the limits even if the time series is drawn from a random population. The key is interpreting the figure according to the properties of the system. A strong significant autocorrelation at lag 1 makes good sense in many biological or physical systems while an isolated barely significant value at, say, lag 18 is unlikely to be something you should get excited about. Always think in terms of mechanism – does what you see in the data make sense? How can we can extract the actual numbers from the ACF object? Figuring out how to do so might take a little sleuthing on your part. If you look at the help page for acf you’ll see that the output produces an array called acf which contains the information on the correlations at various lags. Running str on the ACF object helps too. As always, str is your friend! Thus: yACF &lt;- Acf(y,plot=FALSE) str(yACF) ## List of 6 ## $ acf : num [1:27, 1, 1] 1 0.667 0.395 0.201 0.106 ... ## $ type : chr &quot;correlation&quot; ## $ n.used: int 500 ## $ lag : num [1:27, 1, 1] 0 1 2 3 4 5 6 7 8 9 ... ## $ series: chr &quot;y&quot; ## $ snames: NULL ## - attr(*, &quot;class&quot;)= chr &quot;acf&quot; yACF$acf[,1,1] ## [1] 1.000000000 0.667381959 0.394898921 0.201185905 0.105832323 ## [6] 0.006835833 -0.031653516 -0.043152324 0.030731602 0.101805417 ## [11] 0.080434533 0.006901371 -0.018551768 0.002020845 0.010443740 ## [16] 0.011214921 -0.024411046 -0.032823730 -0.001342588 0.018718373 ## [21] -0.005275216 -0.036887143 -0.055374925 -0.036574252 -0.004123666 ## [26] -0.006268373 0.012301501 Note that value of 1.0 at lag zero. Ugh. I hate that. It shows that when you correlate \\(y\\) to \\(y\\), the correlation is one. There are reasons I can explain for why that is included but it’s mostly there annoy me I think. So, the data above follow an AR(1) model and the acf plot lets us visualize the autocorrelation structure. 5.7.1 Tidy Another benefit of using Acf over acf is that there is a pre-rolled ggplot function ready to go. ggAcf(y) + theme_minimal() 5.8 Partial Autocorrelation Function 5.8.1 PACF Let’s revisit our observation above that while \\(\\phi\\) affects \\(y_{t-1}\\) we still see autocorrelation at higher lags (\\(k=2, k=3\\)). Why? Because if \\(y_t\\) and \\(y_{t-1}\\) are correlated, then \\(y_{t-1}\\) and \\(y_{t-2}\\) must also be correlated. Thus, \\(y_{t}\\) and \\(y_{t-2}\\) will be correlated because they are both connected to \\(y_{t-1}\\). What we’d like to do next is remove that decaying echo using partial regression. The Pacf function lets us look at the partial autocorrelation which shows conditional correlation at lag \\(k\\) after we take into account effects of the the prior lags. Partial regression lets us measure the linear dependence of one variable after removing the effect of another variable. Thus at lag 2, the partial autocorrelation of measures the effect (linear dependence) of \\(y_{t-2}\\) on \\(y_t\\) after removing the effect of that \\(y_{t-1}\\) has on \\(y_{t-2}\\). Our expectation for y is that after taking the AR(1) model into account the correlation at subsequent lags should fall to zero. In other words, if we have a parsimonious model for a first-order process the partial correlations should be very small. Let’s look: ggPacf(y) + theme_minimal() Interpreting ACF plots is pretty straightforward, but what does the PACF plot mean? Well, if you have an AR(1) process the remaining autocorrelation at higher lags will be near zero in your PACF plot. That is, if the true process is AR(1), all autocorrelation is removed from the model – even the residual autocorrelation from the geometric decrease seen at higher lags in the ACF plot. Thus, if you have an AR(1) process, the ACF decays geometrically while the PACF falls to zero (zero’ish) abruptly after lag 1. If you still see a geometric progression decline in a PACF plot you might be looking at different process, like an MA(1) process. We will get there next week. 5.8.2 Underneath the Hood of PACF How does PACF work? How does it relate to the ACF values? How you can get a feel for what PACF is all about? Good questions. So, let’s take a look under the hood of the Pacf function so to speak. The most straightforward answer involves unpacking the idea that the sample autocorrelations and the partial autocorrelations relate via the Durbin-Levinson recursion. But I’ll show an example here that gives the idea using the coefficients from a linear model. Recall \\(y\\) from the last section. Now, let’s warm up by doing the autocorrelation “by hand” using the cor function as well as the Acf function: yACF &lt;- Acf(y,plot=FALSE) # ACF for lag 1 and lag 2 round(yACF$acf[2,,],2) ## [1] 0.67 round(yACF$acf[3,,],2) ## [1] 0.39 # Done by hand yLag1 &lt;- c(y[-1],NA) yLag2 &lt;- c(y[-c(1,2)],NA,NA) round(cor(yLag1,y,use = &quot;complete.obs&quot;),2) ## [1] 0.67 round(cor(yLag2,y,use = &quot;complete.obs&quot;),2) ## [1] 0.4 The coefs differ a little after the third decimal point due to the way that the cor calculates the degrees of freedom vs Acf (I think). But the differences are small. We can put this into a loop and create the ACF plot. yACFv2 &lt;- numeric(length = 25) for(i in 1:25){ yLag &lt;- y[-c(1:i)] yClip &lt;- y[1:length(yLag)] yACFv2[i] &lt;- cor(yClip, yLag) } par(mfcol=c(1,2)) Acf(y,ylim = c(-0.1,0.8),main=&quot;ACF done via `Acf`&quot;) plot(1:25,yACFv2,type=&quot;h&quot;,main=&quot;ACF done by hand&quot;,ylim = c(-0.1,0.8)) abline(h=0) cis &lt;- qnorm((1 + 0.95)/2)/sqrt(n) abline(h=c(-cis,cis),lty=&quot;dashed&quot;,col=&quot;blue&quot;) Now we can do the same for the partial coefficients which we get using a regression analysis through the origin. yPACF &lt;- Pacf(y,plot=FALSE) # PACF coefs for lag 1 and lag 2 round(yPACF$acf[1,,],2) ## [1] 0.67 round(yPACF$acf[2,,],2) ## [1] -0.09 # Done by hand yPACF_Lag1 &lt;- lm(y ~ yLag1 - 1) round(coef(yPACF_Lag1)[1],2) ## yLag1 ## 0.67 yPACF_Lag2 &lt;- lm(y ~ yLag1 + yLag2 - 1) round(coef(yPACF_Lag2)[2],2) ## yLag2 ## -0.09 We can recreate the PACF plot as well. It’s a little more involved. Walk through it and you’ll figure it out. But the key is that we have to build a matrix of lagged predictors as we increase the order. yPACFv2 &lt;- numeric(length = 25) for(j in 2:25){ nCols &lt;- j nRows &lt;- length(y) - j + 1 yLagMat &lt;- matrix(0, nRows, j) for(i in 1:nCols){ yLagMat[ ,i] &lt;- y[i : (i + nRows - 1)] } lm1 &lt;- lm(yLagMat[,1] ~ yLagMat[,-1] - 1) yPACFv2[j] &lt;- coef(lm1)[j - 1] } par(mfcol=c(1,2)) Pacf(y,ylim = c(-0.1,0.8),main=&quot;PACF done via `Pacf`&quot;) plot(1:25,yPACFv2,type=&quot;h&quot;,main=&quot;PACF done by hand&quot;,ylim = c(-0.1,0.8)) abline(h=0) cis &lt;- qnorm((1 + 0.95)/2)/sqrt(n) abline(h=c(-cis,cis),lty=&quot;dashed&quot;,col=&quot;blue&quot;) Just in case anybody really wants the details, the function pacf does this with linear algebra by solving the Toeplitz matrix. The details of this are significantly past the level of detail we typically go into: yACFcoef &lt;- yACF$acf[,,1] yPACFv3 &lt;- numeric(length = 26) for(i in 1:26){ yPACFv3[i] &lt;- solve(toeplitz(yACFcoef[1:i]), yACFcoef[2:(i+1)])[i] } yPACFv3 # PACF by hand version 3 ## [1] 0.667381959 -0.091055966 -0.046528939 0.025185709 -0.091022904 ## [6] 0.015966667 -0.001923789 0.121385775 0.063258196 -0.089661989 ## [11] -0.073835268 0.023019938 0.053175213 0.003885418 0.013368725 ## [16] -0.069177973 -0.015116848 0.042310786 0.015775388 -0.020538866 ## [21] -0.040782006 -0.038322575 0.023721661 0.043779099 -0.013104062 ## [26] 0.045485723 yPACF$acf[,,1] # and the original from the pacf function ## [1] 0.667381959 -0.091055966 -0.046528939 0.025185709 -0.091022904 ## [6] 0.015966667 -0.001923789 0.121385775 0.063258196 -0.089661989 ## [11] -0.073835268 0.023019938 0.053175213 0.003885418 0.013368725 ## [16] -0.069177973 -0.015116848 0.042310786 0.015775388 -0.020538866 ## [21] -0.040782006 -0.038322575 0.023721661 0.043779099 -0.013104062 ## [26] 0.045485723 5.9 AR(1) to AR(p) The AR(1) concept is extendable as an AR(p) model to incorporate multiple time steps as: \\[ y_t = \\sum\\limits_{i=1}^p \\phi_i y_{t-i} + \\epsilon_t \\] where \\(p\\) denotes the order of the model. So an AR(2) model would be \\(y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t\\) and if \\(p=3\\) we’d have \\(y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\phi_3 y_{t-3} + \\epsilon_t\\). We’ll do more on higher order models later including how to fit them and how to interpret them. But for now get used to the idea that higher order models have more memory. 5.10 Random walks A random walk is a special kind of AR1 autocorrelation where the observation at \\(t-1\\) is the jumping off point for the observation at \\(t\\). That means \\(\\phi=1\\) and the time series is not stationary. Rather, the time series “walks.” Because \\(\\phi=1\\) we can leave it out of the equation: \\[ y_t = y_{t-1} + \\epsilon_t\\] A drift term, \\(\\delta\\), can be added so that a random walk model with drift is: \\[ y_t = \\delta + y_{t-1} + \\epsilon_t\\] Run this code a few times and appreciate the plots. Are these models stationary? Here is a random walk without drift: n &lt;- 100 x &lt;- 1:n y &lt;- numeric(length = n) for(i in 2:n){ y[i] &lt;- y[i-1] + rnorm(1) } ggplot() + geom_line(aes(x=x,y=y)) And with drift: y &lt;- numeric(length = n) drift &lt;- 0.2 for(i in 2:n){ y[i] &lt;- drift + y[i-1] + rnorm(1) } ggplot() + geom_line(aes(x=x,y=y)) Oh, here is a question for you to answer – you should be able to spit this answer out without having to calculate anything. What would the Acf() and Pacf() plots look like for these data? How would it change if you increase \\(n\\)? 5.10.1 Stationarity is dead That’s the title of a famous paper in hydrology. It’s controversial and something that has influenced a lot of discussion in the global change community. Look back up at the random walk section above. A random walk is not stationary. Use some very simple algebra (just rearrange) and show with an equation how make \\(y_t = y_{t-1} + \\epsilon_t\\) stationary. Why is stationarity desirable? 5.11 Autocorrelation examples with real data Let’s look a at few examples of autocorrelation with data sets that are already part of R. You’ll work with your own later. 5.11.1 Nile First, let’s look at the Nile data which has measurements of the annual flow of the river Nile from 1871–1970. The units are in 10\\(^8\\) m\\(^3\\). data(Nile) broom::tidy(Nile) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Date&quot;,y=expression(10^8~m^3), title=&quot;River Nile Flow&quot;) + theme_minimal() Is this river flow record autocorrelated? ggAcf(Nile) + theme_minimal() And the PACF ggPacf(Nile) + theme_minimal() Looking at the plot of the original time series it does look like its possible that high flow years follow high flow years and low years follow low years (positive autocorrelation) but it’s hard to say for sure. However the ACF and PACF plots show that there is an AR(1) model at play here. Like our simulated data above the PACF shows no significant autocorrelation at a lag of two years after accounting for the lag at one year. Thus, we can probably say that this record follows a process: \\(y_t = \\phi_1 y_{t-1} + \\epsilon_t\\). 5.11.2 Air Temperature Next, there is a data set of mean annual temperature in degrees Fahrenheit in New Haven, Connecticut, from 1912 to 1971. data(nhtemp) broom::tidy(nhtemp) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Date&quot;,y=expression(degree~F), title=&quot;Mean Annual Temp, New Haven, Connecticut&quot;) + theme_minimal() Is this temperature record autocorrelated? ggAcf(nhtemp) + theme_minimal() And the PACF ggPacf(nhtemp) + theme_minimal() Unlike what we have above, the ACF and PACF plots show that there is likely an AR(2) model at play here. Note that the PACF shows significant autocorrelation at a lag of two years even after accounting for the lag at one year. Thus, we can probably say that this temperature record follows a process: \\(y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t\\). See below for how we might estimate those coefficients. 5.11.3 Sunspots That stuff above is all pretty straightforward. But what about a time series with a cyclical nature? Take a look at a data set of annual sunspot counts from 1700 to 1988. Load and plot the data. data(sunspot.year) broom::tidy(sunspot.year) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Date&quot;,y=&quot;n&quot;, title=&quot;Annual Sunspot Count&quot;) + theme_minimal() This is a whole different level of autocorrelation! Note how the series goes up and then down in a cycle. This means that there will be both positive and negative autocorrelation depending on the lag. ggAcf(sunspot.year) Take some time to wrap your head around this idea. The data are positively autocorrelated for a few years, then negatively autocorrelated, and so on. It should make sense once you realize that sunspots numbers go up and down on an approximately 11-year cycle due to internal hydromagnetic processes. Figuring out what the dominant periodicities are in a time series falls under the realm of studying time series in the frequency domain. We’ll get to that later. 5.12 Choosing the Order of an AR(p) Model by AIC Above, we used the ACF and PACF plots to get a feel for the appropriate order p for an AR(p) model. This graphical approach is important for building intuition. However, you do not have to guess the order just by eye. You can formally select the best model using a criterion like AIC. Before we move on, it is important to be clear about what the ACF and an AR(p) model are actually estimating: The ACF shows correlations between observations at different lags. It measures how strongly \\(y_t\\) and \\(y_{t-k}\\) move together. An AR(p) model fits regression coefficients. It estimates how much past values predict \\(y_t\\), accounting for the effects of earlier lags. Although correlation and regression are related, they are not the same: - A correlation measures association without controlling for anything else. - A regression coefficient measures the direct effect of one predictor while controlling for other predictors. In an AR(p) model, each coefficient shows the effect of a lagged value after adjusting for the others. In R, the ar function fits an AR model and selects the order p automatically: By default, it uses maximum likelihood estimation (MLE) to fit the model (get the coefficients). It chooses the order that minimizes the AIC. Among other things, the ar function returns: The selected order. The estimated AR coefficients. For example, we can fit an AR model to the nhtemp data: # Fit an AR model to nhtemp ar_nhtemp &lt;- ar(nhtemp) ar_nhtemp ## ## Call: ## ar(x = nhtemp) ## ## Coefficients: ## 1 2 ## 0.2183 0.3067 ## ## Order selected 2 sigma^2 estimated as 1.353 In this case, ar uses AIC to select the “best” p and estimates the corresponding regression coefficients. Compare that to the ACF plot above for the nhtemp data. Note: The ar function will estimate the coefficients in several different ways. Typically you have learned to use OLS in regression. For reasons we don’t need to get into ar uses MLE. MLE and OLS both estimate AR models, but they differ slightly: OLS fits by minimizing the sum of squared residuals. MLE fits by maximizing the likelihood of the observed data under the model, assuming a normal distribution. MLE is generally more efficient, especially for small samples. I strongly recommend reading the help page for ar. 5.13 Your work 5.13.1 ACF and PACF with different values of \\(\\phi\\). The code below will simulate four different AR(1) time series with different values of \\(\\phi\\). All of the time series have the exact same residuals (\\(\\epsilon\\)). Plot each series and then plot the ACF and PACF plots for each (12 plots total). Think about your expectations for the graphs before you plot them. Interpret. n &lt;- 100 x &lt;- 1:n epsilon &lt;- rnorm(n = n,mean = 0,sd = 1) # white noise y1 &lt;- numeric(length = n) y2 &lt;- numeric(length = n) y3 &lt;- numeric(length = n) y4 &lt;- numeric(length = n) for(i in 2:n){ y1[i] &lt;- 0.95 * y1[i-1] + epsilon[i] y2[i] &lt;- 0.75 * y2[i-1] + epsilon[i] y3[i] &lt;- 0.50 * y3[i-1] + epsilon[i] y4[i] &lt;- 0.25 * y4[i-1] + epsilon[i] } 5.13.1.1 Extra challenge Can you generate the same kind of AR(1) simulation for y1, y2, y3,and y4 using the arima.sim function? 5.13.2 Roll your own AR(2) Use a loop and simulate your own AR(2) time series with \\(\\mu=0,\\sigma=1\\). You’ll need variables \\(\\phi_1\\) and \\(\\phi_2\\). Set them to \\(\\phi_1=0.3\\) and \\(\\phi_2=0.5\\) to start. Then play with those values. Plot the series and look at the acf and pacf. Think about your expectations for the graphs before you plot them. Interpret. 5.13.2.1 Extra challenge Fit a model to your AR(2) using ar. 5.13.3 Sockeye I’ve been lucky enough to get involved in a ridiculously awesome project in Alaska that looks at how returning salmon fertilize the riparian areas surrounding the stream. As part of that project, which invovled multiple decades of throwing dead salmon around, I became aware of this incredibly detailed time series of sockeye salmon escapement data at Hansen Creek. Hansen Creek is a small (about 4m wide and 10cm deep) in the Wood River system of Bristol Bay, southwestern Alaska, USA (59.317 N,-158.695 E). These data show the total number of salmon returning to spawn in the creek from 1950 onward. The file HansenSockeye.rds is on Canvas. These data are class ts with frequency of one. You read it in with the readRDS function e.g. sock &lt;- readRDS(\"data/HansenSockeye.rds\"). Take a look at the time series and it’s autocorrelation patterns. As a cool hint as to what is going on, sockeye salmon spend 2-3 years in the ocean plus 1-2 years in freshwater to return as 3-5 year-olds to spawn. It’s unclear whether the choice to spend three or five years before returning to spawn is a function of ocean conditions, local population biology/behavior, or some combination, However, otoliths from the fish caresses at Hansen Creek suggest that most of the salmon that are returning four years old. What can you infer about this record in terms of the intersection between the fish ecology and the escapement time-series properties? 5.13.4 Write Up and Reflect Pass in a R Markdown doc with your analysis. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? 5.14 Postscript: A Helpful Plotting Chunk Here is a simple way of making a plot of a time series and its respective ACF and PACF plots. It uses the gridExtra package to put the three plots all together. There are lots of ways of doing this. I will often use the cowplot (Wilke 2024) package and there are many other options. n &lt;- 500 x &lt;- 1:n epsilon &lt;- rnorm(n = n,mean = 0,sd = 1) # white noise y &lt;- numeric(length = n) phi &lt;- 0.6 for(i in 2:n){ y[i] &lt;- phi * y[i-1] + epsilon[i] } simpleLayoutMatrix &lt;- matrix(c(1,1,2,3),nrow = 2,ncol = 2,byrow = TRUE) p1 &lt;- ggplot() + geom_line(aes(x=x,y=y)) + labs(x=&quot;time&quot;) + theme_minimal() p2 &lt;- ggAcf(y) + labs(title=element_blank()) + theme_minimal() p3 &lt;- ggPacf(y) + labs(title=element_blank()) + theme_minimal() grid.arrange(p1,p2,p3, layout_matrix=simpleLayoutMatrix) "],["aside-why-the-acf-and-pacf-look-the-way-they-do-in-an-ma1-process.html", "6 Aside: Why the ACF and PACF Look the Way They Do in an MA(1) Process 6.1 Introduction 6.2 ACF and PACF of a Single MA(1) Simulation 6.3 Simulation Study Across Many MA(1) Coefficients 6.4 Insight 1: PACF Lag 1 Underestimates \\(\\theta\\) 6.5 Insight 2: PACF Lag 2 is Consistently Negative 6.6 Conclusion", " 6 Aside: Why the ACF and PACF Look the Way They Do in an MA(1) Process 6.1 Introduction At this point, you are already familiar with autoregressive (AR) and moving average (MA) models, and how they differ in terms of what drives dependencies in a time series. The focus here is on deepening your understanding of what the diagnostic plots (ACF and PACF) actually reveal in a simple MA(1) setting. A MA(1) (moving average process of order 1) is a foundational model in time series analysis: \\[ y_t = \\theta \\epsilon_{t-1} + \\epsilon_t \\] Here, \\(\\epsilon_t\\) are white noise errors, and \\(\\theta\\) is the MA coefficient. Even though the model doesn’t directly depend on past values of \\(y_t\\), its structure introduces short-term correlation through the overlap of shock terms. We use the term “shock” to emphasize that each \\(\\epsilon_t\\) represents a new, unanticipated influence on the system — something that wasn’t predictable from past values and that directly moves the time series. This document explores why the autocorrelation function (ACF) and partial autocorrelation function (PACF) behave the way they do in MA(1) processes. We focus on two unintuitive and commonly misunderstood features: The PACF at lag 1 tends to underestimate the true \\(\\theta\\). The PACF at lag 2 is typically negative, despite the model only including a lag-1 term. We use simulation to build intuition and illustrate these patterns. 6.2 ACF and PACF of a Single MA(1) Simulation Let’s start by making a MA(1) time series. library(tidyverse) library(forecast) library(gridExtra) set.seed(2346) n &lt;- 500 tm &lt;- 1:n y &lt;- arima.sim(model = list(ma = 0.4), n = n) simpleLayoutMatrix &lt;- matrix(c(1, 1, 2, 3), nrow = 2, byrow = TRUE) p1 &lt;- ggplot() + geom_line(aes(x = tm, y = y)) + labs(x = &quot;Time&quot;) p2 &lt;- ggAcf(y) + labs(title = NULL) p3 &lt;- ggPacf(y) + labs(title = NULL) grid.arrange(p1, p2, p3, layout_matrix = simpleLayoutMatrix) The time series plot shows typical short-term wiggliness — a feature of MA processes driven by shocks. The ACF shows a sharp drop-off after lag 1, which is the signature of an MA(1). The PACF, however, does something subtler: a bump at lag 1, and a surprising dip below zero at lag 2. That’s the focus of this document — understanding why this happens and what it tells us. This simulation shows a typical MA(1) pattern: ACF: significant spike at lag 1 followed by rapid drop-off PACF: small positive spike at lag 1, followed by a negative value at lag 2 6.3 Simulation Study Across Many MA(1) Coefficients The code below builds a large data frame of simulation results. For each randomly chosen MA(1) coefficient (theta), we simulate a time series, compute its PACF, and extract the values at lag 1 and lag 2. This lets us explore how the PACF behaves systematically as theta varies. We will simulate 1000 MA(1) series with \\(\\theta \\in [0.2, 0.9]\\) to examine systematic behavior in the PACF. m &lt;- 1000 # Number of simulations res &lt;- matrix(NA, m, 3) # Object to store theta, PACF lag 1, PACF lag 2 thetas &lt;- runif(m, 0.2, 0.9) # Randomly draw MA(1) coefficients for simulation res[,1] &lt;- thetas # Store theta for(i in 1:m){ y &lt;- arima.sim(model = list(ma = thetas[i]), n = n) # Simulate MA(1) tmp &lt;- Pacf(y, plot = FALSE) # Get PACF res[i,2] &lt;- tmp$acf[1,1,1] # Store lag 1 PACF res[i,3] &lt;- tmp$acf[2,1,1] # Store lag 2 PACF } res &lt;- as.data.frame(res) names(res) &lt;- c(&quot;Simulated_theta&quot;, &quot;PACF_lag1&quot;, &quot;PACF_lag2&quot;) 6.4 Insight 1: PACF Lag 1 Underestimates \\(\\theta\\) This plot compares the true MA(1) coefficient used in each simulation (theta) to the estimated PACF value at lag 1. If the PACF exactly reflected the true coefficient, all points would fall along the dashed 1:1 line. Instead, we see a consistent pattern of underestimation — the PACF at lag 1 grows more slowly than theta, especially as theta increases. This visualizes how the PACF captures indirect correlation, not the actual model parameter. ggplot(res, aes(x = Simulated_theta, y = PACF_lag1)) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dashed&quot;) + geom_point(alpha = 0.4, color = &quot;gray&quot;) + labs(x = expression(theta[1]), y = &quot;PACF Lag 1&quot;, title = &quot;PACF Lag 1 Underestimates the True MA(1) Coefficient&quot;, caption = &quot;Dashed line shows the 1:1 reference line&quot;) + coord_cartesian(xlim = c(0.15, 0.9), ylim = c(0.15, 0.9)) + theme_minimal() 6.4.1 Why This Happens The PACF at lag 1 is the coefficient from regressing \\(y_t\\) on \\(y_{t-1}\\). In an MA(1) model, \\(y_t\\) and \\(y_{t-1}\\) both include \\(\\epsilon_{t-1}\\), creating a correlation. But this correlation is weaker than the actual MA coefficient \\(\\theta\\), because it is shared noise, not a structural dependency. A structural dependency means that one value directly influences another — for example, in an AR(1) model, \\(y_{t-1}\\) directly enters the equation for \\(y_t\\). Shared noise, by contrast, means that two values are correlated because they both include the same random error term. In an MA(1), \\(y_t\\) and \\(y_{t-1}\\) are correlated not because one causes the other, but because both include _{t-1}. The correlation between \\(y_t\\) and \\(y_{t-1}\\) in an MA(1) model is: \\[ \\text{ACF}(1) = \\frac{\\theta}{1 + \\theta^2} \\] This function grows sublinearly and is always smaller than \\(\\theta\\). So when you regress \\(y_t\\) on \\(y_{t-1}\\), the PACF at lag 1 ends up underestimating the true coefficient. This is not bias in a statistical sense — it’s just that the PACF is measuring something different from \\(\\theta\\). The ACF(1) formula comes from calculating the expected covariance between \\(y_t\\) and \\(y_{t-1}\\), then dividing by the variance. Both \\(y_t\\) and \\(y_{t-1}\\) share \\(\\epsilon_{t-1}\\), which creates a positive correlation. The denominator includes both \\(\\epsilon_t\\) and \\(\\theta \\epsilon_{t-1}\\), leading to the extra \\(\\theta^2\\) in the variance. Our simulation confirms this: as \\(\\theta\\) increases, PACF lag 1 rises too, but never reaches the 1:1 line. 6.5 Insight 2: PACF Lag 2 is Consistently Negative This plot shows a strong inverse relationship between PACF lag 1 and lag 2. As PACF lag 1 increases (which happens as theta increases), PACF lag 2 becomes more negative. This pattern reflects how the model’s one-lag memory indirectly creates compensating effects at lag 2 once lag 1 has been accounted for. ggplot(res, aes(x = PACF_lag1, y = PACF_lag2)) + geom_point(alpha = 0.4,color = &quot;gray&quot;) + labs(x = &quot;PACF Lag 1&quot;, y = &quot;PACF Lag 2&quot;, title = &quot;PACF Lag 2 Is More Negative as Lag 1 Increases&quot;, caption = &quot;Each point is a simulated MA(1) process&quot;) + theme_minimal() Here we plot PACF lag 2 against the true theta used in each simulation. The consistent downward slope illustrates that the more influence theta has at lag 1, the more negative the partial correlation becomes at lag 2 — an indirect, but systematic pattern that is unique to MA(1) models. # Also show vs theta ggplot(res, aes(x = Simulated_theta, y = PACF_lag2)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_point(alpha = 0.4, color = &quot;gray&quot;) + geom_smooth(method = &quot;lm&quot;) + labs(x = expression(True~theta[1]), y = expression(PACF~Lag~2~(hat(theta[2]))), title = &quot;PACF Lag 2 Is Systematically Negative&quot;, caption = &quot;This behavior is a hallmark of MA(1) structure&quot;) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 6.5.1 Why This Happens The PACF at lag 2 is the partial correlation between \\(y_t\\) and \\(y_{t-2}\\), holding \\(y_{t-1}\\) constant. In an MA(1), there is no direct influence of \\(y_{t-2}\\) on \\(y_t\\), but they do share overlapping shock terms: \\(y_{t-2}\\) includes \\(\\epsilon_{t-2}\\), which can influence \\(y_{t-1}\\), which in turn influences \\(y_t\\) through \\(\\epsilon_{t-1}\\). So once the regression adjusts for \\(y_{t-1}\\), the residual effect of \\(y_{t-2}\\) is actually in the opposite direction, resulting in a negative coefficient. The strength of this negative partial correlation increases with \\(\\theta\\), leading to a nearly linear negative slope, as oour plot confirms. Theoretically, the PACF value at lag 2 in an MA(1) process has a known form: \\[ \\phi_{22} = \\frac{-\\theta^2}{1 + \\theta^2} \\] Here, \\(\\phi_{22}\\) refers to the partial autocorrelation at lag 2 — specifically, it is the coefficient on \\(y_{t-2}\\) in a regression of \\(y_t\\) on both \\(y_{t-1}\\) and \\(y_{t-2}\\). More generally, \\(\\phi_{kk}\\) denotes the PACF at lag \\(k\\), and it captures the unique contribution of \\(y_{t-k}\\) to predicting \\(y_t\\), after controlling for all intermediate lags. This negative value at lag 2 reflects how the MA(1) structure produces a compensating reversal in direction once the influence of \\(y_{t-1}\\) is accounted for. The notation \\(\\phi_{22}\\) refers to the partial autocorrelation at lag 2 — that is, the coefficient on \\(y_{t-2}\\) in a regression of \\(y_t\\) on both \\(y_{t-1}\\) and \\(y_{t-2}\\). More generally, \\(\\phi_{kk}\\) denotes the PACF at lag \\(k\\), and comes from the Durbin-Levinson algorithm used to estimate partial autocorrelations. It captures the unique contribution of \\(y_{t-k}\\) to predicting \\(y_t\\), after controlling for all intermediate lags \\(y_{t-1}, y_{t-2}, \\ldots, y_{t-(k-1)}\\). This matches the shape in our simulation — a negative curve that grows more negative with larger \\(\\theta\\). 6.6 Conclusion This simulation-driven analysis shows: PACF lag 1 underestimates \\(\\theta\\) because it captures indirect correlation from shared noise, not a structural relationship. PACF lag 2 is consistently negative due to the way overlapping shocks in an MA(1) structure reverse direction when accounting for intermediate lags. Recognizing these patterns helps interpret the ACF and PACF of moving average processes more accurately — and avoids common missteps when students treat PACF values as direct analogues of model parameters. "],["armapq.html", "7 ARMA(p,q) 7.1 Big Idea 7.2 Reading 7.3 Packages 7.4 AR(p) to MA(q) 7.5 ARMA(p,q) 7.6 What about ARIMA(p, d, q)? 7.7 Evaluating ARMA models 7.8 Your work", " 7 ARMA(p,q) 7.1 Big Idea The values in a time series are often not independent – that can be a good thing to find out – understanding what the autocorrelation is like can often tell you a lot about what the underlying processes looks like. 7.2 Reading Have a look at Chapter six from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great book for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 7.3 Packages I’ll use tidyverse (Wickham 2023) some. And we will use the forecast (Hyndman et al. 2025) package as well. I’ll grab a function or two from gridExtra (Auguie 2017) and broom (Robinson, Hayes, and Couch 2025) as well. library(tidyverse) library(forecast) 7.4 AR(p) to MA(q) Last week, we described the auto regressive AR(\\(p\\)) model where the values of \\(y\\) are a function of the prior values of \\(y\\). We can extend this to more time steps and call this an autoregressive model of order \\(p\\) as AR(\\(p\\)): \\[ y_t = \\sum\\limits_{i=1}^p \\phi_i y_{t-i} + \\epsilon_t \\] Thus, an AR(2) model would be \\(y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t\\) where parameters are as we’ve previously defined them. The moving average (MA) model is a similar concept to the AR(\\(p\\)) model but this time the coefficient (\\(\\theta\\) instead of \\(\\phi\\)) affects the residuals and not \\(y\\) itself. The time series therefore is an unevenly weighted moving average of the residuals \\(\\epsilon_t\\). The first-order moving average, or MA(1), model is given by \\(y_t = \\theta_1 e_{t-1} + \\epsilon_t\\) where \\(\\theta\\) is a coefficient ranging from -1 to 1. \\[y_t = \\theta \\epsilon_{t-1} + \\epsilon_t\\] But, we can never observe the values of \\(\\epsilon_t\\) in real time (they are errors, right?), so it is not really a regression in the usual sense. Let’s look at an implementation of a MA(1) process with a loop. I’m going to plot the data, the ACF and the PACF all in one figure. n &lt;- 100 x &lt;- 1:n theta &lt;- 0.8 epsilon &lt;- rnorm(n=n) y &lt;- numeric(length = n) for(i in 2:n){ y[i] &lt;- theta * epsilon[i-1] + epsilon[i] } simpleLayoutMatrix &lt;- matrix(c(1,1,2,3),nrow = 2,ncol = 2,byrow = TRUE) p1 &lt;- ggplot() + geom_line(aes(x=x,y=y)) + labs(x=&quot;Time&quot;) p2 &lt;- ggAcf(y) + labs(title=element_blank()) p3 &lt;- ggPacf(y) + labs(title=element_blank()) gridExtra::grid.arrange(p1,p2,p3, layout_matrix=simpleLayoutMatrix) This is an interesting pattern and characteristic of a MA(1) model. Note the strong first order autocorrelation. It doesn’t have the decay that you have in an AR(1) model. That is characteristic of the MA processes. The PACF shows a strong negative correlation at lag 2 and then oscillates. This is characteristic behavior of the MA process as well but the reasons take a little work to wrap your head around. This is the best explanation I’ve seen for why this occurs that doesn’t invovle a bunch of linear algebra. As with AR(\\(p\\)), we can extend this to longer lags and call this MA(\\(q\\)): \\[ y_t = \\sum\\limits_{i=1}^q \\theta_i \\epsilon_{t-i} + \\epsilon_t\\] 7.5 ARMA(p,q) What’s next? You guessed it an ARMA(\\(p,q\\)). The ARMA model is simply the merger between AR(\\(p\\)) and MA(\\(q\\)) models. The order of the ARMA model is given in parentheses where \\(p\\) is the autoregressive order and \\(q\\) the moving-average order. A common model is the ARMA(1,1) model \\(y_t = \\phi_1 y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t\\). But this can be extended to longer lags as above: \\[ y_t = \\sum\\limits_{i=1}^p \\phi_i y_{t-i} + \\sum\\limits_{i=1}^q \\theta_i \\epsilon_{t-i} + \\epsilon_t\\] Let’s look at an ARMA(1,1) model via simulation: n &lt;- 100 x &lt;- 1:n epsilon &lt;- rnorm(n=n) y &lt;- numeric(length = n) phi &lt;- 0.5 theta &lt;- 0.8 for(i in 2:n){ y[i] &lt;- phi * y[i-1] + theta * epsilon[i-1] + epsilon[i] } Here is a plot of the time series, the ACF and the PACF. p1 &lt;- ggplot() + geom_line(aes(x=x,y=y)) + labs(x=&quot;Time&quot;) p2 &lt;- ggAcf(y) + labs(title=element_blank()) p3 &lt;- ggPacf(y) + labs(title=element_blank()) gridExtra::grid.arrange(p1,p2,p3,layout_matrix=simpleLayoutMatrix) Now, in general we like to use models like this so that we can better understand the system by revealing something about the mechanisms that create the persistence in the data. In the case of the ARMA(1,1) model you can imagine a situation where \\(y_t\\) has an internal dynamic that creates an AR(1) process and something external that creates the MA(1) process. For instance, let’s imagine that we are measuring the growth of a perennial plant that requires a long growing season in \\(t-1\\) to condition growth the following year (\\(t\\)). Perhaps it requires a lot of growing degree days in the current year \\(t\\), and its internal biological needs rely on last year’s stored energy \\(t-1\\) for kicking off growth at the beginning of the year \\(t\\). If growing degrees themselves are autocorrelated, perhaps because of atmospheric dynamics, you’d have internal autocorrelation and external autocorrelation. This might well create an ARMA(1,1) process in growth. As we get into higher order models this kind of understanding can be difficult to develop – so don’t overfit models! Just because a long time series (large \\(n\\)) fits a high order model (like an AR(20)), it doesn’t mean that is the best model if there is no plausible mechanism to explain it. Never forget the sage words of Box: “…essentially, all models are wrong, but some are useful.” 7.6 What about ARIMA(p, d, q)? So far, we’ve worked with AR(\\(p\\)) and MA(\\(q\\)) models, and their combination: ARMA(\\(p, q\\)) models. These assume the time series is stationary—its properties (like mean, variance, and autocorrelation) don’t change over time. 7.6.1 Why Do ARMA Models Require Stationarity? ARMA (Autoregressive Moving Average) models require stationarity because their structure assumes that the statistical properties of the time series remain constant over time. This includes a constant mean, constant variance, and a constant autocorrelation structure. 7.6.1.1 Parameter Stability The coefficients in ARMA models are fixed and describe how current values depend on past values (AR terms) and past errors (MA terms). If the time series is non-stationary, these relationships may change over time, making the model unreliable. 7.6.1.2 Meaningful Lag Relationships The AR terms relate a value to its previous values, and the MA terms relate it to previous shocks (errors). These relationships are only meaningful when the process is consistent over time—i.e., stationary. 7.6.1.3 Inference and Forecasting The validity of statistical inference and forecasting depends on the assumption that the underlying process does not change. Non-stationarity undermines this assumption and can lead to misleading predictions. 7.6.1.4 Mathematical Properties The theoretical behavior of the autocorrelation function (ACF) and partial autocorrelation function (PACF) in ARMA models is derived under the assumption of stationarity. Without it, these properties may not be stable or interpretable. In practice, if a time series is non-stationary, we typically difference the data to induce stationarity before fitting an ARMA model. This leads to the ARIMA model, where the “I” stands for “Integrated” (i.e., differenced). Many environmental time series show non-stationarity. For example: Long-term trends in temperature or sea level, Gradual shifts in nutrient concentrations, Snowpack accumulation over the season. In those cases, ARMA models are often not appropriate becasue the structure they rely on doesn’t hold. To deal with that, we often difference the series: \\[ y&#39;_t = y_t - y_{t-1} \\] This removes linear trends and helps stabilize the mean (refer to last week’s Stationarity is dead section). If the differenced series is stationary, we can model it using ARMA tools. That’s the idea behind ARIMA models. We aren’t going to go into too much depth on ARIMA but we will see the acronym a lot and it’s good to aware of differencing as a tool. The important take away is that ARIMA(\\(p, d, q\\)) means: Difference the series d times, Then fit an ARMA(p, q) model to the result. This is why ARIMA models are often described as ARMA models applied to differenced data. 7.6.2 Simulated Data We’ll simulate a process with an AR(1) and MA(1) component, plus a drifting mean, so it’s non-stationary. This mimics a situation where ARMA wouldn’t work directly but an ARIMA model might. set.seed(123) n &lt;- 100 x &lt;- 1:n y &lt;- numeric(n) epsilon &lt;- rnorm(n) phi &lt;- 0.9 # AR(1) theta &lt;- 0.2 # MA(1) drift &lt;- 0.8 # linear trend (makes the process non-stationary) # Simulate ARMA(1,1) with drift - aka ARIMA(1,1,1) y[1] &lt;- 0 for (i in 2:n) { y[i] &lt;- drift + phi * y[i-1] + theta * epsilon[i-1] + epsilon[i] } ggplot() + geom_line(aes(x=x,y=y)) + labs(x=&quot;time&quot;, title = &quot;Simulated Non-Stationary ARMA(1,1) with Drift&quot;) You’ll see the series drifts upward over time—it’s not stationary. Now difference it: y_diff &lt;- diff(y) # note length of y compared to y_diff! ggplot() + geom_line(aes(x=x[-1],y=y_diff)) + labs(x=&quot;time&quot;, title = &quot;Differenced Series (Looks Stationary)&quot;) The differenced version appears stationary. At this point, we could try fitting an ARMA model to it. We won’t go further into ARIMA models in this course, but understanding that they extend ARMA by differencing non-stationary series gives you a strong foundation for future time series work. 7.7 Evaluating ARMA models 7.7.1 Simulated Data So given that quick intro to ARMA let’s try a baby example of fitting an ARMA model and finding the least bad one. We’ll use the arima.sim function to simulate a ARMA(1,1) time series. n &lt;- 500 x &lt;- 1:n y &lt;- arima.sim(list(ar = 0.8, ma = 0.6), n = n) And a plot. p1 &lt;- broom::tidy(y) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Time&quot;) p2 &lt;- ggAcf(y) + labs(title=element_blank()) p3 &lt;- ggPacf(y) + labs(title=element_blank()) gridExtra::grid.arrange(p1,p2,p3,layout_matrix=simpleLayoutMatrix) Now let’s do some modeling of that series with several different formulations of the ARMA process. Here we are going to fit parameters (e.g., \\(\\phi\\) and \\(\\theta\\)) to a time series (\\(y\\)) for a specified model (e.g., an ARMA(1,1) model). The parameters are fit by maximum likelihood (typically) and just like with most models we can assess the goodness of fit. Here we will use the Arima function in forecast (which is just a slightly fancier version of the arima function in the stats package). We will try to fit ARMA(\\(p\\),\\(q\\)) with \\(p\\) and \\(q\\) ranging from zero to two. Note that we are leaving a zero for the differencing term because \\(y\\) is stationary. arma00 &lt;- Arima(y,order = c(0,0,0)) arma10 &lt;- Arima(y,order = c(1,0,0)) arma20 &lt;- Arima(y,order = c(2,0,0)) arma01 &lt;- Arima(y,order = c(0,0,1)) arma02 &lt;- Arima(y,order = c(0,0,2)) arma11 &lt;- Arima(y,order = c(1,0,1)) arma21 &lt;- Arima(y,order = c(2,0,1)) arma12 &lt;- Arima(y,order = c(1,0,2)) arma22 &lt;- Arima(y,order = c(2,0,2)) Here is what the ARMA(1,0) aka AR(1) spits out. arma10 ## Series: y ## ARIMA(1,0,0) with non-zero mean ## ## Coefficients: ## ar1 mean ## 0.8547 0.0392 ## s.e. 0.0231 0.3389 ## ## sigma^2 = 1.246: log likelihood = -764.01 ## AIC=1534.03 AICc=1534.08 BIC=1546.67 Note that we get estimates of the coefficients (here \\(\\phi_1\\) and a constant) and their standard errors. We also get the log likelihood (\\(l\\)) which is a measure of how strongly the model fits the data with smaller numbers indicating a better fit. With the number of parameters in the model we can get the AIC (\\(-2l + 2k\\), where \\(k\\) is the number of parameters in the model) and with the sample size (\\(n\\)) we can get BIC (\\(-2l + ln(n)k\\)). The AIC and BIC penalize the log likelihood for more complex models. I like thinking of AIC as saying, “Look friend, find me a model that fits well but isn’t overfit just because I threw it a lot of parameters.” BIC says something similar but adds in the sample size, “Look friend, find me a model that fits well but isn’t overfit just because I threw it a lot of parameters and had a huge sample size.” In our models above the sample sizes were all the same so the AIC and BIC will be proportional. When comparing models, the smaller the AIC or BIC the better. There is a lot more you can read about on this stuff but we can leave it alone for now. Let’s see how the ARMA(1,0) fits these data visually. dat &lt;- tibble(x=x,y=arma10$x,yhat=arma10$fitted) dat %&gt;% pivot_longer(cols=2:3) %&gt;% ggplot(mapping = aes(x=x,y=value,color=name)) + geom_line() + labs(x=&quot;Time&quot;) Wow! Pretty impressive. The R\\(^2\\) on that model is excellent at 0.732. Let’s evaluate each of those models using BIC. armaBIC &lt;- BIC(arma00,arma10,arma20,arma01,arma02,arma11,arma21,arma12,arma22) armaBIC ## df BIC ## arma00 2 2194.669 ## arma10 3 1546.671 ## arma20 4 1467.577 ## arma01 3 1704.234 ## arma02 4 1537.683 ## arma11 4 1431.581 ## arma21 5 1437.788 ## arma12 5 1437.789 ## arma22 6 1442.821 From the BIC values it looks like the ARMA(1,1) model is one to investigate. arma11 ## Series: y ## ARIMA(1,0,1) with non-zero mean ## ## Coefficients: ## ar1 ma1 mean ## 0.7298 0.5939 0.0541 ## s.e. 0.0332 0.0396 0.2586 ## ## sigma^2 = 0.978: log likelihood = -703.36 ## AIC=1414.72 AICc=1414.8 BIC=1431.58 Those coefs are very close to what we used in our simulation. Let’s take a closer look at the residuals. If they look clean of structure, we will know that our model accounting for temporal structure in the data. First let’s look at the ACF and PACF of the residuals: y &lt;- arma11$residuals p1 &lt;- broom::tidy(y) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Time&quot;) p2 &lt;- ggAcf(y) + labs(title=element_blank()) p3 &lt;- ggPacf(y) + labs(title=element_blank()) gridExtra::grid.arrange(p1,p2,p3,layout_matrix=simpleLayoutMatrix) Those look lovely. And we can use the tsdiag function. That gives the same info as the residual plots above but also plots the p-values from the Ljung–Box test. Larger values of p suggest independence at that lag. It’s an ugly plot but a useful one: tsdiag(arma11) Let’s see how the ARMA(1,1) fits these data visually. dat &lt;- tibble(x=x,y=arma11$x,yhat=arma10$fitted) dat %&gt;% pivot_longer(cols=2:3) %&gt;% ggplot(mapping = aes(x=x,y=value,color=name)) + geom_line() + labs(x=&quot;Time&quot;) Wow! Pretty impressive. The R\\(^2\\) on that model is excellent at 0.793 Note that while the BIC values indicated ARMA(1,1), some of those other models look pretty good! Why pick ARMA(1,1) over a different model? There is often not a “right” and we often have to be content with the “least wrong” model. Look at the ACF and PACF of the residuals of some of the other models. In general, simpler is better than more complicated. It all gets down to specifying this annoying question, “What are you trying to answer?” 7.7.2 SST Data Real data is always messier than simulated data. Let’s take a look at some. Catherine Pfister and colleagues wrote a very nice paper in the Journal of Ecology a few years back. In the paper they looked at spatial and temporal autocorrelation in Salish Sea kelp abundance and linked it to oceanic drivers like sea surface temperatures. We might delve into that paper in more depth later – it’s a lovely study. See here for the paper and the data are archived here. I pulled out the sea surface temperature from Race Rocks, Canada which goes back to 1921 and is a great long, local time series. Let’s look at the temporal structure in these data. sst &lt;- readRDS(&quot;data/RaceRocksSST.rds&quot;) p1 &lt;- broom::tidy(sst) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Year&quot;,y=expression(degree~C), title=&quot;Sea Surface Temperatures, Race Rocks, Canada&quot;) p2 &lt;- ggAcf(sst) + labs(title=element_blank()) p3 &lt;- ggPacf(sst) + labs(title=element_blank()) gridExtra::grid.arrange(p1,p2,p3,layout_matrix=simpleLayoutMatrix) That is a complicated picture we have from the ACF and PACF plots. It certainly looks like there is a trend in the data so let’s difference it first and then fit some ARMA models. sst_diff &lt;- diff(sst) p1 &lt;- broom::tidy(sst_diff) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Year&quot;,y=expression(degree~C~Anomoly), title=&quot;Differenced Sea Surface Temperatures, Race Rocks, Canada&quot;) p2 &lt;- ggAcf(sst_diff) + labs(title=element_blank()) p3 &lt;- ggPacf(sst_diff) + labs(title=element_blank()) gridExtra::grid.arrange(p1,p2,p3,layout_matrix=simpleLayoutMatrix) Still a complicated picture we have from the ACF and PACF plots with the differencing. Now for some ARMA models. arma00 &lt;- Arima(sst_diff,order = c(0,0,0)) arma10 &lt;- Arima(sst_diff,order = c(1,0,0)) arma20 &lt;- Arima(sst_diff,order = c(2,0,0)) arma01 &lt;- Arima(sst_diff,order = c(0,0,1)) arma02 &lt;- Arima(sst_diff,order = c(0,0,2)) arma11 &lt;- Arima(sst_diff,order = c(1,0,1)) arma21 &lt;- Arima(sst_diff,order = c(2,0,1)) arma12 &lt;- Arima(sst_diff,order = c(1,0,2)) arma22 &lt;- Arima(sst_diff,order = c(2,0,2)) Note that we could have used the undifferenced data above and included a \\(d\\) term in the call to Arima. E.g., an AR(1) model with differencing would be Arima(sst,order = c(1,1,0)) which is the same as Arima(sst_diff,order = c(1,0,0)). I chose to do the differencing manually to be consistent with what we did above. We can then evaluate each model by BIC. armaBIC &lt;- BIC(arma10,arma20,arma01,arma02,arma11,arma21,arma12,arma22,arma00) armaBIC ## df BIC ## arma10 3 115.73152 ## arma20 4 104.65047 ## arma01 3 96.01018 ## arma02 4 94.13930 ## arma11 4 95.68852 ## arma21 5 99.32558 ## arma12 5 96.53865 ## arma22 6 100.78066 ## arma00 2 119.76488 From the BIC values it looks like the ARMA(0,2) model and the ARMA(1,1) models are both quite good. And ARMA(0,1) is appealing too. Here is the ARMA(1,1) model. arma02 ## Series: sst_diff ## ARIMA(0,0,2) with non-zero mean ## ## Coefficients: ## ma1 ma2 mean ## -0.5718 -0.3205 0.0082 ## s.e. 0.1145 0.1217 0.0046 ## ## sigma^2 = 0.1336: log likelihood = -37.98 ## AIC=83.97 AICc=84.42 BIC=94.14 Let’s take a closer look. First let’s look at the ACF and PACF of the residuals for the ARMA(0,2). y &lt;- arma02$residuals p1 &lt;- broom::tidy(y) %&gt;% ggplot(mapping = aes(x=index,y=value)) + geom_line() + labs(x=&quot;Time&quot;) p2 &lt;- ggAcf(y) + labs(title=element_blank()) p3 &lt;- ggPacf(y) + labs(title=element_blank()) gridExtra::grid.arrange(p1,p2,p3,layout_matrix=simpleLayoutMatrix) Those are clean. So what do we do with this information? It does appear that these data follow a fairly complicated time-series model. Which one? Well there isn’t one right answer. The BIC indicated and ARMA(0,2) model and gives clean residuals. But the ARMA(1,1) has a good fit as does the ARMA(0,1). I think you can make an argument for any of those models here. If these were my data, I’d really want to know more about the mechanisms that might create the patterns we see in the data. Pattern is all well and good but process is cooler. 7.7.3 How Do You Decide Which Information Criterion to Use? When fitting ARMA models, you’re often faced with multiple competing options — all with decent fits. So how the heck do you decide which information criterion (IC) to use? AIC? BIC? AICc? Each one has its own logic and use case. Here’s a quick guide to help you choose: Criterion Penalty for Complexity Best For Notes AIC (Akaike Information Criterion) \\(2k\\) Prediction May overfit; balances fit and complexity BIC (Bayesian Information Criterion) \\(k \\cdot \\log(n)\\) Finding the “true” model Stronger penalty on complexity AICc (Corrected AIC) AIC + small-sample correction Small samples Recommended when \\(n / k &lt; 40\\) In general: AICc is often preferred for small sample sizes AIC or BIC can be used for larger samples, depending on whether the focus is on prediction (AIC) or parsimony/inference (BIC). I like to fool myself into thinking I can get the “true” model. But, as in life, there isn’t one answer. Use AIC: For predictive performance When \\(n\\) is moderate to large Use BIC: When aiming to identify the “true” model When sample size is large Use AICc: When sample size is small (typically \\(n &lt; 50\\)) To avoid overfitting in small-sample contexts 7.8 Your work 7.8.1 Bunnies and Kitties You all recall the famous Lynx and Hare data from when you took ecology and learned about predator-prey interactions and Lotka-Volterra equations I bet. I grabbed these data from the ecostudy package and have them as a ts object. The data give the number of pelts (in thousands) for Lynx and Hare traded with the Hudson Bay Trading Company in 1845 to 1935. LynxHare &lt;- readRDS(&quot;data/LynxHare.rds&quot;) str(LynxHare) ## Time-Series [1:91, 1:2] from 1845 to 1935: 19.6 19.6 19.6 12 28 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:2] &quot;Hare&quot; &quot;Lynx&quot; plot(LynxHare,main=&quot;Annual numbers of pelts in tousands&quot;) The adorable Snowshoe Hare is the primary food of the Canada Lynx and the these two species are closely linked by their population biology. Take a moment and write down how you can think about their population cycles in terms of their time-series properties. Can we try to understand how those populations might vary in terms of AR and MA processes? Do some wild and irresponsible speculation. Now, model the Lynx pelt data as an ARMA process. Do some further wild and irresponsible speculation about what you found. How does your thinking about this famous data fit into your understanding of an ARMA model? 7.8.2 Write up and Reflect Write it all up and pass in a knitted R Markdown document. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? We haven’t subjected that to any kind of training/testing scheme so we shouldn’t get too excited↩︎ Again, we haven’t subjected that to any kind of training/testing scheme. We will talk about the challenges of training and testing data for time series models later in the course.↩︎ "],["forecasting.html", "8 Forecasting 8.1 Big Idea 8.2 Reading 8.3 Packages 8.4 Introduction 8.5 A simple forecast 8.6 Using ARMA coefficients to forecast 8.7 Holt-Winters 8.8 Your work", " 8 Forecasting 8.1 Big Idea The correlation structure of a time series itself can often be enough to forecast it for a short time. 8.2 Reading Have a look at Chapter three from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great book for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 8.3 Packages You’ll want the forecast (Hyndman et al. 2025) library. And tidyverse (Wickham 2023) naturally. library(tidyverse) library(forecast) 8.4 Introduction There are entire books written about forecasting time series data and many of them are financial in nature. (You would do well to quit the environmental world, make a lot of money in finance, and then make a nice donation to the University with “ESCI 504” written on the memo line of the check.) The idea is that you can use the properties of the time series itself to forecast it over a short period. If the series has enough temporal structure you can likely predict it (albeit with some error) for some short time period. I have to confess that I’ve never had to do any serious time series forecasting in my own research. I’m interested in describing and accounting for temporal structure as a way of understanding mechanism more than I am in predicting values. I did work with a grad student a few years ago who wanted to forecast river flow from temperature and precipitation records. We found that a time series forecast worked pretty well and sometimes better than using exogenous predictors. So, even if you can’t make a trillion bucks forecasting environmental data, you should pay attention. 8.5 A simple forecast Let’s imagine an AR(1) model and show how we can forecast it using \\(y_{t-1}\\) and \\(\\phi\\) to model \\(y_t\\). Suppose we have a model: \\[ y_t = \\phi y_{t-1} + \\epsilon_t \\] where \\(\\phi = 0.7\\) and \\(\\epsilon_t \\sim \\mathcal{N}(0, 1)\\). If we know the current value \\(y_t\\), we can forecast the next value \\(y_{t+1}\\) as: \\[ \\hat{y}_{t+1} = \\phi y_t = 0.7 \\times y_t \\] Since this is a one-step-ahead forecast, the expected value of the error term \\(\\epsilon_{t+1}\\) is zero, so our best forecast is just the deterministic part. However, that forecast has some uncertainty associated with it that we can model as well. The forecast error is defined as: \\[ e_{t+1} = y_{t+1} - \\hat{y}_{t+1} = \\phi y_t + \\epsilon_{t+1} - \\phi y_t = \\epsilon_{t+1} \\] So the one-step-ahead forecast error has variance: \\[ \\text{Var}(e_{t+1}) = \\text{Var}(\\epsilon_{t+1}) = 1 \\] This means our forecast is unbiased, but there is always a ±1 SD range of uncertainty. 8.5.1 Two-step-ahead forecast To forecast two steps ahead, we use our prediction for \\(y_{t+1}\\) to forecast \\(y_{t+2}\\). The true value is: \\[ y_{t+2} = \\phi y_{t+1} + \\epsilon_{t+2} \\] We don’t know \\(y_{t+1}\\), but we can plug in our forecast: \\[ \\hat{y}_{t+2} = \\phi \\hat{y}_{t+1} = \\phi^2 y_t \\] This is the best guess based on information available at time \\(t\\). But now there are two sources of uncertainty: the new shock \\(\\epsilon_{t+2}\\), and the fact that \\(y_{t+1}\\) itself was only an estimate. When we expand the expression for \\(y_{t+2}\\), we get: \\[ y_{t+2} = \\phi (\\phi y_t + \\epsilon_{t+1}) + \\epsilon_{t+2} = \\phi^2 y_t + \\phi \\epsilon_{t+1} + \\epsilon_{t+2} \\] So the two-step-ahead forecast error is: \\[ e_{t+2} = y_{t+2} - \\hat{y}_{t+2} = \\phi \\epsilon_{t+1} + \\epsilon_{t+2} \\] And the variance of this error is: \\[ \\text{Var}(e_{t+2}) = \\phi^2 \\text{Var}(\\epsilon_{t+1}) + \\text{Var}(\\epsilon_{t+2}) = \\phi^2 + 1 \\] With \\(\\phi = 0.7\\), that gives: \\[ \\text{Var}(e_{t+2}) = (0.7)^2 + 1 = 0.49 + 1 = 1.49 \\] So, while the one-step-ahead forecast has a variance of 1, the two-step-ahead forecast has a higher variance of 1.49. This increase reflects the compounding uncertainty as we project further into the future. And you can foresee that if you want to forecast three steps out, the variance would increase again as additional error terms enter the expression, each scaled by a power of \\(\\phi\\). The forecast error would include \\(\\phi^2 \\epsilon_{t+1} + \\phi \\epsilon_{t+2} + \\epsilon_{t+3}\\), and its variance would be: \\[ \\text{Var}(e_{t+3}) = \\phi^4 + \\phi^2 + 1 \\] In general, for an \\(k\\)-step-ahead forecast in an AR(1) model with white noise variance \\(\\sigma^2\\), the forecast error variance is: \\[ \\text{Var}(e_{t+k}) = \\sigma^2 \\sum_{j=0}^{k-1} \\phi^{2j} \\] This is a geometric series, and if \\(|\\phi| &lt; 1\\), it converges to: \\[ \\lim_{k \\to \\infty} \\text{Var}(e_{t+k}) = \\frac{\\sigma^2}{1 - \\phi^2} \\] So even though uncertainty grows with forecast horizon, it levels off to a fixed upper bound when the process is stationary. Recall how the ACF function in an AR(1) trailed off geometrically? This is the same idea. 8.5.2 Example Let’s show this in a simple simulation. Below, we use the last observed value \\(y_t\\) to generate a one-step-ahead forecast: \\[ \\hat{y}_{t+1} = \\phi y_t \\] Then, we use this forecasted value to produce a two-step-ahead forecast: \\[ \\hat{y}_{t+2} = \\phi \\hat{y}_{t+1} \\] and so on. This is called chained forecasting, and it mimics what we would do in practice when future values are unknown. While each step uses the same rule, the uncertainty compounds because each forecast depends on previous estimates rather than actual values. # Simulate an AR(1) process and do chained forecasting set.seed(62) # for reproducibility n &lt;- 10 phi &lt;- 0.7 epsilon &lt;- rnorm(n) # Generate the AR(1) series y &lt;- numeric(n) y[1] &lt;- epsilon[1] for (t in 2:n) { y[t] &lt;- phi * y[t - 1] + epsilon[t] } # Last observed value y_t &lt;- y[n] # Forecasts y_hat_1 &lt;- phi * y_t # one-step-ahead y_hat_2 &lt;- phi * y_hat_1 # two-step-ahead (chained) y_hat_3 &lt;- phi * y_hat_2 # three-step-ahead (chained) # Forecast SDs (1, phi^2 + 1, phi^4 + phi^2 + 1) se_1 &lt;- 1 se_2 &lt;- sqrt(phi^2 + 1) se_3 &lt;- sqrt(phi^4 + phi^2 + 1) 8.6 Using ARMA coefficients to forecast Now that we get the idea, let’s work on a more sophisticated example using the fitted coefficients from an AR model. That is, we will build on our experience and use just the internal autocorrelation of a time series to forecast a few steps forward. Doing so is more interesting given stronger and longer memory processes. Thus, we will take the data of annual sunspot counts which you’ll recall vary over about a decade and forecast the time series. data(sunspot.year) autoplot(sunspot.year) + labs(x=&quot;Year&quot;, y =&quot;Sunspots (n)&quot;) + theme_minimal() Until now we have been fitting AR and ARMA “by hand.” We’ve been thinking about the models and deciding on the order based on ACF, PACF, AIC, etc. That is the smart way of doing things. It makes you think about the models, the processes, and so on. But some of you have already discovered the the ar and auto.arima functions which simplify this process (dangerously so4). The later is in the forecast library. Here we will use ar to fit an AR(\\(p\\)) model and \\(p\\) will be selected for us magically (actually by AIC). sunspot.ar &lt;- ar(sunspot.year) sunspot.ar ## ## Call: ## ar(x = sunspot.year) ## ## Coefficients: ## 1 2 3 4 5 6 7 8 ## 1.1305 -0.3524 -0.1745 0.1403 -0.1358 0.0963 -0.0556 0.0076 ## 9 ## 0.1941 ## ## Order selected 9 sigma^2 estimated as 267.5 Note the the class of the sunspot.ar object and think about methods. Most models have a predict method. So, now that we have the AR(9) model for the sunspots we can predict sunspots out a few years. sunspot.forecast &lt;- predict(sunspot.ar, n.ahead = 10) plot(sunspot.year,xlim=c(1700, 1998),col=&quot;grey&quot;,lwd=1.5, ylab=&quot;Sunspots (n)&quot;) lines(sunspot.forecast$pred,col=&quot;darkgreen&quot;,lwd=1.5) This is pretty cool. The sunspot data fit an AR(9) process and the predict function (look at ?predict.ar) captures the cyclic nature of the sunspot data. Let’s make a better plot by zooming in a little and adding the standard error of the forecast to the plot. I’ll go ggplot for this one. forecast_tb &lt;- tibble(index = time(sunspot.forecast$pred), yhat = sunspot.forecast$pred, lower = sunspot.forecast$pred - sunspot.forecast$se, upper = sunspot.forecast$pred + sunspot.forecast$se) obs_tb &lt;- broom::tidy(sunspot.year) %&gt;% rename(y = value) ggplot() + geom_line(data = obs_tb,mapping = aes(x=index,y=y)) + geom_ribbon(data = forecast_tb, mapping = aes(x=index,ymin=lower, ymax=upper), fill=&quot;darkgreen&quot;,alpha=0.5) + geom_line(data = forecast_tb, mapping = aes(x=index,y=yhat), col=&quot;darkgreen&quot;) + labs(y=&quot;Sunspots (n)&quot;,x=&quot;Year&quot;) + scale_x_continuous(expand = c(0,0),limits=c(1950,1998)) ## Warning: Removed 250 rows containing missing values or values outside the scale range ## (`geom_line()`). But what would happen if we extended this forward? Where will the predictions go? What about the errors? 8.7 Holt-Winters We’ve already worked with data containing seasonality and a trend. E.g., the CO\\(_2\\) data you’ve used when you were decomposing time series back towards the start of class. Forecasting this kind of data is more involved than the AR or ARMA approach we used above. Recall stationarity? The Holt-Winters algorithm is a way forward. The book has an excellent explanation of the theory behind it. I’m not going to walk you through all of it but I want to explain, to some extent, the idea behind exponential smoothing. In simplest terms, exponential smoothing is a weighted average where points that are immediately back in time affect the predicted value more than points that are further back in time. Any time you want to forecast (predict), you need a model. Holt-Winters uses triple exponential smoothing to estimate \\(\\hat y_{t+1}\\) as a function of past values of \\(y_t\\). The triple part means that Holt Winters applies the exponential smoothing concept to the 1) trend, 2) season, and 3) residuals. So let’s look at what exponential smoothing is. But we can start to unpack exponential smoothing much more simply using averages and moving averages. We’ve talked about kind of stuff a little but this is a good chance to get further into it. Let’s think about this with some code. Below we have a time series \\(y_t\\) with seven values and we will use those past values to predict the 8th value. x &lt;- 1:7 y &lt;- c(8,5,12,11,12,9,19) p1 &lt;- ggplot(mapping = aes(x=x,y=y)) + geom_point() + geom_line() + lims(x=c(1,9)) p1 If I want to estimate \\(\\hat y_8\\) we could start with a simple average. The idea is that if \\(y\\) is a random variable it will have a central tenancy so forecasting it to the mean is kind of reasonable, right? So we will assume that \\(\\hat y_8 = \\bar y\\) and add it to the plot. yhat1 &lt;- mean(y) yhat1 ## [1] 10.85714 p1 + geom_point(mapping = aes(x=8,y=yhat1),color=&quot;blue&quot;,size=3) We can also use moving averages to get just a tiny bit more sophisticated. You used these some when we were reading and plotting data. Recall the filter and rollmean functions? Let’s roll our own but in this case we won’t roll the function along the length of \\(y\\) but just select how many points back in time to use to estimate \\(\\hat y_8\\) : maFunc &lt;- function(x,n=2){ nx &lt;- length(x) mean(x[(nx-n+1):nx]) } yhat2 &lt;- maFunc(y,n=4) # 4 point yhat2 ## [1] 12.75 p1 + geom_point(mapping = aes(x=8,y=yhat2),color=&quot;blue&quot;,size=3) I know some of you are already programming quite a bit and have asked questions about how to write functions. Feel free to ignore if this doesn’t interest you. I’ll slip a little code snippet in here here so that if we set n to NULL we will just get the average. maFunc &lt;- function(x,n=2){ if(is.null(n)) { mean(x) } else{ nx &lt;- length(x) mean(x[(nx-n+1):nx]) } } maFunc(y,n=NULL) ## [1] 10.85714 # aka mean(y) OK, back to work. Note that while we think about averages, moving or otherwise, we usually calculate it as the sum of the values divided by the number of values. E.g., sum(x)/length(x). For the three point moving average that would calculate \\(y_8\\) this would look like: sum(y[5:7]) / 3 ## [1] 13.33333 # aka maFunc(y,3) ## [1] 13.33333 An algebraic thing that you might not have considered is that we can so the same thing as: \\[\\hat y_8 = y_5 \\times \\frac{1}{3} + y_6 \\times \\frac{1}{3} + y_7 \\times \\frac{1}{3}\\] Or sum(y[5]*0.333 + y[6]*0.333 + y[7]*0.333) ## [1] 13.32 That is, we are moving the denominator up and multiplying each value by \\(1/3\\). This gets the same answer and treats all observations for calculating the average the same way. Those three values are weighted evenly. However, we can also add weights to the average. You’ve done this at some point in your careers. The idea is to make some points have more influence. In time series this is almost always done to make more recent values matter more in the average. Thus we can supply a vector of weights, w, that sum to one that will affect the way the average is calculated. The length of w will be the length of the moving average and the weights will reflect the relative importance of that observations.For example here is a weighted moving average of 4 with 10%, 20%, 30% and 40% given to those last 4 points. wmaFunc &lt;- function(x,w){ nx &lt;- length(x) nw &lt;- length(w) sum(x[(nx-nw+1):nx] * w) } yhat3 &lt;- wmaFunc(y,w=c(0.1,0.2,0.3,0.4)) yhat3 ## [1] 13.8 p1 + geom_point(mapping = aes(x=8,y=yhat3),color=&quot;blue&quot;,size=3) So above we are estimating the 8th value of \\(y\\) using the four prior values where each is value is weighted: \\[\\hat y_8 = y_4 \\times 0.1 + y_5 \\times 0.2 + y_6 \\times 0.3 + y_7 \\times 0.4\\] Note that for this to make sense, the weights adding up to 1. If you used weights like 0.9, 0.8, 0.7, 0.6 (which add up to 3.0) you’d get something ridiculous for the estimate If you were programming defensively you’d add a check to the top of wmaFunc to make sure the argument w summed to one. Again ignore this if you like: wmaFunc &lt;- function(x,w){ if(!isTRUE(all.equal(sum(w),1,tolerance = 0.01))) {stop(&quot;w must sum to 1&quot;)} nx &lt;- length(x) nw &lt;- length(w) sum(x[(nx-nw+1):nx] * w) } Understanding weights in general is a pretty fundamental skill and especially important for what we are going to do next. Take some time to make sure you follow the concept. Oh, and recall the filter function? We just recreated some of what it does. Look at the help page for filter and see if you see how this ties together note the last non-NA values from this call to filter. Because the base filter function conflicts with dplyr’s filter we will call stats::filter explicitly. wmaFunc(y,w=c(1/3,1/3,1/3)) ## [1] 13.33333 stats::filter(y,filter = c(1/3,1/3,1/3)) ## Time Series: ## Start = 1 ## End = 7 ## Frequency = 1 ## [1] NA 8.333333 9.333333 11.666667 10.666667 13.333333 NA Let’s pause and see what these estimates look like: preds &lt;- tibble(x=8, yhat=c(yhat1,yhat2,yhat3), description=c(&quot;mean&quot;,&quot;moving avg&quot;,&quot;weighted moving avg&quot;)) p1 + geom_point(preds,mapping = aes(x=x,y=yhat), color=&quot;blue&quot;,size=3) + geom_text(preds,mapping = aes(x=x,y=yhat,label=description), position = position_nudge(y = -0.25),color=&quot;blue&quot;,size=3) These are all reasonable predictions but not especially interesting. But let’s think about the weighted moving average differently. Imagine the weighted average where we consider the entire series and assign exponentially smaller weights as we go back in time. For example if we started with a smoothing coefficient of \\(\\alpha = 0.9\\), our weights going back in time from 7 would be: \\[0.9^1, 0.9^2, 0.9^3, 0.9^4, 0.9^5, ...\\] Or: \\[0.900, 0.810, 0.729, 0.656, 0.590, ... \\] You can see that this decreases in a predictable negative exponential decay and as the time step backwards increases the weights approach zero. E.g., if we used \\(\\alpha = 0.9\\) and looked at the weights going back 50 time steps: x &lt;- 1:50 y &lt;- 0.9^x ggplot() + geom_point(aes(x=x,y=y)) + labs(y=&quot;Weight&quot;, x=&quot;Number of backwards time steps&quot;) You’ve probably spotted a problem. We can’t use these weights like we did the moving average because the that the series sums to more than one. That screws up the weighted moving average concept. What Holt (or maybe all the way back to Poisson – I’m not up on the history of math) did was solve this elegantly as: \\[ \\hat y_t = \\alpha \\times y_t + (1 - \\alpha) \\times \\hat y_{t-1}\\] Spend some time appreciating this formula. It shows that your estimate is the sum of two products. This is essentially a weighted average with \\(\\alpha\\) and \\(1 - \\alpha\\) as the weights. Since these two terms sum to one we can use it as a moving average. This is a recursive formula where \\(1 - \\alpha\\) is multiplied by the prior expected value \\(\\hat y_{t-1}\\) so this crawls back to the beginning of the series. We implement this in a loop: expFunc &lt;- function(x,alpha=0.5){ xhat &lt;- x[1] nx &lt;- length(x) for(i in 2:nx){ xhat[i] &lt;- alpha * x[i] + (1 - alpha) * x[i-1] } xhat[nx] } y &lt;- c(8,5,12,11,12,9,19) yhat &lt;- expFunc(y,alpha=0.9) yhat ## [1] 18 We can see what these look like for different values of \\(\\alpha\\): x &lt;- 1:7 y &lt;- c(8,5,12,11,12,9,19) preds &lt;- tibble(x=8, yhat=c(expFunc(y,alpha=0.8), expFunc(y,alpha=0.4), expFunc(y,alpha=0.2)), description=c(&quot;alpha = 0.8&quot;, &quot;alpha = 0.4&quot;, &quot;alpha = 0.2&quot;)) ggplot(mapping = aes(x=x,y=y)) + geom_point() + geom_line() + lims(x=c(1,9)) + geom_point(preds,mapping = aes(x=x,y=yhat), color=&quot;blue&quot;,size=3) + geom_text(preds,mapping = aes(x=x,y=yhat,label=description), position = position_nudge(y = -0.25),color=&quot;blue&quot;,size=3) This is the base concept underneath Holt-Winters smoothing. The book walks you through how to go from \\(\\hat y_t = \\alpha \\times y_t + (1 - \\alpha) \\times \\hat y_{t-1}\\) (eq. 3.15) to the entire Holt-Winters method (eq. 3.21) which is composed of combining the simple smoothing as shown above (\\(\\alpha\\)) with the trend (\\(\\beta\\)) and seasonality (\\(\\gamma\\)) to to do the forecasting. The trick is figuring out what the parameters (\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\)) are. The HoltWinters function does this with numeric optimization – in the default case minimizing the sum of the squared errors for all three parameters. Here is it in action: fname &lt;- &quot;https://www.esrl.noaa.gov/gmd/webdata/ccgg/trends/co2/co2_mm_mlo.txt&quot; co2 &lt;- read.table(fname) co2 &lt;- co2[,4] co2 &lt;- ts(co2,start=c(1958,3),frequency = 12) hw &lt;- HoltWinters(co2) hw ## Holt-Winters exponential smoothing with trend and additive seasonal component. ## ## Call: ## HoltWinters(x = co2) ## ## Smoothing parameters: ## alpha: 0.5306546 ## beta : 0.01455503 ## gamma: 0.3019897 ## ## Coefficients: ## [,1] ## a 427.2432590 ## b 0.2152986 ## s1 -1.7319226 ## s2 -3.1603238 ## s3 -3.1203881 ## s4 -1.7525459 ## s5 -0.4967728 ## s6 0.4805736 ## s7 1.1996301 ## s8 1.6694987 ## s9 2.9822135 ## s10 3.4534960 ## s11 2.7131830 ## s12 0.6168803 plot(co2,xlim=c(1980,2040),ylim=c(340,460), ylab=expression(CO[2]~(ppm))) lines(predict(hw,n.ahead = 120),col=&quot;blue&quot;) 8.8 Your work 8.8.1 A silly prediction Let’s look at globally averaged methane concentrations. The data are monthly and run back to 1983. fname &lt;- &quot;https://gml.noaa.gov/webdata/ccgg/trends/ch4/ch4_mm_gl.txt&quot; ch4 &lt;- read.table(fname) ch4 &lt;- ch4[,3:4] names(ch4) &lt;- c(&quot;DecimalDate&quot;,&quot;ppb&quot;) ggplot(ch4,mapping = aes(x=DecimalDate,y=ppb)) + geom_line() + labs(x=&quot;Date&quot;,y=expression(CH[4]~(ppb)), title=&quot;Global Monthly Mean Methane Concentration&quot;) # make a ts object ch4.ts &lt;- ts(ch4$ppb,start=c(1983,7),frequency = 12) 8.8.1.1 Train and Test Let’s divide the data into two parts. The first chuck of the time series we can use for training a model and we can use the second chunk for testing the model. Thus, you can forecast the training data over the time period of the data you withheld and see how compares it to the test data. This is standard model building stuff to do, right? Training and testing data are fantastic ways of withholding information in order to assess a model’s skill (as long as you have data to spare). ch4.train &lt;- window(ch4.ts, end = 2016) ch4.test &lt;- window(ch4.ts, start = 2016) ggplot() + geom_line(data=broom::tidy(ch4.train), mapping = aes(x=index,y=value), color=&quot;darkred&quot;) + geom_line(data=broom::tidy(ch4.test), mapping = aes(x=index,y=value), color=&quot;darkblue&quot;) + geom_vline(xintercept = 2016) + geom_text(aes(x=2016,y=1900,label=&quot;Train Test&quot;),nudge_x = -0.5) + labs(x=&quot;Date&quot;,y=expression(CH[4]~(ppb)), title=&quot;Global Monthly Mean Methane Concentration&quot;) + theme_minimal() 8.8.1.2 AR(\\(p\\)) Forecast And fit an AR(\\(p\\)) model via: ch4.ar &lt;- ar(ch4.train) ch4.ar ## ## Call: ## ar(x = ch4.train) ## ## Coefficients: ## 1 2 3 4 ## 1.0802 -0.1542 -0.0289 0.0891 ## ## Order selected 4 sigma^2 estimated as 71.72 Forecast the training data over the period of the testing data. Overlay the testing data on your plot. How do they compare visually? Now compare the predicted data to the withheld (test) data statistically (e.g., correlation and RMSE). Is this a “good” model? Can you explain what is going on with these predictions? 8.8.1.3 Holt-Winters Forecast And make a Holt-winters model too. ch4.hw &lt;- HoltWinters(ch4.train) ## Warning in HoltWinters(ch4.train): optimization difficulties: ERROR: ## ABNORMAL_TERMINATION_IN_LNSRCH ch4.hw ## Holt-Winters exponential smoothing with trend and additive seasonal component. ## ## Call: ## HoltWinters(x = ch4.train) ## ## Smoothing parameters: ## alpha: 0.8729582 ## beta : 0.01981042 ## gamma: 1 ## ## Coefficients: ## [,1] ## a 1839.8827680 ## b 0.5998914 ## s1 1.8281395 ## s2 1.3145093 ## s3 0.5110901 ## s4 -1.9972249 ## s5 -7.0008862 ## s6 -9.9788581 ## s7 -6.6378429 ## s8 -0.2315650 ## s9 4.5676653 ## s10 6.2974446 ## s11 4.8814883 ## s12 2.5872320 Forecast the training data over the period of the testing data. Overlay the testing data on your plot. How do they compare visually? Now compare the predicted data to the withheld (test) data statistically (e.g., correlation and RMSE). Is this a “good” model? Can you explain what is going on with these predictions? 8.8.2 Write Up and Reflect Pass in a R Markdown doc with your analysis. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? Please use these automatic functions with care. They are blind algorithms that just operate until their stopping criteria are met. With great power comes great responsibility. They optimize based on information criteria, not predictive power. E.g.,auto.arima uses AICc, AIC, or BIC to pick models. These penalize complexity, but not harshly enough when sample sizes are large or when noise structures are complex. A model can “look” optimal by those criteria yet still generalize poorly. By default, auto.arima will consider fairly large values for AR and MA terms (max.p = 5, max.q = 5), which can fit noise patterns. In short samples, this is especially risky. In time series, near-identical scores can come from very different models. Without domain insight, functions like auto.arima might favor a more complex model that explains idiosyncrasies and not the “truth.”↩︎ "],["cross-correlation.html", "9 Cross Correlation 9.1 Big Idea 9.2 Reading 9.3 Packages 9.4 Lags 9.5 From ACF to CCF 9.6 How Ccf() Normalizes Correlations 9.7 Your work", " 9 Cross Correlation 9.1 Big Idea Here we extend the concept of the autocorrelation function to a bivariate case of how y might be correlated with lagged values of x. 9.2 Reading Have a look at Chapter three from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great book for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 9.3 Packages I’ll use forecast (Hyndman et al. 2025) as well as tidyverse (Wickham 2023) syntax. library(tidyverse) library(forecast) 9.4 Lags Here is a quick primer on different ways of lagging data. n &lt;- 10 x &lt;- ts(rnorm(n)) x ## Time Series: ## Start = 1 ## End = 10 ## Frequency = 1 ## [1] 0.079854780 0.233410688 0.107206736 1.150652223 0.903326353 ## [6] -0.965635624 -0.146067990 -0.559301795 -0.214952105 0.001970085 # v1: Lag by hand xLag_v1 &lt;- ts(c(NA,x[-n])) # v2: use stats::lag (&quot;little el lag&quot;) # Note the sign of k: negative k starts later. xLag_v2 &lt;- stats::lag(x, k = -1) xLag_v2 ## Time Series: ## Start = 2 ## End = 11 ## Frequency = 1 ## [1] 0.079854780 0.233410688 0.107206736 1.150652223 0.903326353 ## [6] -0.965635624 -0.146067990 -0.559301795 -0.214952105 0.001970085 # v3: use Hmisc::Lag (&quot;big el lag&quot;) # Note that shift is specifies the number of observations # to be shifted to the right -- positive shift starts later. xLag_v3 &lt;- Hmisc::Lag(x, shift = 1) xLag_v3 ## Time Series: ## Start = 1 ## End = 10 ## Frequency = 1 ## [1] NA 0.07985478 0.23341069 0.10720674 1.15065222 0.90332635 ## [7] -0.96563562 -0.14606799 -0.55930180 -0.21495211 # ^^ This is the same behavior that dplyr::lag has. E.g., dplyr::lag(c(x),n=1) ## [1] NA 0.07985478 0.23341069 0.10720674 1.15065222 0.90332635 ## [7] -0.96563562 -0.14606799 -0.55930180 -0.21495211 # But dplyr::lag doesn&#39;t like ts objects, hence the c(x). # It&#39;s all gross to parse out, I know. tsp(xLag_v1) ## [1] 1 10 1 tsp(xLag_v2) ## [1] 2 11 1 tsp(xLag_v3) ## [1] 1 10 1 # ts.union helps you see what&#39;s going on ts.union(x,xLag_v1,xLag_v2,xLag_v3) ## Time Series: ## Start = 1 ## End = 11 ## Frequency = 1 ## x xLag_v1 xLag_v2 xLag_v3 ## 1 0.079854780 NA NA NA ## 2 0.233410688 0.07985478 0.079854780 0.07985478 ## 3 0.107206736 0.23341069 0.233410688 0.23341069 ## 4 1.150652223 0.10720674 0.107206736 0.10720674 ## 5 0.903326353 1.15065222 1.150652223 1.15065222 ## 6 -0.965635624 0.90332635 0.903326353 0.90332635 ## 7 -0.146067990 -0.96563562 -0.965635624 -0.96563562 ## 8 -0.559301795 -0.14606799 -0.146067990 -0.14606799 ## 9 -0.214952105 -0.55930180 -0.559301795 -0.55930180 ## 10 0.001970085 -0.21495211 -0.214952105 -0.21495211 ## 11 NA NA 0.001970085 NA 9.5 From ACF to CCF Chapter 3 introduces cross-correlation in the context of forecasting. Here, we reinforce the idea with a simple example and tie it back to the concept of autocorrelation. Autocorrelation is the correlation of a variable with its own past values (e.g., \\(y_t\\) with \\(y_{t-1}\\)). Cross-correlation extends this idea to two different time series, asking: how does \\(x_{t-k}\\) relate to \\(y_t\\)? You’re already used to the idea that you can correlate two variables \\(x\\) and \\(y\\), but you can also lag \\(x\\) like we did for autocorrelation. For example, \\(y_t\\) can be correlated to \\(x_{t-1}\\), which we can implement like this: cor(y[2:n], x[1:(n-1)]). Here’s a toy example: n &lt;- 100 x &lt;- rnorm(n) y &lt;- c(0, x[-n]) + rnorm(n, sd = 0.5) cor(x, y) # doesn&#39;t correlate ## [1] 0.1149159 cor(x[1:(n-1)], y[2:n]) # correlates great! ## [1] 0.8908105 Ccf(x, y) # the cross correlation plot By hand, you can see that \\(\\text{cor}(x_{t-1}, y_t) = 0.891\\), whereas \\(\\text{cor}(x_t, y_t) = 0.115\\). We can view these correlations across multiple lags using the cross-correlation plot from Ccf. That function returns the correlation between \\(x_{t+k}\\) and \\(y_t\\) for various values of lag \\(k\\). A positive lag on the plot means that x is lagged behind y (i.e., you’re correlating earlier y-values with later x-values), while a negative lag means x leads y — and might be useful for forecasting. This kind of lagged relationship is useful for predictive modeling, where we use past values of one variable to forecast the present (or future) values of another. Thus, given a time series like \\(x_t\\), we could forecast \\(y_t\\) based on \\(x_{t-1}\\): # lag x xLag &lt;- c(0, x[-n]) # note this does the same lag operation xLag &lt;- dplyr::lag(x, n = 1, default = 0) lmY &lt;- lm(y ~ xLag) plot(y, predict(lmY)) abline(lmY, col = &quot;blue&quot;) Cross-correlation is commonly used to identify short-lag predictive relationships. This is common in economics (e.g., jobless claims leading GDP) and in environmental science — for instance, where river discharge might precede changes in turbidity, or phytoplankton peaks might precede zooplankton booms. As with all time series correlations, beware of spurious relationships — especially when both series share strong seasonal or trend components. 9.6 How Ccf() Normalizes Correlations The eagle-eyed among you will notice that the values from Ccf don’t align exactly with what cor produces. For the detail oriented, here is the scoop. The ccf() function in R computes cross-correlations that are normalized in a specific way. It uses the full-series mean and standard deviation for both variables, even when the number of overlapping points decreases at higher lags. This section shows what that means in practice and how to manually reproduce the ccf() result at lag 1. Let’s create some data. n &lt;- 100 x &lt;- rnorm(n) # make y depend on lagged x (lag = 1) y &lt;- 0.8 * c(0, x[1:(n-1)]) + rnorm(n, sd = 0.5) # Lagged vectors aligned for lag 1 x_lag &lt;- x[1:(n - 1)] y_trunc &lt;- y[2:n] We construct y to depend on a lagged version of x, and use 0 as the first value to avoid introducing an NA. Let’s get the correlation at lag one using cor() cor(x_lag, y_trunc) ## [1] 0.8302037 This computes a standard Pearson correlation using only the overlapping values. It uses n - 1 observations and calculates means and standard deviations from that subset. Let’s compare that to what ccf() gives ccf_obj &lt;- Ccf(x, y, lag.max = 10) ccf_lag1 &lt;- ccf_obj$acf[ccf_obj$lag == -1] ccf_lag1 ## [1] 0.8253849 That correlation at lag 1 from ccf_lag1 is slightly different from the result of cor(x_lag, y_trunc) (0.8302037). Why? Because ccf(): - Uses only n - 1 data pairs at lag 1 - But computes the mean and standard deviation using all n values from the full series of x and y This ensures that correlation values at all lags are on the same scale. We can manually reproduce what ccf() does k &lt;- 1 # lag num &lt;- sum((x_lag - mean(x)) * (y_trunc - mean(y))) # numerator denom &lt;- (n - k) * sd(x) * sd(y) # denominator uses full-series SDs manual_ccf &lt;- num / denom manual_ccf ## [1] 0.8253849 This manual calculation matches the value from ccf() exactly. You might ask yourself: why does this matter? Well, this normalization approach allows ccf() (and acf()) to produce values that are comparable across different lags, even as the number of overlapping pairs shrinks. It’s a subtle point, but crucial for accurate interpretation of time series correlations. And when lags get really large like in the autotrophs and heterotrophs example, it’s worth understanding what happens under the hood. 9.7 Your work Look at the file PhytoGrazersExp1.rds. It contains data used in a mesocosm simulation. The producers time series shows relative biomass of phytoplankton while the grazers time series shows the zooplankton biomass. The times are indexed by day. pg &lt;- readRDS(&quot;data/PhytoGrazersExp1.rds&quot;) # make these wide data long pgLong &lt;- pg %&gt;% pivot_longer(cols=-1) # and make a nice ggplot ggplot(pgLong,aes(x=day,y=value,color=name)) + geom_line() + geom_smooth(se = FALSE) + labs(y=&quot;Relative biomass&quot;, x=&quot;Number of days from start of experiment&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; It seems clear that there is an offset between the two series. Just eyeballing it, it looks like there is a four to six week lag between the grazers and producers. Use cross correlation and see if you can determine the lagged relationship more specifically. The file PhytoExp2.rds shows you just the phytoplankton from another experiment. Can you forecast the grazers over that time period? That is, show how you can use the lagged relationship between producers and grazers you found above to model the grazers in PhytoExp2.rds. 9.7.1 Write Up and Reflect Pass in a R Markdown doc with your analysis. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? "],["regression.html", "10 Regression 10.1 Big Idea 10.2 Reading 10.3 Packages 10.4 Regression 10.5 Your work", " 10 Regression 10.1 Big Idea OLS regression can be problematic with time series because the assumption of independence gets violated pretty easily. But, there are other approaches that are useful. 10.2 Reading Have a look at Chapter five (through section 5.3) from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great book for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 10.3 Packages You’ll want to install the nlme (Pinheiro, Bates, and R Core Team 2025) library if you don’t have it. We are going to fit a linear model using generalized least squares as opposed to ordinary least squares and you’ll need the gls function from nlme. I’ll use forecast (Hyndman et al. 2025) as well and tidyverse (Wickham 2023). library(tidyverse) library(nlme) library(forecast) 10.4 Regression 10.4.1 Adjusting OLS As know, time series data are typically autocorrelated in some way. We’ve seen this over and over and we’ve learned to spot it and to fit models of various complexity to describe it. A very common situation with environmental data is to want to do an ordinary least squares regression using time series data. If your data are a time series, you are doing time-series regression. And that has different pitfalls than standard regression. The biggest issue is that if the residuals (errors) are autocorrelated, you have violated the assumption of independence. What does this mean? Well, the standard errors on the coefficient estimates tend to be underestimated (and the t-scores overestimated) when the autocorrelations of the errors at low lags are positive. This means that any statistical inference could be wrong, e.g., the the p-values from a t-test will be artificially low. But despair not! If we take the the time series structure of the errors into account we can usually still model the association between \\(x\\) and \\(y\\) effectively. Is this a little opaque? Let’s make up an example. Let’s say you wanted to build a model of salmon in a creek as function of the summer water temperature. Temps get too warm and the wee fish suffer and die before they get counted by the poor intern. You can make up a mechanism however you like. This is just a thought experiment, right? So you make a nice linear model \\(y=\\beta_0 + \\beta_1 x + \\epsilon\\) where \\(y\\) is the number of salmon, \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the coefficient on the variable \\(x\\) which is the water temperature, and \\(\\epsilon\\) is the residual. You have a gorgeous time series every year (\\(t\\)) for some number of years which makes this a time series regression: \\(y_t=\\beta_0 + \\beta_1 x_t + \\epsilon_t\\). You run an OLS regression and get a negative slope and a t-test says that your model has skill (good \\(R^2\\), \\(p&lt;0.05\\), etc). Great – you fire off a letter to Nature, or write your thesis, or go put some wood in a stream, or just bathe in the satisfaction of a job well done. But of course your model won’t fit perfectly so you’ll have residuals or errors from the model (\\(\\epsilon\\)). Now, if those residuals are not independent and identically distributed (~iid), you have violated the assumptions of the OLS regression. The standard errors might be wrong, the p-values might be inflated. Alas. The classic reason that residuals are not ~iid in time series is because they are not independent and that means autocorrelation. In general here is a good approach when dealing with time series variables in a regression. Start by doing an ordinary least squares regression. Look at the AR structure of the residuals with ACF and PACF plots. If the residuals look clean then carry on. Note that it’s OK for the variable \\(y\\) or \\(x\\) to be autocorrelated – watch the residuals. If the residuals have AR structure, estimate and diagnose an appropriate AR or ARMA model. If needed, get the adjusted regression coefficients (estimates, standard errors). We want to have a model that adjusts for the AR structure in the residuals. We can create an adjusted data set by hand and use OLS (see text) or use GLS with an appropriate correlation structure (below). Let’s create a data set to play with. We will generate two time series. The variable x will be ordinary white noise. The variable y will be a function of x plus AR(1) noise \\(\\epsilon\\). We will pretend we don’t know about \\(\\epsilon\\) and perform two regressions of \\(y=f(x)\\) and look at the residuals and the estimate of the slope (\\(\\hat\\beta\\)). set.seed(47) # for reproducibility n &lt;- 100 phi &lt;- 0.8 x &lt;- ts(rnorm(n)) epsilon &lt;- arima.sim(model = list(ar=phi),n = n) epsilon &lt;- epsilon - mean(epsilon) # demean B0 &lt;- 0 # intercept B1 &lt;- 0.5 # slope y &lt;- B0 + B1*x + epsilon # step 1 - the regular ols regression ols1 &lt;- lm(y~x) # step 2 - inspect the residuals Acf(residuals(ols1)) # problem with independence Pacf(residuals(ols1)) # looks like an AR1 # step 3 - get a good model ar1 &lt;- ar(residuals(ols1))$ar ar1 ## [1] 0.7429079 # two ways of getting a model that works # step 4 - method 1 - adjust by hand # y lagged at one time step: # (see poscript below of notes on lagging) y.lag1 &lt;- c(NA,y[-n]) head(cbind(y,y.lag1)) ## Time Series: ## Start = 1 ## End = 6 ## Frequency = 1 ## y y.lag1 ## 1 1.27400585 NA ## 2 -0.66681772 1.27400585 ## 3 -0.03946659 -0.66681772 ## 4 1.07026783 -0.03946659 ## 5 0.52585722 1.07026783 ## 6 0.86750649 0.52585722 y2 &lt;- y - ar1*y.lag1 # Use ar1 to make y white noise x.lag1 &lt;- c(NA,x[-n]) # x lagged at one time step x2 &lt;- x - ar1*x.lag1 # Use ar1 to make x white noise ols2 &lt;- lm(y2~x2) Acf(residuals(ols2)) # clean 10.4.2 GLS We have all been taught OLS. OLS is a method for estimating the unknown parameters in a linear regression model – e.g., estimating the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)) of a line in the formula \\(y=\\beta_0 + \\beta_1x\\). OLS works by choosing the parameters of a linear function by minimizing the sum of the squares of the differences between the observed dependent variable (\\(y\\)) in the given data set and those predicted by the linear function (e.g., \\(y=\\beta_0 + \\beta_1x\\) in the example above, but note that you can have more than one independent variable in a multiple regression). If conditions like homoscedasticity, independence, and normality are met, the OLS estimators are optimal (often called BLUE – Best Linear Unbiased Estimators). In other words, when the assumptions are met, OLS works. OLS has a nice property that teaching it is relatively simple and can be done without a bunch of matrix algebra. You can calculate the sum of squares by hand and in intro stats courses we relate OLS to simpler concepts like correlation. The other common least squares approach is generalized least squares (GLS). GLS is a method of estimation of a linear process which accounts for heteroscedasticity and structure in the error term. If OLS is used for homoscedastic regressions (i.e. \\(y\\) has the same variance for each \\(x\\)), GLS is used for heteroscedastic regression (different variances). GLS can also be used for correlated (non-independent) residuals. The theory with GLS is that the residuals can be transformed so that the variances are equal and uncorrelated. You can think of it like this: OLS minimizes the sum of the squared distances between the observed and predicted values assuming uncorrelated and equal variance in errors. GLS minimizes the squared distances relative to the covariance structure of the residuals. Formally, the GLS estimator is: \\[ \\hat{\\beta}_{GLS} = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} y \\] where \\(\\Sigma\\) is the error covariance matrix. When using gls() from the nlme package in R, estimation is typically done via Restricted Maximum Likelihood (REML), which is particularly useful when estimating variance parameters in the presence of correlation. But the core idea is still weighted least squares — just with more structure in how we define those weights. In spatial and time-series analysis, the error term is often autocorrelated. So, if the important difference between OLS and GLS is the assumptions made about the error term of the model, we can use GLS to allow for autocorrelated errors and avoid having inefficient estimation of the unknown parameters in a linear model. Why not always use GLS over OLS?. It’s a good question and I think of it along the lines of the KISS principle. If you meet the OLS assumptions, use it. If you don’t then look for alternatives like GLS. Also, GLS requires you to know (or correctly guess) the structure of the residual correlations — if you get that wrong, you might do more harm than good. We will start by assuming epsilon follows an iid process (e.g., \\(N(0,1)\\)) and we will fit a model with uncorrelated errors. E.g., here we use gls but this gives the same info as lm would because gls assumes iid errors unless told otherwise. # step 1 - the regular ols regression but fit with gls rather than lm gls1 &lt;- gls(y~x) summary(gls1) ## Generalized least squares fit by REML ## Model: y ~ x ## Data: NULL ## AIC BIC logLik ## 395.8891 403.644 -194.9446 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 0.01618039 0.1690315 0.0957241 0.9239 ## x 0.18575084 0.1726484 1.0758912 0.2846 ## ## Correlation: ## (Intr) ## x -0.053 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.4730783 -0.3348105 0.1230050 0.5216485 2.3144795 ## ## Residual standard error: 1.687976 ## Degrees of freedom: 100 total; 98 residual # step 2 - inspect the residuals Acf(residuals(gls1)) # problem with independence Pacf(residuals(gls1)) # looks like an AR1 The residuals of this model are autocorrelated as an AR(1) process from looking at the plots. Thus, we have violated the assumption of independent residuals. But do not despair. We will use a GLS model that allows us to specify an AR(1) correlation structure for the residuals and perform hypothesis testing in the presence of this temporal structure. Doing so, will give us new and improved estimates of the parameters. We will thus sleep well knowing in our hearts that we have done good. We will do this by updating the gls1 object and specifying that errors follow an autoregressive process with the corARMA function. By specifying corARMA(p=1) we are asking nlme to fit an AR(1) error structure. An AR(1) assumes that residuals closer in time are more similar and the correlation decays geometrically. If we had residuals that followed a different process (e.g., an ARMA(1,1) model) we could try to fit that with corARMA(p=1, q=1). The update function will then try to estimate the AR and MA parameters using numerical optimization. This can fail when models are too complex (e.g., too high an order). # step 3 - update the model with an appropriate correlation structure cs1 &lt;- corARMA(p=1) gls2 &lt;- update(gls1,correlation=cs1) summary(gls2) ## Generalized least squares fit by REML ## Model: y ~ x ## Data: NULL ## AIC BIC logLik ## 307.26 317.5998 -149.63 ## ## Correlation Structure: AR(1) ## Formula: ~1 ## Parameter estimate(s): ## Phi ## 0.7946776 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) -0.0697539 0.5035316 -0.138529 0.8901 ## x 0.4496653 0.0948284 4.741882 0.0000 ## ## Correlation: ## (Intr) ## x -0.023 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.4516182 -0.4319559 0.1445074 0.5484367 2.1868956 ## ## Residual standard error: 1.767387 ## Degrees of freedom: 100 total; 98 residual # step 4 - and inspect the residuals. Note type=&quot;normalized&quot; # in the calls to `residuals`. Acf(residuals(gls2,type=&quot;normalized&quot;)) # clean! We ask for the “normalized” residuals above because they are the transformed residuals that account for the error correlation and heteroscedasticity structure we specified. They’re essentially standardized residuals that assume your model’s covariance matrix is correct. Zooming out a bit. We fit a linear model of the form \\(y=\\beta_0 + \\beta_1x\\) using GLS and got out estimates for the intercept (\\(\\hat\\beta_0\\)) and slope (\\(\\hat\\beta_1\\)). First, we assumed that the error term was uncorrelated. Looking at summary, we can see the the estimate of the slope (\\(\\hat\\beta_1\\)) is 0.186 and significantly different than zero. We also have a variety of goodness of fit statistics and some other goodies. I’ll walk through those in the video. The important thing to wrap your head around here is that we fit GLS with uncorrelated errors meaning it’s essentially identical to OLS. Compare the above to the more familiar summary(lm(y~x)). So what’s the big deal? We started with a model that showed x as a significant predictor of y. We ended up, after much wailing and rending of garments, with a model that showed x as a significant predictor of y. Is it a better model? Was it worth the trouble? Let’s consider a few things. anova(gls1,gls2) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls1 1 3 395.8891 403.6440 -194.9445 ## gls2 2 4 307.2600 317.5998 -149.6300 1 vs 2 90.62914 &lt;.0001 The GLS model with the correlated residuals has a lower AIC and BIC than the naive uncorrelated model. The original function we used to make \\(y\\) was \\(y=\\beta_0 + \\beta_1x + \\epsilon\\) where \\(\\beta_0 = 0\\) and \\(\\beta_1\\) = 0.5. Both the GLS models got the intercept right (-0.07 for gls2 and 0.02 for gls1). But the GLS model with the correlated residuals does a better job of getting the true value of the slope (0.45 vs 0.19). The error estimate on \\(\\beta_1\\) was lower on the model with correlated residuals than the naive uncorrelated model (0.0948 vs 0.1726). This is because GLS uses the correct structure of error variance and correlation, which improves efficiency of parameter estimation. Finally, we did it “right” – that should be satisfying. What would have happened if \\(\\beta_1\\) had been lower? That is, what if \\(y\\) was made up of more noise and less \\(x\\)? E.g., \\(y=\\beta_0 + 0.5(\\beta_1)x + 2(\\epsilon)\\)? This model, with 100 points should be pretty robust on the simple hypothesis test on the slope (H\\(_1\\): \\(\\beta_1 \\neq 0\\)) but we know that parameter estimation is inefficient with autocorrelated residuals so having clean residuals and good parameter estimation is important. There is a lot to learn about how the nlme library goes about fitting the correlation structures. The definitive book on the subject is Pinheiro and Bates (2000) and should be something you look to for more information. Zuur’s 2009 book “Mixed Effects Models and Extensions in Ecology with R” is also quite good. 10.5 Your work 10.5.1 What Does Generalized Mean? Below I’m going to ask you to compare GLS and OLS using some real data. But first, write a paragraph that explains the difference between OLS and GLS. What is a generalized model? 10.5.2 Regression Simulation Now, revisit the code above and adjust n, B1, and phi to see how the results of an OLS model change. Is OLS or GLS better at capturing the true value of B1 when there are non-independent errors? Or is OLS robust to moderate autocorrelation? You can do this in a full blown simulation of course, but you can just mess around with the values a bit and try to get a feel for it. 10.5.3 Regression for real Finally, let’s look at some data. A great paper by Connie Woodhouse and colleagues looks at how annual flows on the Colorado River are affected by precipitation, soil moisture, and temperature. Read the paper. It’s great and short. dat &lt;- readRDS(&quot;data/woodhouse.rds&quot;) dat &lt;- dat %&gt;% mutate(MAF = LeesWYflow / 1e6) # add a 5-yr moving average dat &lt;- dat %&gt;% mutate(MAF5yr = c(stats::filter(MAF,filter = rep(0.2,5)))) ggplot(data=dat, aes(x=Year)) + geom_line(aes(y=MAF),color=&quot;lightgreen&quot;) + geom_line(aes(y=MAF5yr),color=&quot;darkgreen&quot;) + labs(y= &quot;Millions of Acre Feet&quot;, title=&quot;Colorado River at Lee&#39;s Ferry&quot;, sub = &quot;Total Flow for Water Year&quot;) + scale_x_continuous(expand=c(0,0)) ## Warning: Removed 4 rows containing missing values or values outside the scale range ## (`geom_line()`). The main gist of the paper is that precipitation explains most of the variability in the river’s flow but that temperature is important under certain conditions. E.g., “Different combinations of temperature, precipitation, and soil moisture can result in flow deficits of similar magnitude, but recent droughts have been amplified by warmer temperatures that exacerbate the effects of relatively modest precipitation deficits.” This is cool: Transpiration and evaporation are greater when it’s warmer, right? The cool thing that Woodhouse et al. have done is tease out the relationship between precipitation and temperature in a new way. The data they used are online and I’ve grabbed the relevant info and put it in the file woodhouse.rds as a data.frame. Explore the data (go back to week one for ideas of what to do – plot your data!) and eventually make a model of river flow (LeesWYflow) as a function of October to April precipitation (OctAprP). The units on flow are cumulative flow over the water year in acre feet and cumulative October to April precipitation is in mm. Is it a good model? How do the residuals look in a time series context? Interpret. Does a GLS approach change your interpretation from an OLS approach? Given your work with the simulated data above do you think Woodhouse et al. are on safe footing with OLS? 10.5.4 Write Up and Reflect Pass in a R Markdown doc with your analysis. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? "],["filtering-smoothing.html", "11 Filtering, smoothing 11.1 Big Idea 11.2 Packages 11.3 Filters as visual aids 11.4 Using filters 11.5 The Case of the Poorly-Launched Sensors 11.6 Your work", " 11 Filtering, smoothing 11.1 Big Idea Very often one wants to smooth or filter data to analyze or work only with particular aspects of the data. This kind of analysis is a big deal in time-series analysis. Smoothing is a way of isolating different frequencies of data but still thinking in the time domain. 11.2 Packages You will want the dplR (Bunn et al. 2025) package – a subtle work of staggering genius – as well as zoo (Zeileis, Grothendieck, and Ryan 2025) and hydroTSM (Zambrano-Bigiarini 2024). library(tidyverse) library(dplR) library(hydroTSM) library(zoo) 11.3 Filters as visual aids Recall the very simple moving average filters we made earlier in class? Let’s revisit. People who work with time series are forever adding smooth lines to their plots in order to highlight signal and de-emphasize noise. It’s like a sickness. And everybody has a different way of doing it. We already looked at moving averages using the filter() function. Let’s just take a quick peek at some other common ones. We’ve been using a lot simulated data, which is something I really like to do. It’s good for setting up situations where we want to see if we understand how a method works. But let’s use some tree-ring data this time to demonstrate a few different smoothing options. This is a tree-ring chronology from near the arctic treeline in Canada that is one of the onboard data sets in dplR. I’ve pulled out the years for plotting. data(cana157) dat &lt;- tibble(yrs = time(cana157), Z = scale(cana157[,1])) nyrs &lt;- nrow(dat) p1 &lt;- ggplot() + geom_hline(yintercept = 0) + geom_line(data=dat, mapping = aes(x=yrs,y=Z), color = &quot;grey&quot;) + labs(x=&quot;Year&quot;,y=&quot;Tree Growth (Z-Score)&quot;, title=&quot;Twisted Tree Heartrot Hill Tree-Ring Chronology&quot;) + theme_minimal() p1 11.3.1 Moving (running) average We’ve already looked at moving averages (aka running averages). Let’s revisit. These have the advantage of being dirt simple. Below we will see that they definitely emphasize low frequency in the examples below (at 20, 50, and 100 years) but retain some jaggedness too. sfPal &lt;- PNWColors::pnw_palette(name=&quot;Starfish&quot;,n=3,type=&quot;discrete&quot;) dat &lt;- dat %&gt;% mutate(ma20 = c(stats::filter(x=Z, filter=rep(x=1/20,times=20), sides=2)), ma50 = c(stats::filter(x=Z, filter=rep(x=1/50,times=50), sides=2)), ma100 = c(stats::filter(x=Z, filter=rep(x=1/100,times=100), sides=2))) p1 + labs(subtitle = &quot;Moving Average Filters&quot;) + geom_line(data=dat, mapping = aes(x=yrs,y=ma20), color = sfPal[1],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=ma50), color = sfPal[2],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=ma100), color = sfPal[3],linewidth=1) ## Warning: Removed 19 rows containing missing values or values outside the scale range ## (`geom_line()`). ## Warning: Removed 49 rows containing missing values or values outside the scale range ## (`geom_line()`). ## Warning: Removed 99 rows containing missing values or values outside the scale range ## (`geom_line()`). 11.3.2 Hanning The Hanning filter is similar to the moving average in the sense that the curve emphasizes low frequency variability and loses the jaggedness over the moving average. It’s also a simple filter (look at the code by typing hanning at the R prompt) but it is a start into thinking in the frequency domain. It’s part of a family of functions called window functions that are zero-valued outside of some interval chosen by the user. It’s used quite a bit by time-series wonks and it is implemented in dplR with the function hanning(). I’ll skip the theory here but it’s a great precursor to the work we will do with spectral analysis next week. dat &lt;- dat %&gt;% mutate(han20 = hanning(Z,n=20), han50 = hanning(Z,n=50), han100 = hanning(Z,n=100)) p1 + labs(subtitle = &quot;Hanning Filters&quot;) + geom_line(data=dat, mapping = aes(x=yrs,y=han20), color = sfPal[1],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=han50), color = sfPal[2],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=han100), color = sfPal[3],linewidth=1) ## Warning: Removed 19 rows containing missing values or values outside the scale range ## (`geom_line()`). ## Warning: Removed 49 rows containing missing values or values outside the scale range ## (`geom_line()`). ## Warning: Removed 99 rows containing missing values or values outside the scale range ## (`geom_line()`). Like the moving average the smooth is shorter than the input data. It’s too bad because its nice to preserve the ends of the data but there is much wringing of hands and gnashing of teeth about the “right” way of running a smoothing algorithm where the data are sparse. This so-called end-member problem is the subject of a lot of work in the time-series literature. Let’s look at dat. Here are the first 75 rows in a sortable table. DT::datatable(round(dat,2),options = list(pageLength = 75)) 11.3.3 Smoothing splines The function smooth.spline() fits a smooth curve to a set of observations using piecewise polynomials. Splines are used very commonly in time-series work both for smoothing and interpolating. We will look at interpolation below. The spar parameter is the one you can play with in smooth.spline(). Larger numbers smooth the data more heavily. Sadly the spar doesn’t translate into time units very easily. dat &lt;- dat %&gt;% mutate(ss0.2 = smooth.spline(Z,spar=0.2)$y, ss0.4 = smooth.spline(Z,spar=0.4)$y, ss0.6 = smooth.spline(Z,spar=0.6)$y) p1 + labs(subtitle = &quot;Smoothing Spline Filters&quot;) + geom_line(data=dat, mapping = aes(x=yrs,y=ss0.2), color = sfPal[1],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=ss0.4), color = sfPal[2],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=ss0.6), color = sfPal[3],linewidth=1) 11.3.4 Loess I’ve become increasingly fond of the loess smoother which uses a local, linear polynomial fit to smooth data. The parameter to adjust is the span span. Because span is the proportion of points used in the smoothing, the smaller the number the stiffer the curve will be. So a span of 0.05 would use 5% of the points. We can make the curves similar to the number of years in our moving average or hanning filters with some division. dat &lt;- dat %&gt;% mutate(f20.lo = loess(Z~yrs, span = 20/nyrs)$fitted, f50.lo = loess(Z~yrs, span = 50/nyrs)$fitted, f100.lo = loess(Z~yrs, span = 100/nyrs)$fitted) p1 + labs(subtitle = &quot;Loess Filters&quot;) + geom_line(data=dat, mapping = aes(x=yrs,y=f20.lo), color = sfPal[1],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=f50.lo), color = sfPal[2],linewidth=1) + geom_line(data=dat, mapping = aes(x=yrs,y=f100.lo), color = sfPal[3],linewidth=1) Unlike the moving averages and hanning filters, the smooth is the same length as the input data. That’s nice…but preserving the ends of the data means that there is less information at play on the ends than there is in the middle. There is no way to model the ends members to everybody’s satisfaction. Let’s look at dat again. DT::datatable(round(dat,2),options = list(pageLength = 75)) 11.4 Using filters The work above shows how to look at a time series at lower frequencies – it was eye candy. But what if we wanted to use those filters in some kind of analytic way? Here are some examples. 11.4.1 Aggregating Sometimes you have data at one resolution but want it at another (coarser) resolution. E.g., you have hourly counts of fish escapement but want to sum up the hours in a day to get the total fish per day or average it to get fish per hour. We’ve done this already but here is an example worth revisiting. Let’s go back to the Bellingham airport data. The file kbliMonthlyTavg.rds contains a zoo object with monthly average temperatures from Bellingham. We will use filter, rollmean, and dmseasonalto get summer temperature. Here are three ways of doing the same thing. I’ll work with ts, and zoo objects. And pull in the dm2seasonal function from the wonderful hydroTSM package. tavgZoo &lt;- readRDS(&quot;data/kbliMonthlyTavg.rds&quot;) tavgTS &lt;- ts(tavgZoo,start=c(1941,1),frequency = 12) # let&#39;s look at a few years plot(window(tavgTS,start=2000, end = 2004), ylab=expression(degree*C)) # get summer (JJA) temps three ways # 1. using filter on a ts object tmp &lt;- stats::filter(tavgTS, rep(1/3,3), sides = 1) summerTavg_v1 &lt;- tmp[seq(8,length(tmp),by=12)] # 2. using zoo -- essentially the exact same as above tmp &lt;- rollmean(tavgZoo,k=3, align = &quot;right&quot;) summerTavg_v2 &lt;- tmp[months(time(tmp)) == &quot;August&quot;] # 3. using hydroTSM summerTavg_v3 &lt;- hydroTSM::dm2seasonal(tavgZoo,season=&quot;JJA&quot;,FUN=mean) # note they are all the same head(summerTavg_v1) ## [1] 15.71615 16.04912 14.60194 15.79032 14.75443 15.26229 head(summerTavg_v2) ## 1941-08-01 1942-08-01 1943-08-01 1944-08-01 1945-08-01 1946-08-01 ## 15.71615 16.04912 14.60194 15.67921 15.10165 15.42063 head(summerTavg_v3) ## 1941 1942 1943 1944 1945 1946 ## 15.71615 16.04912 14.60194 15.67921 15.10165 15.42063 # Looks like it is getting warmer. plot(summerTavg_v2,ylab=expression(degree*C),xlab=&quot;Time&quot;) abline(lm(summerTavg_v2~time(summerTavg_v2)),col=&quot;red&quot;) Go through the code and make sure you follow what just happened at each step. 11.4.2 Removing low frequency variability Sometimes we use filters as visual aids that help us see certain aspects of the data. Other times we want to work only with low or high frequency data. A common way of removing some kinds of variability is to take residuals from a filter to remove, say, low-frequency variability. For instance, a perennial problem in working with tree-ring data is the problem of detrending the biological growth curve from the signal that you usually want – the year-to-year variability caused by, say, plant available water. Smaller trees have larger rings solely as a function of geometry. This leads to a natural negative exponential decay in growth as the tree ages that has nothing to do with climate. We can remove that signal by fitting a loess curve and then taking the residuals via division (subtraction can be used too). Dendrochronologists call this the ring-width index. data(co021) core1 &lt;- tibble(yr = time(co021), mm = co021$`641114`) core1 &lt;- core1 %&gt;% drop_na() core1 ## # A tibble: 694 × 2 ## yr mm ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1270 1.48 ## 2 1271 2.33 ## 3 1272 1.32 ## 4 1273 0.67 ## 5 1274 0.82 ## 6 1275 1.22 ## 7 1276 0.52 ## 8 1277 0.57 ## 9 1278 0.4 ## 10 1279 0.65 ## # ℹ 684 more rows n &lt;- nrow(core1) ggplot(data=core1) + geom_line(mapping = aes(x = yr, y = mm)) + labs(x=&quot;Year&quot;, y = &quot;Ring Width (mm)&quot;, subtitle = &quot;Sample 641114 Schulman Old Tree No. 1, Mesa Verde&quot;) We will fit a loess model of ring wing width as a function of time. Note this is a first-order polynomial (degree = 1) while the default is to use a 2nd-order polynomial. The argument span gives the percentage of the points used. Since there are 694 this corresponds to 69 years being used at each fit. loessModel &lt;- loess(formula = mm ~ yr, data = core1, degree = 1, span = 0.05) loessFit &lt;- broom::augment(loessModel) Because the output object from loessModel model is, well, yucky, I used augment from broom to put it in a nicer format. Here is a plot of the fit. loessFit %&gt;% ggplot() + geom_line(mapping = aes(x = yr, y = mm), color=&quot;grey&quot;) + geom_line(mapping = aes(x = yr, y = .fitted), color = &quot;black&quot;) + labs(x=&quot;Year&quot;, y = &quot;Ring Width (mm)&quot;, caption = &quot;5% loess smooth&quot;, subtitle = &quot;Sample 641114 Schulman Old Tree No. 1, Mesa Verde&quot;) If instead of a proportion of the data, we can specify a number of points (years) to use like we did above. E.g., 100-year smooth. Is 100 years the right value? Nobody can really say. But this is one approach for filtering out this relatively low-frequency (100 period = 0.01 f). loessModel &lt;- loess(formula = mm ~ yr, data = core1, degree = 1, span = 100/n) loessFit &lt;- broom::augment(loessModel) loessFit %&gt;% ggplot() + geom_line(mapping = aes(x = yr, y = mm), color=&quot;grey&quot;) + geom_line(mapping = aes(x = yr, y = .fitted), color = &quot;black&quot;) + labs(x=&quot;Year&quot;, y = &quot;Ring Width (mm)&quot;, caption = &quot;100-year loess smooth&quot;, subtitle = &quot;Sample 641114 Schulman Old Tree No. 1, Mesa Verde&quot;) We can make the RWI via: core1$rwi &lt;- loessFit$mm/loessFit$.fitted names(core1) &lt;- c(&quot;Year&quot;, &quot;Ring Width (mm)&quot;,&quot; Ring Width Index (RWI)&quot;) core1 %&gt;% pivot_longer(cols=-Year) %&gt;% ggplot(mapping = aes(x = Year, y = value)) + geom_line() + labs(x=&quot;Year&quot;, y = element_blank(), subtitle = &quot;Sample 641114 Schulman Old Tree No. 1, Mesa Verde&quot;) + facet_grid(rows=vars(name),scales = &quot;free&quot;) 11.5 The Case of the Poorly-Launched Sensors Let’s look at another use for filters. You have a critical bit weather data from a temperature logger and a solar radiation sensor that you inherited for your work. Your thesis, tenure packet, soul, whatever depend on faithful analysis. As is common with inherited data, you can’t control the quality. It turns out that these sensors were deployed by a hapless undergrad. The loggers were supposed to be deployed to record hourly over the summer of 2014. They were supposed to be launched synchronously on June 1 at midnight. Beware! The good-for-nothing adviser didn’t read the manual or spend enough time with the aforementioned undergrad. Regrettably, the temperature data started collecting at three minutes past the hour and collected every other hour. The radiation sensor was launched correctly but the cable wasn’t connected well enough and as a result there are a bunch of missing data over the summer. Load the data in as two data.frame objects. Here is a summary and plot. rad &lt;- readRDS(&quot;data/rad.rds&quot;) tmp &lt;- readRDS(&quot;data/tmp.rds&quot;) summary(tmp[,2]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -5.818 7.889 10.490 10.537 13.432 21.640 summary(rad[,2]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0 0.0 49.5 280.0 530.0 1239.0 220 pTmp &lt;- ggplot(tmp) + geom_line(mapping = aes(x=DateTime,y=tmp)) + labs(x=&quot;Date&quot;,y=expression(degree * C), subtitle = &quot;Air Temperature&quot;) + theme_minimal() pRad &lt;- ggplot(rad) + geom_line(mapping = aes(x=DateTime,y=rad)) + labs(x=&quot;Date&quot;,y=expression(W~m^-2), subtitle = &quot;Radiation&quot;) + theme_minimal() gridExtra::grid.arrange(pTmp,pRad) Here is a better look at the times: head(tmp$DateTime) ## [1] &quot;2014-06-01 00:03:00 UTC&quot; &quot;2014-06-01 02:03:00 UTC&quot; ## [3] &quot;2014-06-01 04:03:00 UTC&quot; &quot;2014-06-01 06:03:00 UTC&quot; ## [5] &quot;2014-06-01 08:03:00 UTC&quot; &quot;2014-06-01 10:03:00 UTC&quot; head(rad$DateTime) ## [1] &quot;2014-06-01 00:00:00 UTC&quot; &quot;2014-06-01 01:00:00 UTC&quot; ## [3] &quot;2014-06-01 02:00:00 UTC&quot; &quot;2014-06-01 03:00:00 UTC&quot; ## [5] &quot;2014-06-01 04:00:00 UTC&quot; &quot;2014-06-01 05:00:00 UTC&quot; 11.5.1 Fix time on temperature The times on the temperature data are pretty easy to fix. They are off by three minutes from what you wanted. Now three minutes is not a big deal. Let’s just snap those back to the hour. We can do that easily buy subtracting 180 seconds from each one. The dates and times in the temperature data.frame is a POSIXct object. Internally, these are a measure of seconds from an origin (usually January 1, 1970). Just subtract the requisite number of seconds: tmp$DateTime &lt;- tmp$DateTime - 180 head(tmp$DateTime) ## [1] &quot;2014-06-01 00:00:00 UTC&quot; &quot;2014-06-01 02:00:00 UTC&quot; ## [3] &quot;2014-06-01 04:00:00 UTC&quot; &quot;2014-06-01 06:00:00 UTC&quot; ## [5] &quot;2014-06-01 08:00:00 UTC&quot; &quot;2014-06-01 10:00:00 UTC&quot; Times are fixed. That was easy. 11.5.2 Fill missing data on radiation Ok. That was a fun excursion working with zoo. But let’s use filters to impute data gaps. About 10% of the data are missing on the radiation data.frame. Curses. Let’s interpolate it. First we will make it a zoo object and make a plot of the first 100 observations so we can see the problem more clearly. zooRad &lt;- zoo(rad[,2],rad[,1]) # let&#39;s look at the first 100 observations. plot(zooRad[1:100],type=&quot;b&quot;,pch=20,ylab=expression(W~m^-2)) The zoo package has several ways of filling NA values. Let’s run a linear interpolation and a cubic smoothing spline. In this case, the results are very similar because the radiation data are quite well behaved (sun comes up and sun goes down). zooRad.approx &lt;- na.approx(zooRad) zooRad.spline &lt;- na.spline(zooRad) plot(zooRad.approx[1:100],type=&quot;b&quot;,pch=20,ylab=expression(W~m^-2),col=&quot;blue&quot;,lwd=2) lines(zooRad.spline[1:100],lwd=2,col=&quot;red&quot;,lty=&quot;dotted&quot;) lines(zooRad[1:100],col=&quot;grey&quot;,lwd=1) That was easy too. Note the different ways of filling in the missing data. Look at na.locf as well. They all use some kind of filtering to impute the missing data. 11.6 Your work 11.6.1 Filter Fit some smooths to the data below. What’s looks good? What feels “right” in terms of a good filter? n &lt;- 500 dat &lt;- tibble(x = 1:n, y = c(arima.sim(model = list(order = c(2,0,1), ar=c(0.7, -0.2), ma = -0.4), n=n))) dat$y &lt;- dat$y + 0.3 * sin(2 * pi / 100 * dat$x) + 0.5 * sin(2 * pi / 200 * dat$x) ggplot(data = dat,mapping = aes(x=x,y=y)) + geom_line() 11.6.2 Revisit the case of the poorly launched sensors In the poorly launched sensors debacle there were a few problems. The radiation data was spotty but collecting every hour (2208 observations). The temp data was collecting every other hour (1104 observations) AND doing so at three minutes after the hour. We fixed the second part but not the first one. Your task is to align these data sets. They have to be hourly resolution for some reason you can invent. Figure out a way to take the tmp object above and change it to have hourly rather than two hourly resolution. I can think of several ways to do it but I’m going to leave you up to your own devices to figure out a way to do it. When you are done the radiation and temperature data should be aligned with 2208 observations each. 11.6.3 Write Up and Reflect Pass in a R Markdown doc with your analysis. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? "],["the-freqs-come-out-at-night.html", "12 The Freqs Come out at Night 12.1 Big Idea 12.2 Reading 12.3 Packages 12.4 The Frequency Domain 12.5 Sunspot Example 12.6 Your work 12.7 References 12.8 Postscript: A Glimpse of Wavelets", " 12 The Freqs Come out at Night 12.1 Big Idea We move from the time domain to the frequency domain. That means that we think about data in terms of the embedded periodicities rather than as a squiggly line. It’s head scratcher at first but worth the effort. 12.2 Reading Have a look at Chapter nine from Cowpertwait and Metcalfe (2009). It’s OK to skim the readings in this book. It’s not a great book for our purposes as many of you haven’t taken linear algebra and the book occasionally goes that way. But it’s useful to hum your way through the chapter nonetheless. 12.3 Packages We will just use the tidyverse (Wickham 2023) for the main part. I’ll show you things using dplR (Bunn et al. 2025) and waveslim (Whitcher 2024) too. library(tidyverse) 12.4 The Frequency Domain Thus far, we’ve looked at time series as a squiggly line—changes in some variable as a function of time. This is the time domain view, where the horizontal axis is time and our focus is on detecting trends, abrupt shifts, autocorrelation, and so on. This approach works well when events unfold sequentially and our interest lies in when things happen. But there’s another, equally powerful way to look at time series: the frequency domain. Here, we shift perspective and ask: What cycles are embedded in this series? Rather than asking when something happened, we ask how often it tends to happen. In the frequency domain, we represent a time series in terms of its periodic components—the frequencies that “make up” the data. The result is a view of variance as a function of frequency: how much of the ups and downs in the data are due to processes operating at different time scales. This is more than just a mathematical trick. Many natural and environmental systems are driven by periodic forcing—think of daily, seasonal, tidal, or orbital cycles. When we work in the frequency domain, we often get a clearer view of these underlying processes. It can help us detect signals that are hard to see in the time domain, especially when they’re buried in noise. Our main tool for this is spectral analysis, which lets us decompose a signal into its component frequencies. The idea dates back to Joseph Fourier, who showed that any periodic signal can be expressed as a sum of sine and cosine waves—what we now call a Fourier series. In fact, even signals that are not obviously periodic can be approximated as sums of sinusoids. A time series that looks messy and irregular may just be a combination of overlapping cycles operating at different frequencies and amplitudes. The mathematical tool that performs this decomposition is the Fourier transform. This is the same technology behind MP3 audio compression and JPEG images: representing complex information as the sum of simple waves. In our case, we’ll use it to analyze time series and detect dominant periodicities—what cycles are strong, which are weak, and which are just noise. As always, the goal is not just to describe patterns, but to get closer to mechanism. If we can link frequency-domain patterns to known physical drivers—like El Niño, solar cycles, or Milankovitch forcing—we gain a deeper understanding of how environmental systems behave. 12.4.1 Same Data, Two Domains Look at these two plots. This is the same signal two ways: once in the time domain and once in the frequency domain. The two plots have exactly the same information – just a different lens.5 The time-domain plot shows the signal as it changes over time. The frequency-domain plot shows how much of the signal is happening at different frequencies. You can see strong peaks around 0.02 (i.e., a 50-unit cycle) and 0.005 (a 200-unit cycle). This is the power of spectral analysis: it reveals hidden periodicity even when the time series looks noisy or chaotic. 12.4.2 Worked example Let’s start with a very quick and clean example. We will create the signal wav as the sum of a few sine waves plus some noise. Just as we do when we work in the time domain we will always plot our data! Here is a sine wave with a period of 250 (frequency of 0.004). Thus, over 1000 observations there are four oscillations. sfPal &lt;- PNWColors::pnw_palette(name=&quot;Starfish&quot;,n=5,type=&quot;discrete&quot;) n &lt;- 1000 dat &lt;- tibble(tm = seq(1,n), wav250 = 0.3 * sin(2 * pi / 250 * tm)) p1 &lt;- ggplot() + labs(x=&quot;Some Time&quot;,y=&quot;Some Signal&quot;) p1 + geom_line(data=dat,mapping = aes(x=tm,y=wav250),color=sfPal[1]) + labs(title = &quot;Period 250, f=1/250=0.004&quot;, subtitle = &quot;Amplitude = 0.3&quot;) Let’s add to it with some more sine waves. Run this code one line at time to see each sine function. dat &lt;- dat %&gt;% mutate(wav5 = 0.5 * sin(2 * pi / 5 * tm), wav10 = sin(2 * pi / 10 * tm), wav50 = 0.75 * sin(2 * pi / 50 * tm)) p1 + geom_line(data=dat,mapping = aes(x=tm,y=wav50),color=sfPal[2]) + labs(title = &quot;Period 50, f=1/50=0.02&quot;, subtitle = &quot;Amplitude = 0.75&quot;) p1 + geom_line(data=dat,mapping = aes(x=tm,y=wav10),color=sfPal[3]) + labs(title = &quot;Period 10, f=1/10=0.1&quot;, subtitle = &quot;Amplitude = 1&quot;) p1 + geom_line(data=dat,mapping = aes(x=tm,y=wav5),color=sfPal[4]) + labs(title = &quot;Period 5, f=1/5=0.2&quot;, subtitle = &quot;Amplitude = 0.5&quot;) And sum them. dat &lt;- dat %&gt;% mutate(allWav = wav250 + wav50 + wav10 + wav5) p1 + geom_line(data=dat,mapping = aes(x=tm,y=allWav),color=sfPal[5]) + labs(title = &quot;Multiple frequencies combined&quot;) Zoom in a bit: p1 + geom_line(data=dat,mapping = aes(x=tm,y=allWav),color=sfPal[5]) + labs(title = &quot;Multiple frequencies combined&quot;) + lims(x=c(500,600)) ## Warning: Removed 899 rows containing missing values or values outside the scale range ## (`geom_line()`). Now let’s add a little noise and we have a series that looks a lot like something we might see in nature. dat &lt;- dat %&gt;% mutate(allWav = allWav + rnorm(n)) p1 + geom_line(data=dat,mapping = aes(x=tm,y=allWav),color=sfPal[5]) + labs(title = &quot;Multiple frequencies combined&quot;) A little noise and an awful lot of repeating signals. How can we analyze that series and find out what it is made of? We will calculate a periodogram using a fast Fourier transform and make a plot (also called a periodogram). The periodogram distributes the variance of a signal over frequency. In R, we will use the spectrum function which can also smooth the data in the frequency domain so that we don’t get overwhelmed by too many spikes in the plot. The text has a great, and easy to follow, explanation of how and why this smoothing works. For this signal though, we will look at the raw periodogram knowing that allWav has four sine waves in it. spectrum(dat$allWav,log=&quot;no&quot;) This is the raw periodogram. By default the plot puts the spectrum on a log axis. I’ve turned that off above but I suggest you run that again to see the log scales: spectrum(dat$allWav). The spectrum is scaled so that its area is one half the variance of the series. Again, the text discusses why, as well as discussing the bandwidth parameter (which relates to the smoothing). You can think of the y-axis as being the amplitude of the signal at a given frequency (or its power or even its information). Play with the smoothing argument, e.g., spectrum(wav,span=5,log=\"no\"). Now, we know the frequencies in these data. Are they in the periodogram? Go through and add vertical lines to the plot where we know the frequency of the sine waves. E.g., wav5 oscillates every five values and has a period of five and frequency of \\(1/5=0.2\\), thus abline(v=1/5). 12.5 Sunspot Example Simulated examples are a great way of mucking about in the frequency domain. The book has two great examples of doing spectral analysis using AR(1) models with both negative and positive values of \\(\\phi\\). Check those out. But now, let’s look at some real data – the trusty sunspot counts. data(sunspot.year) plot(sunspot.year) What periodicities / frequencies are embedded in these data? ss.spec &lt;- spectrum(sunspot.year, span=5, plot= FALSE) plot(ss.spec,log=&quot;no&quot;, type=&quot;h&quot;, xlab=&quot;Frequency (cycles / yr)&quot;, ylab=&quot;Spectral density&quot;) Two things. First, note the spikes at the 0.09 range of frequencies. What is that period (years)? About eleven years because \\(~f=1/p\\). There is also some amplitude/power/information at lower frequencies. What might that be? Can you see that in the time domain? Second, you can from the code above that you can plot an object of class spec with the plot command. See the help page for plot.spec for details (?plot.spec) on how to fuss with the plot. Here is an example where you can zoom in on the x-axis: plot(ss.spec,log=&quot;no&quot;, type=&quot;h&quot;, xlab=&quot;Frequency (cycles / yr)&quot;, ylab=&quot;Spectral density&quot;, xlim=c(0,0.2)) Here is an example where I’ll make the plot with ggplot. Note the addition of the second x-axis showing period. Try as I might, I still think in periods and not frequencies. dat &lt;- tibble(freqs = ss.spec$freq, specs = ss.spec$spec) fPretty &lt;- pretty(dat$freqs) pPretty &lt;- round(1 / fPretty, 2) ggplot(data = dat,mapping = aes(x=freqs,xend=freqs,y=0,yend=specs)) + geom_segment() + scale_y_continuous(name = &quot;Spectral density&quot;, expand = c(0,0)) + scale_x_continuous(name = &quot;Frequency (cycle / yr)&quot;, sec.axis=sec_axis(~., breaks=fPretty, labels = pPretty, name=&quot;Period (yr / cycle)&quot;)) Now, let’s look at sunspots but using the monthly data. data(sunspot.month) plot(sunspot.month) tsp(sunspot.month) # note freq = 12 ## [1] 1749.00 2024.75 12.00 The sunspot.month is the same data as in sunspot.year but with a frequency of 12 instead of one. Think about what that means for the frequency domain before you run the following code: ss.spec &lt;- spectrum(sunspot.month, span=5, plot= FALSE) dat &lt;- tibble(freqs = ss.spec$freq, specs = ss.spec$spec) fPretty &lt;- pretty(dat$freqs) pPretty &lt;- round(1 / fPretty, 2) ggplot(data = dat,mapping = aes(x=freqs,xend=freqs,y=0,yend=specs)) + geom_segment() + scale_y_continuous(name = &quot;Spectral density&quot;, expand = c(0,0)) + scale_x_continuous(name = &quot;Frequency (cycle / yr)&quot;) See if you can figure out why the x-axis has changed with the monthly sunspot data as compared to the annual data. 12.6 Your work Mess around with what I have above with the sine waves. You don’t need to show me any of that work but I suggest playing with that example until you are comfortable with the span argument, finding peaks, logging and unlogging the axes, and so on. Try adding a trend. 12.6.1 Generate three signals Using a reasonable sample size, generate three time series: white noise, an AR(1) process, and a random walk. Plot the spectras. Do they look they way you thought they would? 12.6.2 Milankovitch Many—hopefully all—of you have encountered the concept of orbital forcing through Milankovitch cycles. These are the slow, rhythmic changes in Earth’s orbit and axial tilt that influence how much solar energy reaches different parts of the planet. These cycles operate over tens to hundreds of thousands of years and are widely accepted as a major driver of long-term climate variability, including the timing of ice ages. In this section, we get to work with one of the most iconic data sets in paleoclimate science: reconstructed summer insolation (in W/m²) at 65°N latitude over the past five million years. These data come from Berger and Loutre (1991), whose calculations are foundational in the field. The file jul65N.rds contains a data frame with two columns: kya: Time, in thousands of years ago W.per.m2: July insolation at 65°N These are not noisy, messy observational data — they are calculated from celestial mechanics using well-understood physics. That’s what makes this so compelling: you are looking at the long-term heartbeat of the Earth’s orbital rhythm. Your task is to apply spectral analysis to these data and identify the dominant periodicities. What cycles jump out from the periodogram? Are they in the the canonical range of Milankovitch frequencies associated with eccentricity, obliquity, and precession? Take a moment to appreciate this: we are peering into multi-million-year cycles in Earth’s orbital geometry, captured in a handful of sine waves. It’s one of the most elegant examples of how the frequency domain can reveal deep, physical structure hidden in long time series. jul65N &lt;- readRDS(&quot;data/jul65N.rds&quot;) ggplot(data = jul65N, aes(x=kya,y=W.per.m2)) + geom_line(color=sfPal[5]) + labs(x=&quot;Thousands of years ago&quot;,y = expression(W/m^2), title=&quot;July insolation at 65N&quot;) 12.6.3 Write Up and Reflect Pass in a R Markdown doc with your analysis. Leave all code visible, although you may quiet messages and warnings if desired. Turn in your knitted html. The last section of your document should include a reflection where you explain how it all went. What triumphs did you have? What is still confusing? 12.7 References Berger A. and Loutre M. F. (1991) Insolation values for the climate of the last 10 million years. Quaternary Sciences Review, Vol. 10 No. 4, pp. 297-317, 1991. 12.8 Postscript: A Glimpse of Wavelets Wavelets: the pictures that take a thousand words to explain. We won’t go deep into wavelet analysis in this course, but it’s worth seeing what the fuss is about. Wavelets are a powerful tool that extend the ideas of the frequency domain by letting us ask how frequency content changes over time. Unlike Fourier analysis, which assumes that the dominant frequencies are constant throughout the whole series, wavelets can reveal when those frequencies are active and when they fade. Think of it like this: a periodogram tells you what’s in the signal, but not when. Wavelets let you look at the signal’s frequency content as it evolves through time. That’s especially useful for messy, real-world data where processes may come and go — like a 20-year cycle that’s strong in one century and absent in the next. In this postscript, we load a tree-ring series and apply a wavelet transform to visualize the timing and scale of periodicity in the data. You’ll see two types of wavelet decomposition: - A continuous wavelet transform, which produces a heatmap showing power (strength) at different periods across time. - A discrete wavelet transform, which breaks the time series into additive components at different time scales (e.g., 2-year, 4-year, 8-year). These are not tools you need to use — just something to marvel at. We’ll talk briefly about wavelets in class, especially in cases where classic spectral methods fall short. Wavelets are how you get at signals that change — not just in strength, but in when they show up. They’re a great example of how data science keeps evolving to help us make sense of complex, dynamic systems. library(waveslim) library(dplR) # load a bunch of raw tree-ring data data(co021) plot(co021, plot.type=&quot;spag&quot;) # detrend and average the individual series to make a chronology co021.rwi &lt;- detrend(co021, method=&quot;Spline&quot;) co021.crn &lt;- chron(co021.rwi) # here is the signal we will look at: plot(co021.crn, add.spline=TRUE, nyrs=64) dat &lt;- co021.crn[, 1] yrs &lt;- time(co021.crn) # continuous wavelet transform out.wave &lt;- morlet(y1 = dat, x1 = yrs, p2 = 8, dj = 0.1, siglvl = 0.99) wavelet.plot(out.wave, useRaster=NA, reverse.y = TRUE) # discrete wavelet transform nYrs &lt;- length(yrs) nPwrs2 &lt;- trunc(log(nYrs)/log(2)) - 1 dat.mra &lt;- mra(dat, wf = &quot;la8&quot;, J = nPwrs2, method = &quot;modwt&quot;, boundary = &quot;periodic&quot;) YrsLabels &lt;- paste(2^(1:nPwrs2),&quot;yrs&quot;,sep=&quot;&quot;) par(mar=c(3,2,2,2),mgp=c(1.25,0.25,0),tcl=0.5, xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) plot(yrs,rep(1,nYrs),type=&quot;n&quot;, axes=FALSE, ylab=&quot;&quot;,xlab=&quot;&quot;, ylim=c(-3,38)) title(main=&quot;Multiresolution decomposition of dat&quot;,line=0.75) axis(side=1) mtext(&quot;Years&quot;,side=1,line = 1.25) Offset &lt;- 0 dat.mra2 &lt;- scale(as.data.frame(dat.mra)) for(i in nPwrs2:1){ # x &lt;- scale(dat.mra[[i]]) + Offset x &lt;- dat.mra2[,i] + Offset lines(yrs,x) abline(h=Offset,lty=&quot;dashed&quot;) mtext(names(dat.mra)[[i]],side=2,at=Offset,line = 0) mtext(YrsLabels[i],side=4,at=Offset,line = 0) Offset &lt;- Offset+5 } box() In theory, the time and frequency domains are mathematically equivalent and you can reconstruct one from the other using a full Fourier transform. But in practice, smoothing, truncation, or sampling limitations can obscure some details in one domain or the other.↩︎ "],["references-1.html", "13 References", " 13 References This document was built using the R packages listed below. Citations are automatically generated from the file packages.bib. Auguie, Baptiste. 2017. gridExtra: Miscellaneous Functions for \"Grid\" Graphics. https://doi.org/10.32614/CRAN.package.gridExtra. Bunn, Andy, Mikko Korpela, Franco Biondi, Filipe Campelo, Stefan Klesse, Pierre Mérian, Fares Qeadan, and Christian Zang. 2025. dplR: Dendrochronology Program Library in r. https://github.com/OpenDendro/dplR. Cowpertwait, Paul S. P., and Andrew V. Metcalfe. 2009. Introductory Time Series with r. New York: Springer. https://doi.org/10.1007/978-0-387-88698-5. Hyndman, Rob, George Athanasopoulos, Christoph Bergmeir, Gabriel Caceres, Leanne Chhay, Kirill Kuroptev, Mitchell O’Hara-Wild, et al. 2025. Forecast: Forecasting Functions for Time Series and Linear Models. https://pkg.robjhyndman.com/forecast/. Pinheiro, José, Douglas Bates, and R Core Team. 2025. Nlme: Linear and Nonlinear Mixed Effects Models. https://svn.r-project.org/R-packages/trunk/nlme/. Robinson, David, Alex Hayes, and Simon Couch. 2025. Broom: Convert Statistical Objects into Tidy Tibbles. https://broom.tidymodels.org/. Whitcher, Brandon. 2024. Waveslim: Basic Wavelet Routines for One-, Two-, and Three-Dimensional Signal Processing. https://waveslim.blogspot.com. Wickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org. Wilke, Claus O. 2024. Cowplot: Streamlined Plot Theme and Plot Annotations for Ggplot2. https://wilkelab.org/cowplot/. Zambrano-Bigiarini, Mauricio. 2024. hydroTSM: Time Series Management and Analysis for Hydrological Modelling. https://github.com/hzambran/hydroTSM. Zeileis, Achim, Gabor Grothendieck, and Jeffrey A. Ryan. 2025. Zoo: S3 Infrastructure for Regular and Irregular Time Series (z’s Ordered Observations). https://zoo.R-Forge.R-project.org/. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
